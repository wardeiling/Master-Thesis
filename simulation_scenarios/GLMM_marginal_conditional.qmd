---
title: "generalized mixed linear model: marginal versus conditional effects"
author: "Ward B. Eiling"
format: html
bibliography: GLMM_references.bib
execute:
  warning: false
  message: false
---

```{r}
#| label: packages
library(tidyverse)
library(here)
library(geepack)
library(lme4)
```


In this document, we will shed light on an issue that is well-known in the field of generalized linear mixed models (GLMMs), namely the discrepancy between population-averaged (PA; i.e., marginal) and subject-specific (SS; i.e., conditional-on-the-random-effect) interpretations of a fixed slope when the link function is non-linear. Note that subject-specific does not implicate the presence of a random slope. The main distinction between SS and PA models is whether the regression coefficients describe an individual's or the average population response to changing $x$ [@Zeger1988]. An excellent example between the two is given by @Zeger1988:

> "For example, if $x_{it}$, indicates whether subject $i$ smokes at time $t$, and Yit is the presence/absence of respiratory infection, the PA model estimates the difference in infection rates between smokers and nonsmokers; the SS model estimates the expected change in an individual's probability of infection given a change in smoking status." (p. 1051)

This document will explain why a discrepancy between the PA and SS interpretations of the fixed slope arises when the link function is non-linear. In this treatment, we will focus on very simple GLMMs with only one predictor and a random intercept.

## Generalized Mixed Linear Model with Non-linear Logit Function

In this document, we will reproduce the figure made by Dimitri Rizopoulos in de course [CE08](https://www.drizopoulos.com/courses/EMC/CE08.pdf) (slide 321), where the generalized mixed linear model (GLMM) with a non-linear logit function results in a discrepancy between the marginal and conditional relationships (see also @Zeger1988 for the similar and original treatment). Let us consider the following GLMM

$$
\text{log} \frac{\pi_{it}}{1-\pi_{it}} = \beta_0 + \beta_1 X_{it} + b_{0i} + \epsilon_{it} \quad b_{0i} \sim \mathcal{N}(0,\sigma_{b0}^2)
$$

Where $\pi_{it} = Pr(Y_{it} = 1)$ the probability of a positive response for subject $i$ at time $t$, $Y_{it}$ is the binary response variable, $X_{it}$ is the continuous covariate, $A_{it}$ is the binary treatment indicator, and $b_{0i}$ is the random intercept. The fixed effects are $\beta_0$ and $\beta_1$ for the intercept and the continuous covariate. The random intercept $b_{0i}$ and the error term $\epsilon_{it}$ are assumed to be normally distributed with mean zero and variance $\sigma_{b0}^2$ and $\sigma_{\epsilon}^2$, respectively.

We specify the following parameter values

-   $\beta_0 = 0$
-   $\beta_1 = 0.5$
-   $\sigma_{b0}^2 = 0.5$
-   $\sigma_{\epsilon}^2 = 0$

For simplicity, this model does not include a treatment effect $A_{it}$ nor exogenous noise $\epsilon_{it}$. Now we will simulate data from this model and directly infer the relationships between the continuous covariate $X_{it}$ and the probability of a positive response $\pi_{it}$.

```{r}
#| label: fig-marginal-vs-conditional
#| fig-cap: "Population, conditional and individual logit curves for a GLMM with a non-linear logit function."
#| fig.width: 10
#| fig.height: 8

set.seed(123) # For reproducibility

# Parameters
beta_0 <- 0 # for simplicity
beta_1 <- 0.9
sigma_b0 <- sqrt(4)

n_individuals <- 20 # Number of individuals to plot
X_grid <- seq(-5, 5, length.out = 10000) # Values for X_it

# Generate random intercepts for individuals
b0 <- rnorm(n_individuals, mean = 0, sd = sigma_b0)

# Conditional-level logit and probability: the mean individual (b0 = 0)
logit_conditional <- beta_0 + beta_1 * X_grid + 0 # expected value
pi_conditional <- 1 / (1 + exp(-logit_conditional))

# Individual-specific logits and probabilities
logit_individuals <- sapply(b0, function(b) beta_0 + beta_1 * X_grid + b) # expected value
pi_individuals <- 1 / (1 + exp(-logit_individuals))

# Population-level logit and probability: average probabilities across individuals
pi_population <- rowMeans(pi_individuals)

# save the upcoming plot
# png("figure/fig-marginal-vs-conditional.png", width = 1200, height = 1000, res = 150)

# Plot the conditional-level curve
plot(X_grid, pi_conditional, type = "l", lwd = 4, col = "black",
     ylim = c(0, 1), xlim = c(-5, 5), xlab = expression(X[it]), ylab = "Probability",
     main = "Population, Conditional and Individual Logit Curves")

# Add individual curves (grey lines)
for (i in 1:n_individuals) {
  lines(X_grid, pi_individuals[, i], col = "grey", lwd = 1)
}

# add the population-level curve (red line)
lines(X_grid, pi_population, col = "red", lwd = 4)

# Add a legend
legend("topleft", legend = c("conditional", "population", "individuals"),
       col = c("black", "red",  "grey"), lty = c(1, 1, 1), lwd = c(4, 4, 1))

# dev.off()
```

When we estimate the GLMM with the log link function, we obtain the conditional-on-the-random effects interpretation of fixed-effects regression coefficients (i.e., the effects of covariates on changes in an individualâ€™s transformed mean response). This is directly opposed by marginal models such as GEE, where "population averages are the targets of inference" (Rizopoulos, CE08, p. 331)

Let's now take the scenario where the predictor $X_{it}$ is a binary variable.

```{r}
#| label: fig-overlay-logistic
#| fig-cap: "Binary predictor with interpolation of logistic curves for a GLMM with a non-linear logit function."
#| fig.width: 10
#| fig.height: 8

set.seed(123) # For reproducibility

# Parameters
beta_0 <- 0
beta_1 <- 5
sigma_b0 <- sqrt(4)

n_individuals <- 20 # Number of individuals to plot

# Continuous logistic predictor
X_grid_cont <- seq(0, 1, length.out = 10000)

# Binary predictor
X_grid_bin <- c(0, 1)

# Generate random intercepts for individuals
b0 <- rnorm(n_individuals, mean = 0, sd = sigma_b0)

# Continuous predictor: Compute conditional and individual probabilities
logit_conditional <- beta_0 + beta_1 * X_grid_cont
pi_conditional <- 1 / (1 + exp(-logit_conditional))

logit_individuals_cont <- sapply(b0, function(b) beta_0 + beta_1 * X_grid_cont + b)
pi_individuals_cont <- 1 / (1 + exp(-logit_individuals_cont))

pi_population_cont <- rowMeans(pi_individuals_cont)

# Binary predictor: Compute probabilities
logit_individuals_bin <- sapply(b0, function(b) outer(beta_0 + beta_1 * X_grid_bin, b, "+"))
pi_individuals_bin <- 1 / (1 + exp(-logit_individuals_bin))

pi_population_bin <- rowMeans(pi_individuals_bin)

# Plot the continuous logistic curve
plot(X_grid_cont, pi_conditional, type = "l", lwd = 4, col = "black",
     ylim = c(0, 1), xlim = c(0, 1), xlab = expression(X[it]), ylab = "Probability",
     main = "Binary Predictor with Interpolation of Logistic Curves")

# Add individual curves (grey lines) for continuous logistic function
for (i in 1:n_individuals) {
  lines(X_grid_cont, pi_individuals_cont[, i], col = "grey", lwd = 1)
}

# Add population-level curve (red line) for continuous logistic function
lines(X_grid_cont, pi_population_cont, col = "red", lwd = 4)

# Overlay binary predictor points (black and red)
points(X_grid_bin, 1 / (1 + exp(-(beta_0 + beta_1 * X_grid_bin))), col = "black", pch = 16, cex = 1.5)
for (i in 1:n_individuals) {
  points(X_grid_bin, pi_individuals_bin[, i], col = "grey", pch = 16, cex = 0.6)
}
points(X_grid_bin, pi_population_bin, col = "red", pch = 16, cex = 1.5)

# Add a legend
legend("topleft", legend = c("conditional (interpolation)", "population (interpolation)", "individuals (interpolation)", 
                             "datapoint conditional", "datapoint population", "datapoints individuals"),
       col = c("black", "red", "grey", "black", "red", "grey"), 
       lty = c(1, 1, 1, NA, NA, NA), 
       pch = c(NA, NA, NA, 16, 16, 16), 
       lwd = c(4, 4, 1, NA, NA, NA), pt.cex = c(NA, NA, NA, 1.5, 1.5, 0.6))
```

When we fit a GLMM with a log-link function and the variance of the random intercept is non-zero, the fixed effects are interpreted according to the subject-specific interpretation. As a result, GLMMs are most useful when the main scientific objective is to make inferences about individuals rather than population averages.

## Mixed Linear Model: Generalized Linear Mixed Model with Identity Link Function

Let us now consider a simple mixed linear model with a continuous outcome variable $Y_{it}$ and a continuous covariate $X_{it}$, where the random intercept $b_{0i}$ is normally distributed with mean zero and variance $\sigma_{b0}^2$. The model is given by

$$
Y_{it} = \beta_0 + \beta_1 X_{it} + b_{0i} + \epsilon_{it} \quad b_{0i} \sim \mathcal{N}(0,\sigma_{b0}^2)
$$

Where $\epsilon_{it}$ is the error term. We will simulate data from this model and directly infer the relationships between the continuous covariate $X_{it}$ and the outcome variable $Y_{it}$.

```{r}
#| label: fig-mixed-linear-model
#| fig-cap: "Population, conditional and individual response curves for a mixed linear model with an identity link function."
#| fig.width: 10
#| fig.height: 8

set.seed(123) # For reproducibility

# Parameters
beta_0 <- 0 # Intercept
beta_1 <- 0.9 # Slope
sigma_b0 <- sqrt(4) # Standard deviation of random intercepts

n_individuals <- 20 # Number of individuals to plot
X_grid <- seq(-5, 5, length.out = 10000) # Values for X_it

# Generate random intercepts for individuals
b0 <- rnorm(n_individuals, mean = 0, sd = sigma_b0)

# create centering option
expected_b0_is_zero = TRUE
if (expected_b0_is_zero == TRUE){
  b0 <- b0 - mean(b0)  # Centering step
}

# Conditional-level prediction (mean individual, b0 = 0)
y_conditional <- beta_0 + beta_1 * X_grid

# Individual-specific predictions
y_individuals <- sapply(b0, function(b) beta_0 + beta_1 * X_grid + b)

# Population-level prediction (average across individuals)
y_population <- rowMeans(y_individuals)

# Save the upcoming plot
# png("figure/fig-marginal-vs-conditional-mlm.png", width = 1200, height = 1000, res = 150)

# Plot the conditional-level curve
plot(X_grid, y_conditional, type = "l", lwd = 4, col = "black",
     ylim = range(y_individuals), xlim = c(-5, 5), xlab = expression(X[it]), ylab = expression(Y),
     main = "Population, Conditional, and Individual Response Curves")

# Add individual curves (grey lines)
for (i in 1:n_individuals) {
  lines(X_grid, y_individuals[, i], col = "grey", lwd = 1)
}

# Add the population-level curve (red line)
lines(X_grid, y_population, col = "red", lwd = 4)

# Add a legend
legend("topleft", legend = c("conditional", "population", "individuals"),
       col = c("black", "red", "grey"), lty = c(1, 1, 1), lwd = c(4, 4, 1))

# dev.off()
```

Now, we can see that while the population-averaged and subject-specific lines are parallel to one another, they are not equivalent due to the fact that the expected value (i.e., mean) of the random intercept is not exactly zero. One way of fixing this is (1) to simulate data with a very large sample size N and only show a certain amount in the plot or (2) to simulate data with a random intercept that is exactly zero. The latter is done in the code above by centering the random intercepts. However, when the expected value of the random intercept is zero, the population-averaged and subject-specific lines are equivalent, implying that both interpretations are valid.

## Mixed Linear Model with Time-Varying Endogenous Covariate

Laten we nu een ingewikkelder voorbeeld beschouwen van een MLM uit sectie 2.2 van Qian et al. (2020) inclusief een time-varying covariaat $X_{it}$. In dit geval wordt elk individu op 2 tijdspunten geobserveerd ($T_i = 2$), en de covariaat op het tweede tijdspunt is de lag-1 uitkomst: $X_{i2} = Y_{i2}$. Door de uitkomst te laggen, hebben we in wezen drie tijdspunten: $X_{i1}$, $X_{i2} = Y_{i2}$, en $Y_{i3}$.

Stel dat de variabelen worden gegenereerd volgens het volgende multilevel lineare model (MLM) met een random intercept:

$$
b_i \sim N(0, \sigma_b^2),
$$

$$
X_{i1} \sim N(0, \sigma_{X_1}^2) \text{ independently of } b_i,
$$

$$
Y_{i2} \mid X_{i1}, b_i \sim N(\beta_0 + \beta_1 X_{i1} + b_i, \sigma_\epsilon^2),
$$

$$
X_{i2} = Y_{i2},
$$

$$
Y_{i3} \mid X_{i1}, Y_{i2}, X_{i2}, b_i \sim N(\beta_0 + \beta_1 X_{i2} + b_i, \sigma_\epsilon^2).
$$

Dit impliceert the conditionele relatie

$$
E[Y_{it+1} \mid X_{it}] = \beta_0 + \beta_1 X_{i1} + b_i.
$$

terwijl de marginale relatie ingewikkelder is

$$
E[Y_{i2} \mid X_{i1}] = \beta_0 + \beta_1 X_{i1}.
$$

$$
E[Y_{i3} \mid X_{i2}] = (1-\rho \zeta - \rho) \beta_0 + [(1-\rho\zeta) \beta_1+\rho] X_{i2}.
$$

waar $\rho = \frac{\sigma_b^2}{\sigma_b^2 + \sigma_\epsilon^2}$ en $\zeta = \frac{\beta_1 \sigma_{X_1}^2}{\beta_1 \sigma_{X_1}^2 + \sigma_b^2 + \sigma_\epsilon^2}$ (see [marginal-relationship-resource](https://bookdown.org/mike/data_analysis/linear-mixed-models.html#random-intercepts-model)). Laten we nu weer data simuleren en zowel de marginale als conditionele relaties tussen de covariaat $X_{it}$ en de uitkomst variabele $Y_{it}$ onderzoeken, maar we zetten $\sigma^2_{\epsilon} = 0$ zodat dit niet afleidt. Let's now simulate data from this model and directly infer the relationships between the continuous covariate $X_{it}$ and the outcome variable $Y_{it}$.

```{r}
#| label: dgm-section2.2
#| cache: true
#| eval: false
#| echo: true

### Data Generation 

set.seed(123)

# parameters
sigma_b = 3
sigma_X1 = 1
sigma_epsilon = 1 # let's exclude this for now
beta_0 = 0.8
beta_1 = 2

n_individuals = 100000

# generate random intercepts
simdata <- data.frame(id = 1:n_individuals,
                      b0 = rnorm(n_individuals, mean = 0, sd = sigma_b),
                      X1 = rnorm(n_individuals, mean = 0, sd = sigma_X1), # we have no stable between-person differences in X1
                      X2 = rep(NA, n_individuals),
                      Y2 = rep(NA, n_individuals),
                      Y3 = rep(NA, n_individuals))

# create a loop to compute the expected values for each individual
for (i in 1:n_individuals) {
  simdata$Y2[i] <- beta_0 + beta_1 * simdata$X1[i] + simdata$b0[i] + rnorm(1, mean = 0, sd = sigma_epsilon)
  simdata$X2[i] <- simdata$Y2[i]
  simdata$Y3[i] <- beta_0 + beta_1 * simdata$X2[i] + simdata$b0[i] + rnorm(1, mean = 0, sd = sigma_epsilon)
}

saveRDS(simdata, "simulation_scenarios/output/simdata_section2.2-eps.rds")
```

Let us check if the formulas supplied in Qian et al. (2020) can be used to retrieve the true marginal effect.

```{r}
#| label: marginal-effect-computation
#| cache: true

# load the simulated data
simdata <- readRDS(here("simulation_scenarios/output/simdata_section2.2-eps.rds"))

### Compute Marginal and Conditional Effects

# parameters
sigma_b = 3
sigma_X1 = 1
sigma_epsilon = 1 # we excluded this term
beta_0 = 0.8
beta_1 = 2

# compute rho and zeta
rho = sigma_b^2 / (sigma_b^2 + sigma_epsilon^2) # should be 1, since sigma_epsilon = 0
zeta = beta_1 * sigma_X1^2 / (beta_1 * sigma_X1^2 + sigma_b^2 + sigma_epsilon^2)

# compute marginal effect
marginal_intercept_x1_y2 = beta_0
marginal_slope_x1_y2 = beta_1

marginal_intercept_x2_y3 = (1 - rho * zeta - rho) * beta_0
marginal_slope_x2_y3 = (1 - rho * zeta) * beta_1 + rho
```

Let us now estimate the marginal effects separately for $Y_2$ and $Y_3$ as well as for $Y_t$.

```{r}
### Estimate Marginal Effects

lm_y2_x1 <- lm(simdata$Y2 ~ simdata$X1)$coef
lm_y3_x2 <- lm(simdata$Y3 ~ simdata$X2)$coef

# turn the X1 and X2 variables into one column X and Y1 and Y2 into one column Y
simdata_long <- simdata %>%
  # Convert to long format
  pivot_longer(cols = starts_with("X"),
               names_to = "time_X",
               values_to = "X") %>%
  pivot_longer(cols = starts_with("Y"),
               names_to = "time_Y",
               values_to = "Y") %>%
  # Extract time indices
  mutate(time_X = as.numeric(gsub("\\D", "", time_X)),
         time_Y = as.numeric(gsub("\\D", "", time_Y))) %>%
  # Keep only rows where time_X aligns with time_Y - 1
  filter(time_X + 1 == time_Y) %>%
  # Create a single time column for the current timepoint
  mutate(time = time_X) %>%
  # Select and rename columns
  select(id, time, b0, X_lag1 = X, Y)

# compute cluster means for the level 1 predictor X
simdata_long$cluster_mean_X <- ave(simdata_long$X_lag1, simdata_long$id, FUN = mean)

### estimate combined marginal effect of x on y

# Raw X as predictor
(lm_y_xlag1 <- lm(Y ~ X_lag1, data = simdata_long)$coef)
(mlm_y_xlag1 <- summary(lmer(Y ~ X_lag1 + (1 | id), data = simdata_long))$coefficients[,1])
(gee_ind_y_xlag1 <- geepack::geeglm(Y ~ X_lag1, id = id, data = simdata_long, family = gaussian, corstr = "independence")$coef)
# these two estimation methods yield identical results for the parameter values (but not for SEs!)

# Mundlak's contextual model: raw X and cluster mean X as predictors
(lm_y_xlag1_mundlak <- lm(Y ~ X_lag1 + cluster_mean_X, data = simdata_long)$coef)
(mlm_y_xlag1_mundlak <- summary(lmer(Y ~ X_lag1 + cluster_mean_X + (1 | id), data = simdata_long))$coefficients[,1])
(gee_ind_y_xlag1_mundlak <- geepack::geeglm(Y ~ X_lag1 + cluster_mean_X, id = id, data = simdata_long, family = gaussian, corstr = "independence")$coef)

# store all models in a list
all_models <- list(lm_rawx = lm_y_xlag1,
                   mlm_rawx = mlm_y_xlag1,
                   gee_ind_rawx = gee_ind_y_xlag1,
                   lm_mundlak = lm_y_xlag1_mundlak,
                   mlm_mundlak = mlm_y_xlag1_mundlak,
                   gee_ind_mundlak = gee_ind_y_xlag1_mundlak)

# create table with appropriate column names
table <- matrix(data = NA, nrow = 6, ncol = 2, dimnames = list(c("lm_rawx", "mlm_rawx", "gee_ind_rawx",
                                                                  "lm_mundlak", "mlm_mundlak", "gee_ind_mundlak"),
                                                                c("X_lag1", "cluster_mean_X")))

# fil table with estimates assigning to the correct row and column
for (i in 1:length(all_models)){
  # check coefficients in the model
  names <- names(all_models[[i]])
  if ("X_lag1" %in% names){
    table[i,1] <- all_models[[i]]["X_lag1"]
  }
  if ("cluster_mean_X" %in% names){
    table[i,2] <- all_models[[i]]["cluster_mean_X"]
  }
}

table
```

We can see that when merely including the raw predictor X into the three models, the marginal models (that assume independence: GEE indep, linear regression) yield different results to the MLM, due to violation of the endogeneity assumption.

Interestingly, however, when we apply Mundlak's contextual approach to the three models, the parameter estimates are identical across the three models, since correct specification of the MLM addresses the endogeneity of the covariate.

let us visualize the density plots for $Y_2$ and $Y_3$.

```{r}
# create density plot for Y2 and Y3 with overlay
ggplot(simdata_long, aes(x = Y, fill = as.factor(time))) +
  geom_density(alpha = 0.5) +
  theme_minimal()
```

We can see that the density for $Y_3$ is slightly shifted to the right and has increased variability compared to $Y_2$. This is due to the fact that $Y_3$ not only borrows variability from $X_1$ but also from the random intercept $b_i$.

Let us now plot the marginal and conditional effect in separate and combined plots.

```{r}
#| label: fig-section2.2
#| cache: true
#| fig-cap: "True conditional effect and estimated marginal effects for 10,000 simulated individuals in a situation with a time-varying endogenous covariate in a mixed linear model."
#| fig-height: 10
#| fig-width: 10

### Plot Marginal and Conditional Effects 

# Create separate plots for Y2 and Y3

# png("figure/fig-mlm-time-varying-covariate.png", res = 300, width = 4000, height = 2000)

par(mfrow = c(2, 2))
plot(simdata$X1, simdata$Y2, col = "blue", pch = 16, cex = 0, xlab = expression(X[i1]), ylab = expression(Y[i2]),
     xlim = c(-15, 15), ylim = c(-40, 40), main = "Effect of X1 on Y2")
abline(a = beta_0, b = beta_1, col = "blue", lty = 1, lwd = 4) # true conditional effect X1 Y2  
abline(reg = lm_y2_x1, col = "red", lty = 2, , lwd = 2) # estimated marginal effect X1 Y2
abline(a = marginal_intercept_x1_y2, b = marginal_slope_x1_y2, col = "red", lty = 1, lwd = 2) # true marginal effect X1 Y2

legend("topleft", legend = c("true conditional effect", "true marginal effect", "estimated marginal effect"),
       col = c("blue", "red", "red"), lty = c(1, 1, 2), lwd = c(4, 2, 2))

plot(simdata$X2, simdata$Y3, col = "blue", pch = 16, cex = 0, xlab = expression(X[i2]), ylab = expression(Y[i3]),
     xlim = c(-15, 15), ylim = c(-40, 40), main = "Effect of X2 on Y3")
abline(a = beta_0, b = beta_1, col = "blue", lty = 1, lwd = 4) # true conditional effect X2 Y3
abline(reg = lm_y3_x2, col = "orange", lty = 2, , lwd = 2) # estimated marginal effect X2 Y3
abline(a = marginal_intercept_x2_y3, b = marginal_slope_x2_y3, col = "orange", lty = 1, lwd = 2) # true marginal effect X2 Y3

legend("topleft", legend = c("true conditional effect", "true marginal effect", "estimated marginal effect"),
       col = c("blue", "orange", "orange"), lty = c(1, 1, 2), lwd = c(4, 2, 2))

# dev.off()

# Create one plot for Y2 and Y3

# png("figure/fig-mlm-time-varying-covariate-together.png", res = 300, width = 2000, height = 2000)

# create empty plot
plot(1, type = "n", xlim = c(-15, 15), ylim = c(-40, 40), xlab = expression(X[it]), ylab = expression(Y[it+1]), 
     main = "Effect of X_t on Y_t+1")

# now add the marginal effects
abline(a = beta_0, b = beta_1, col = "blue", lty = 1, lwd = 4) # true conditional effect X1 Y2

abline(a = marginal_intercept_x1_y2, b = marginal_slope_x1_y2, col = "red", lwd = 2) # true marginal effect X1 Y2
abline(lm(simdata$Y2 ~ simdata$X1), col = "red", lty = 2, lwd = 2) # estimated marginal effect X1 Y2
abline(a = marginal_intercept_x2_y3, b = marginal_slope_x2_y3, col = "orange", lwd = 2) # true marginal effect X2 Y3
abline(lm(simdata$Y3 ~ simdata$X2), col = "orange", lty = 2, lwd = 2) # estimated marginal effect X2 Y3

legend("topleft", legend = c("true conditional effect X1 Y2", "true marginal effect X1 Y2", "estimated marginal effect X1 Y2", "true marginal effect X2 Y3", "estimated marginal effect X2 Y3"),
       col = c("blue", "red", "red", "orange", "orange"), lty = c(1, 1, 2, 1, 2), lwd = c(4, 2, 2, 2, 2))

# create plot for the total effect of X on Y
plot(1, type = "n", xlim = c(-15, 15), ylim = c(-40, 40), xlab = expression(X[it]), ylab = expression(Y[it+1]), 
     main = "Total Effect of X_t on Y_t+1")

abline(a = beta_0, b = beta_1, col = "blue", lty = 1, lwd = 4) # true conditional effect X on Y
abline(reg = lm_y_xlag1, col = "violet", lty = 2, lwd = 2) # estimated total marginal effect X on Y

legend("topleft", legend = c("true conditional effect", "estimated total marginal effect"),
       col = c("blue", "violet"), lty = c(1, 2), lwd = c(4, 2))

# dev.off()
```

In @fig-section2.2 and @tbl-section2.2 we can see that the true and estimated marginal effects are equivalent to the true conditional effects for the relationship between $X_{i1}$ and $Y_{i2}$, but not for the relationship between $X_{i2}$ and $Y_{i3}$. This is due to the endogeneity of the covariate of $X_{i2}$, which implies dependence on the random intercept and, in turn, $E[b_i \mid X_{it}]$ is nonzero. What is interesting, however, is the discrepancy between the computed (through the formula of Qian et al. (2020)) and estimated (through linear regression) marginal intercept for $Y_{i2}$ and $Y_{i3}$; and the alignment of the slope values.

```{r}
#| label: tbl-section2.2
#| echo: false

### create table with overview of results
results <- data.frame(
  row.names = c("intercept X1 Y2", "slope X1 Y2", "intercept X2 Y3", "slope X2 Y3"),
  specified_conditional_effect = c(beta_0, beta_1, beta_0, beta_1),
  true_marginal_effect = c(marginal_intercept_x1_y2, marginal_slope_x1_y2, marginal_intercept_x2_y3, marginal_slope_x2_y3),
  estimated_marginal_effect = c(lm_y2_x1[1], lm_y2_x1[2], lm_y3_x2[1], lm_y3_x2[2])
)

knitr::kable(results, caption = "Summary of separate true conditional and estimated marginal effects for the simulated data (N = 100,000).", digits = 3)
```

In @tbl-section2.2-combined we can see that the estimated marginal effects differ from the true conditional effects for the relationship between $X_{it}$ and $Y_{it+1}$. This is again due to the fact that the expected value of the random intercept given the covariate is nonzero. In addition, it may be observed that the linear regression and the GEE with working independence yield the same parameter estimates, which is unsurprising given the independence of the observations and lack of random effects.

```{r}
#| label: tbl-section2.2-combined
#| echo: false

# create table with combined marginal and conditional effects
results_combined <- data.frame(
  row.names = c("intercept X_t Y_t+1", "slope X_t Y_t+1"),
  specified_conditional_effect = c(beta_0, beta_1),
  lm_est_marginal_effect = c(lm_y_xlag1[1], lm_y_xlag1[2]),
  gee_ind_est_marginal_effect = c(gee_ind_y_xlag1[1], gee_ind_y_xlag1[2])
)

knitr::kable(results_combined, caption = "Summary of total true conditional and estimated marginal effects for the simulated data (N = 100,000).", digits = 3)
```

Laten we dit voorbeeld nu proberen te creeeren met een range aan begin waardes voor X1 met X1_grid.

```{r}
#| label: dgm-section2.2-grid
#| cache: true
#| eval: false
#| echo: true

# Data generation
set.seed(123)

# Parameters
sigma_b <- 3
sigma_epsilon <- 1 # let's exclude this for now
beta_0 <- 0.8
beta_1 <- 2

n_individuals <- 100 # Reduced for example purposes
X1_grid <- seq(-15, 15, length.out = 1000) # Grid variable
n_grid <- length(X1_grid)

# Create an array to store the results: dimensions are individuals x grid points x variables
# Variables: 1 = X2, 2 = Y2, 3 = Y3
simdata <- array(NA, dim = c(n_individuals, n_grid, 3))

# Generate random intercepts for individuals
b0 <- rnorm(n_individuals, mean = 0, sd = sigma_b)

# Loop over individuals and grid values
for (i in 1:n_individuals) {
  epsilon_y2 <- rnorm(1, mean = 0, sd = sigma_epsilon)
  epsilon_y3 <- rnorm(1, mean = 0, sd = sigma_epsilon)
  for (j in 1:n_grid) {
    X1 <- X1_grid[j]
    Y2 <- beta_0 + beta_1 * X1 + b0[i] + epsilon_y2
    X2 <- Y2
    # X2_grid <- X1 <- X1_grid[j] + b0[i]
    Y3 <- beta_0 + beta_1 * X2 + b0[i] + epsilon_y3

    # Save the results
    simdata[i, j, 1] <- X2
    simdata[i, j, 2] <- Y2
    simdata[i, j, 3] <- Y3
  }
}

# Check dimensions and a slice of the array
dim(simdata)  # Should be (n_individuals, n_grid, 3)
# simdata[1, , ]  # First individual across the grid

# Compute the marginal effect Y2 by taking the mean across individuals
X2 <- simdata[, , 1]
Y2 <- simdata[, , 2]
Y3 <- simdata[, , 3]

# dim(Y2)  # (n_individuals, n_grid)
# dim(Y3)  # (n_individuals, n_grid)

marginal_Y2 <- colMeans(Y2)
marginal_Y3 <- colMeans(Y3)

# Compute the conditional effect by taking the mean individual (b0 = 0)
Y_conditional <- beta_0 + beta_1 * X1_grid + 0

# compute the conditional effect for Y3
X2_grid <- colMeans(X2) # X2_grid is the same as marginal_Y2
Y3_conditional <- beta_0 + beta_1 * X2_grid
```

Let us now plot these direct simulations to illustrate the marginal and conditional effects.

```{r}
#| label: fig-section2.2-grid
#| fig-height: 6
#| fig-width: 10

# select 20 random individuals 
set.seed(123)
simdata_20persons <- simdata[sample(1:n_individuals, 20), ,]
Y2_individuals <- simdata_20persons[, , 2]
Y3_individuals <- simdata_20persons[, , 3]

par(mfrow = c(1, 2))

### Plot Y2

# Plot the conditional-level curve
plot(X1_grid, Y_conditional, type = "l", lwd = 6, col = "black",
      xlim = c(-30, 30), ylim = c(-80, 80), xlab = expression(X[i1]), ylab = expression(Y[i2]),
     main = "Effect of X1 on Y2")

# Add individual curves (grey lines)
for (i in 1:20) {
  lines(X1_grid, Y2_individuals[i, ], col = "grey", lwd = 1)
}

# Add the population-level curve (red line)
lines(X1_grid, marginal_Y2, col = "red", lwd = 4)

# abline(a = 0.8, b = 2, col = "blue", lwd = 4)

# Add a legend
legend("topleft", legend = c("conditional", "population", "individuals"),
       col = c("black", "red", "grey"), lty = c(1, 1, 1), lwd = c(4, 4, 1))

### Plot Y3

# Create X2_grid based on the mean of X2 across individuals
# X2_grid <- colMeans(X2)
# X2_grid <- seq(min(X2_grid), max(X2_grid), length.out = 10000)

# Plot the conditional-level curve
plot(X2_grid, Y3_conditional, type = "l", lwd = 4, col = "black",
     xlim = c(-30, 30), ylim = c(-80, 80), xlab = expression(X[i1]), ylab = expression(Y[i3]),
     main = "Effect of X2 on Y3")
# THIS IS DISTORTED BECAUSE WE ARE PLOTTING Y3 AGAINST X1 instead of X2

# Add individual curves (grey lines)
for (i in 1:20) {
  lines(X2_grid, Y3_individuals[i, ], col = "grey", lwd = 1)
}

# Add the population-level curve (red line)
lines(X2_grid, marginal_Y3, col = "red", lwd = 4)

# abline(a = 0.8, b = 2, col = "blue", lwd = 4)

# Add a legend
legend("topleft", legend = c("conditional", "population", "individuals"),
       col = c("black", "red", "grey"), lty = c(1, 1, 1), lwd = c(4, 4, 1))

# var(X1_grid)
# plot(density(X1_grid))
```


Let us now create a density plot for the marginal and conditional effects.

```{r}
#| fig-height: 7
#| fig-width: 12

# replicate all elements as many times as there are individuals.
# X1 <- matrix(rep(X1_grid, each = n_individuals), nrow = n_individuals , byrow = T)

# create a matrix with zero values of n_individuals - 1 by n_grid
zeros_matrix <- matrix(rep(NA, (n_individuals-1) * n_grid), nrow = n_individuals - 1, byrow = T)
# bind the two matrices together
X1 <- rbind(X1_grid, zeros_matrix)
Y2_marg_plot <- rbind(marginal_Y2, zeros_matrix)
Y3_marg_plot <- rbind(marginal_Y3, zeros_matrix)

par(mfrow = c(1, 2))

plot(X1, Y2, col = "grey", pch = 16, cex = 1, xlab = expression(X[i1]), ylab = expression(Y[i2]),
     xlim = c(-40, 40), ylim = c(-90, 90))

abline(a = 0.8, b = 2, col = "black")

# Add the population-level curve (red line)
points(X1, Y2_marg_plot, col = "red")

plot(X2, Y3, col = "grey", pch = 16, cex = 1, xlab = expression(X[i2]), ylab = expression(Y[i3]),
     xlim = c(-40, 40), ylim = c(-90, 90))

abline(a = 0.8, b = 2, col = "black")

# Add the population-level curve (red line)
points(X2, Y3_marg_plot, col = "red")
```

First of all, it becomes clear that the variability in $Y_{i3}$ is higher than in $Y_{i2}$, which is due to the added variability imposed by the random intercept: we can see that the original range from -15 to 15 in X1 changes to a range from -40 to 40 X2. It should also be noted that due to the endogeneity, people with a great value on the random intercept will tend to have greater values on both $X_{i2}$ and $Y_{i3}$. 

Let's now see what the effect is of certain parameters in the marginal and conditional effect of example in section 2.2.

```{r}
### Compute Marginal and Conditional Effects

# parameters
sigma_b = 3
sigma_X1 = 1
sigma_epsilon = 0 # we excluded this term
beta_0 = 0.8
beta_1 = 2

# compute rho and zeta
rho = sigma_b^2 / (sigma_b^2 + sigma_epsilon^2) # should be 1, since sigma_epsilon = 0
zeta = beta_1 * sigma_X1^2 / (beta_1 * sigma_X1^2 + sigma_b^2 + sigma_epsilon^2)

# compute marginal effect
marginal_intercept_x1_y2 = beta_0
marginal_slope_x1_y2 = beta_1

marginal_intercept_x2_y3 = (1 - rho * zeta - rho) * beta_0
marginal_slope_x2_y3 = (1 - rho * zeta) * beta_1 + rho

plot(1, type = "n", xlim = c(-15, 15), ylim = c(-40, 40), xlab = expression(X[it]), ylab = expression(Y[it+1]), 
     main = "Effect of X_t on Y_t+1")

abline(a = beta_0, b = beta_1, col = "blue", lty = 1, lwd = 4) # true conditional effect X1 Y2
abline(a = marginal_intercept_x1_y2, b = marginal_slope_x1_y2, col = "red", lwd = 2) # true marginal effect X1 Y2
abline(a = marginal_intercept_x2_y3, b = marginal_slope_x2_y3, col = "orange", lwd = 2) # true marginal effect X2 Y3

legend("topleft", legend = c("true conditional effect", "true marginal effect X1 Y2", "true marginal effect X2 Y3"),
       col = c("blue", "red", "orange"), lty = c(1, 1, 1), lwd = c(4, 2, 2))
```

