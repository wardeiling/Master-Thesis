---
title: "Estimation of Effects of Endogenous Time-Varying Covariates: A Comparison Of Multilevel Linear Modeling and Generalized Estimating Equations"
subtitle: "Research Report" 
author: 
  - name: "Ward B. Eiling (9294163)"
    orcid: "0009-0007-8114-9497"
    affiliation: 
      - name: "Utrecht University"
      - department: "Methodology and Statistics"
date: "Dec 22, 2024"
date-format: long
format: 
  pdf:
    papersize: a4
    fig-pos: 'H'
    keep-tex: true
    # toc: true
    number-sections: true
    colorlinks: true
    indent: true
    header-includes:
      - \usepackage{fancyhdr}
      - \usepackage{amsmath}
      # - \usepackage{float}
      # - \usepackage{tikz}
      # - \usepackage{subcaption}
    geometry:
      - top=2.5cm
      - bottom=2.5cm
      - left=2.5cm
      - right=2.5cm
    mainfont: Latin Modern Roman
    sansfont: Latin Modern Roman
    linestretch: 2
    fontsize: 12pt
    template-partials: 
      - "before-body.tex"
bibliography: references.bib
link-citations: true
csl: apa.csl
execute:
  message: false
  warning: false
fig-cap-location: top
---

```{r}
#| label: Packages
#| echo: false

# for knitting the document
library(papaja)
library(rlang)
library(rmarkdown)
library(knitr)
# for general data manipulation and plotting
library(tidyverse)
# for estimation
library(lme4)
library(geepack)
# for diagrams
library(dagitty)
library(ggdag)
library(DiagrammeR)
library(magick)
library(pdftools)
```

\newpage

# Introduction

<!-- -   Start with a paragraph describing a problem in the real-life world (so that a BoS member not familiar with statistics understands why you are pursuing research in this direction); -->

Across a wide range of disciplines, researchers analyze clustered longitudinal, observational data to investigate prospective causal relationships between variables. When analyzing such data, the psychological sciences most commonly resort to the multilevel linear model [MLM, @mcneish2017], which---in the context of longitudinal data analysis---separates observed variance into stable between-person differences and within-person fluctuations [@hamaker2020]. Conversely, other fields, such as biostatistics and econometrics often favour generalized estimating equations (GEE) for the analysis of longitudinal data [@mcneish2017]. Despite some cross-disciplinary efforts to compare these methods [@muth2016; @mcneish2017; @yan2013], their scarcity may leave researchers with limited guidance in choosing the most suitable approach for their application.

<!-- -   Then, add a paragraph describing what is known in the literature; -->

Recent evidence has highlighted an issue present in both methods, where controlling for *time-varying endogenous covariates* may lead to biased causal estimates [@qian2020; @pepe1994]. A time-varying covariate is *endogenous* if it is directly or indirectly influenced by prior treatment or outcome, meaning its value may be determined by earlier stages of the process [@qian2020]. As a result of including these covariates in the mentioned models, ordinary interpretations of the coefficients are no longer valid [@qian2020, p. 3]. According to @diggle2002, this issue not only pertains GEE and MLM, but *all* longitudinal data analysis methods.

However, due to a divide between the disciplines that employ these methods, such critiques of the MLM appear to have largely failed to reach the applied researcher in psychology. One specific reason might be that the technical jargon in other disciplines makes it difficult for researchers to recognize when and how these issues emerge[^1]. As a result, researchers may address related problems in disconnected literatures but fail to understand each other. For instance, while the MLM literature emphasizes on the distinction between different centering methods and the effect of cross-level interactions on parameter interpretations [e.g., @hamaker2020], the GEE literature appears to focus more on the marginal and conditional interpretations of model parameters [e.g., @pepe1994].

[^1]: For instance, the term 'endogeneity' in econometrics, while related, has a distinct meaning from that of an endogenous variable, which can lead to confusion.

<!-- -   Also, describe what is not known and which gap you will address in your thesis; End with a clear research question and, if applicable, your hypothesis (for confirmatory research questions – it should be a testable hypothesis) or expectations (for exploratory research questions – can be much vaguer because of the explorative nature of the question). -->

Through a cross-fertilization of these literatures, this project aims to (1) explain the issue of including endogenous covariates in analyses involving GEE, MLM and DSEM (a widely used framework in the social sciences based on MLM) in a psychological context and (2) establish guidelines on how researchers can prevent this issue in their longitudinal data analysis. Accordingly, the following research questions will be addressed: *In which cases do the inclusion of endogenous variables in multilevel linear models and generalized estimating equations result in a discrepancy between marginal and conditional estimates?* In line with the literature [@diggle2002; @qian2020; @pepe1994], we expect that the inclusion of endogenous time-varying covariates in longitudinal data analyses may result in bias that—depending on the circumstances—can promote the potential for faulty inferences. To isolate the issue described in @qian2020, we will focus on the following sub-questions: (1) When removing the interaction $\beta_1$ from generative model 3, is there a difference between the marginal and conditional estimates of the treatment effect? (2) When removing the random slope $b_{i2}$ from generative model 3, is there a difference between the marginal and conditional estimates of the treatment effect?

<!-- It would be interesting to compare the RI-CLPM to the CLPM, as the former is said to be superior in addressing certain confounders, but could it also be more susceptible to time-varying moderators? -->

\newpage

# Methods

To obtain a better understanding of the issue exposed by @qian2020, two methods were employed. First, graphical methods were used provide insight into the presence and extent of bias with potential violation of assumptions: (a) path diagrams were used to evaluate the conditional independence assumption and (b) directed acyclic graphs (DAGs) were used to evaluate the backdoor criterion (Pearl, 1988, 2009). Second, a simulation study was performed to reproduce the results for the generative models (GMs) from @qian2020 and to further isolate the issue using additional GMs.

## Data Generation

In the simulation @qian2020 considered three generative models (GMs), all of which have an endogenous time-varying covariate. In GM1 and GM2, the endogenous covariate $X_{it}$ equals the previous outcome $Y_{it}$ plus some random noise, so the *conditional independence* assumption is valid. In GM3, the endogenous covariate depends directly on $b_{i0}$, violating the assumption. To isolate the issue in GM3, we consider two variations on this model: GM3A, where the random slope $b_{i2}$ for the treatment $A_{it}$ is removed; GM3B, where the interaction term $\beta_1 A_{it} X_{it}$ is removed. Note that the conditional independence assumption is violated in either of these variations. The details of the generative models are described below. We follow the notation of @qian2020 to allow for direct comparison, but rewrite the equations into within- and between-person models [see @raudenbush2002]. We accompany the equations of the GMs with graphical representations, where random effects are represented by grey circles, observed variables by squares and relationships across variables by arrows. The path diagrams of the three data generating models shows the discrepancies between the different generative models---especially concerning the interaction effects---more clearly than DAGs.

<!-- and GM3C, where the fixed effect $\alpha_1$ is removed -->

### Generative Model 1

In GM1, we considered a simple case with only a random intercept and a random slope for $X_{it}$. The outcome is generated according to the following repeated-observations or within-person model (level 1):

$$
Y_{it+1} = \pi_{0i} + \pi_{1i} X_{it} + \pi_{2i} A_{it} + \pi_{3i} A_{it} X_{it} + \epsilon_{it+1}
$$

with the person-level or between-person model (level 2):

$$
\pi_{0i} = \alpha_0 + b_{i0}, \quad b_{i0} \sim \mathcal{N}(0, \sigma_{b0}^2),
$$

$$
\pi_{1i} = \alpha_1,
$$

$$
\pi_{2i} = \beta_0 + b_{i2}, \quad b_{i2} \sim \mathcal{N}(0, \sigma_{b2}^2),
$$

$$
\pi_{3i} = \beta_1.
$$

By substitution, we get the single equation model:

$$
\begin{aligned}
Y_{it+1} &= \pi_{0i} + \pi_{1i} X_{it} + \pi_{2i} A_{it} + \pi_{3i} A_{it} X_{it} + \epsilon_{it+1} \\
&= (\alpha_0 + b_{i0}) + \alpha_1 X_{it} + (\beta_0 + b_{i2}) A_{it} + \beta_1 A_{it} X_{it} + \epsilon_{it+1} \\
&= \alpha_0 + \alpha_1 X_{it} + b_{i0} + A_{it} (\beta_0 + \beta_1 X_{it} + b_{i2}) + \epsilon_{it+1}.
\end{aligned}
$$

The random effects $b_{i0} \sim \mathcal{N}(0, \sigma_{b0}^2)$ and $b_{i2} \sim \mathcal{N}(0, \sigma_{b2}^2)$ are independent of each other. The covariate is generated as $X_{i1} \sim \mathcal{N}(0, 1)$, and for $t \geq 2$,

$$
X_{it} = Y_{it} + \mathcal{N}(0, 1).
$$

The randomization probability $p_t = P(A_{it} = 1 \mid H_{it})$ is constant at $1/2$. Thus, $A_{it} \sim \text{Bernoulli}(0.5)$ for $i = 1, \ldots, N$ and $t = 1, \ldots, T$. The exogenous noise is $\epsilon_{it+1} \sim \mathcal{N}(0, \sigma_\epsilon^2)$.

@fig-GM1_path shows the path diagram for GM1.

```{r}
#| label: fig-GM1_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-cap: "Path diagram for Generative Model 1 ($t = 1, 2, 3$)"
#| eval: true

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, -2) {$X_{1i}$};
  \node (X1var) at (0, -3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, 2) {$A_{1i}$};
  \node[draw, rectangle] (A1X1) at (0.3, 0) {$A_{1i}\cdot X_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, -2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, -3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, 2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, -2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, -3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2X2) at (5.3, 0) {$A_{2i}\cdot X_{2i}$}; % Increased x-coordinate

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate
  \node[draw, circle, fill=gray!20] (b2) at (3, 4) {$b_{i2}$}; % Adjusted to align with middle row

  % Arrows with path coefficients
  \draw [->] (b0) -- node[right] {$+$} (Y2);
  \draw [->] (b0) -- node[right] {$+$} (Y3);
  \draw [->] (A2) edge node[below left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[below] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[below] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (A1X1) edge node[above] {$\beta_1$} (Y2);
  \draw [->] (A2X2) edge node[above left] {$\beta_1$} (Y3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);
  \draw [<->, out=45, in=135, looseness=2] (b2) to node[above] {$\sigma^2_{b2}$} (b2);

  % Additional paths for A and X interaction effects
  \draw[->, dashed] (b2) -- (1.5,1); % Adjusted x-coordinates
  \draw[->, dashed] (b2) -- (5,1.2); % Adjusted x-coordinates

\end{tikzpicture}
```

<!-- ### Generative Model 1A -->

<!-- GM1A is the same as GM1, except that the random slope $b_{i2}$ for the treatment $A_{it}$ is removed. The single equation model thus becomes: -->

<!-- $$ -->

<!-- Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + b_{i0} + A_{it} (\beta_0 + \beta_1 X_{it}) + \epsilon_{it+1}. -->

<!-- $$ -->

<!-- ### Generative Model 1B -->

<!-- GM1B is the same as GM1A, except that the interaction term $\beta_1 A_{it} X_{it}$ is removed. The single equation model thus becomes: -->

<!-- $$ -->

<!-- Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + b_{i0} + \beta_0 A_{it}  + \epsilon_{it+1}. -->

<!-- $$ -->

### Generative Model 2

In GM2, we considered the case with a random intercept and random slopes for (1) covariate $X_{it}$, (2) treatment $A_{it}$, and (3) the interaction between $A_{it}$ and $X_{it}$; and with a time-varying randomization probability for treatment. The outcome is generated according to the same repeated-observations model presented in GM1. However, the person-level model is different:

$$
\pi_{0i} = \alpha_0 + b_{i0}, \quad b_{i0} \sim \mathcal{N}(0, \sigma_{b0}^2),
$$

$$
\pi_{1i} = \alpha_1 + b_{i1}, \quad b_{i1} \sim \mathcal{N}(0, \sigma_{b1}^2),
$$

$$
\pi_{2i} = \beta_0 + b_{i2}, \quad b_{i2} \sim \mathcal{N}(0, \sigma_{b2}^2),
$$

$$
\pi_{3i} = \beta_1 + b_{i3}, \quad b_{i3} \sim \mathcal{N}(0, \sigma_{b3}^2).
$$

By substitution, we get the single equation model:

$$
\begin{aligned}
Y_{it+1} &= \pi_{0i} + \pi_{1i} X_{it} + \pi_{2i} A_{it} + \pi_{3i} A_{it} X_{it} + \epsilon_{it+1} \\ 
&= (\alpha_0 + b_{i0}) + (\alpha_1 + b_{i1}) X_{it} + (\beta_0 + b_{i2}) A_{it} + (\beta_1 + b_{i3}) A_{it} X_{it} + \epsilon_{it+1} \\ 
&= \alpha_0 + \alpha_1 X_{it} + b_{i0} + b_{i1} X_{it} + A_{it} \left( \beta_0 + \beta_1 X_{it} + b_{i2} + b_{i3} X_{it} \right) + \epsilon_{it+1}.
\end{aligned}
$$

The random effects $b_{ij} \sim \mathcal{N}(0, \sigma_{bj}^2)$, for $j = 0, 1, 2, 3$, are independent of each other. The covariate is generated as $X_{i1} \sim \mathcal{N}(0, 1)$, and for $t \geq 2$,

$$
X_{it} = Y_{it} + \mathcal{N}(0, 1).
$$

The randomization probability depends on $X_{it}$:

$$
p_t = P(A_{it} = 1 \mid H_{it}) = 
\begin{cases} 
0.7 & \text{if } X_{it} > -1.27, \\
0.3 & \text{if } X_{it} \leq -1.27,
\end{cases}
$$

where the cutoff $-1.27$ was chosen so that $p_t$ equals 0.7 or 0.3 for about half of the time. In other words, if the value of the covariate for any given person and time point is above the cutoff, the probability of receiving the treatment $p_t$ is 0.7; otherwise, it is 0.3. Accordingly, $A_{it} \sim \text{Bernoulli}(p_t)$ for $i = 1, \ldots, N$ and $t = 1, \ldots, T$. The exogenous noise is $\epsilon_{it+1} \sim \mathcal{N}(0, \sigma_\epsilon^2)$.

@fig-GM2_path shows the path diagram for GM2. ADD THE RANDOM INTERACTIN SLOPE BI3!!!!!

```{r}
#| label: fig-GM2_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-cap: "Path diagram for Generative Model 2 ($t = 1, 2, 3$)"
#| eval: true

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, -2) {$X_{1i}$};
  \node (X1var) at (0, -3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, 2) {$A_{1i}$};
  \node[draw, rectangle] (A1X1) at (0.3, 0) {$A_{1i}\cdot X_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, -2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, -3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, 2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, -2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, -3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2X2) at (5.3, 0) {$A_{2i}\cdot X_{2i}$}; % Increased x-coordinate

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate
  \node[draw, circle, fill=gray!20] (b2) at (3, 4) {$b_{i2}$}; % Adjusted to align with middle row

  % Arrows with path coefficients
  \draw [->] (b0) -- node[right] {$+$} (Y2);
  \draw [->] (b0) -- node[right] {$+$} (Y3);
  \draw [->] (A2) edge node[below left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[below] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[below] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (A1X1) edge node[above] {$\beta_1$} (Y2);
  \draw [->] (A2X2) edge node[above left] {$\beta_1$} (Y3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);

  \draw [->, bend left=40] (X1) to (A1); 
  \draw [->, bend left=40] (X2) to (A2);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);
  \draw [<->, out=45, in=135, looseness=2] (b2) to node[above] {$\sigma^2_{b2}$} (b2);

  % Additional paths for A and X interaction effects
  \draw[->, dashed] (b2) -- (1.5,1); % Adjusted x-coordinates
  \draw[->, dashed] (b2) -- (5,1.2); % Adjusted x-coordinates

\end{tikzpicture}
```



<!-- ### Generative Model 2A -->

<!-- GM2A is the same as GM2, except that the random slopes $b_{i1}$, $b_{i2}$ and $b_{i3}$ are removed. The single equation model then becomes the same as GM1A, but with the time-varying randomization probabilities of GM2. -->

<!-- ### Generative Model 2B -->

<!-- GM2B is the same as GM2A, except that the interaction term $\beta_1 A_{it} X_{it}$ is removed. The single equation model then becomes the same as GM1B, but with the time-varying randomization probabilities of GM2. -->

### Generative Model 3

GM3 is the same as GM1, except that the covariate $X_{it}$ depends directly on $b_{i0}$:

$$
X_{i1} \sim \mathcal{N}(b_{i0}, 1), \quad X_{it} = Y_{it} + \mathcal{N}(b_{i0}, 1) \text{ for } t \geq 2.
$$

@fig-GM3_path shows the path diagram for GM3.

```{r}
#| label: fig-GM3_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-cap: "Path diagram for Generative Model 3 ($t = 1, 2, 3$)"
#| eval: true

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, -2) {$X_{1i}$};
  \node (X1var) at (0, -3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, 2) {$A_{1i}$};
  \node[draw, rectangle] (A1X1) at (0.3, 0) {$A_{1i}\cdot X_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, -2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, -3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, 2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, -2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, -3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2X2) at (5.3, 0) {$A_{2i}\cdot X_{2i}$}; % Increased x-coordinate

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate
  \node[draw, circle, fill=gray!20] (b2) at (3, 4) {$b_{i2}$}; % Adjusted to align with middle row

  % Arrows with path coefficients
  \draw [->] (b0) -- node[right] {$+$} (Y2);
  \draw [->] (b0) -- node[right] {$+$} (Y3);
  \draw [->] (A2) edge node[below left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[below] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[below] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (A1X1) edge node[above] {$\beta_1$} (Y2);
  \draw [->] (A2X2) edge node[above left] {$\beta_1$} (Y3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);
  \draw [->] (b0) edge (X1);
  \draw [->] (b0) edge (X2);
  \draw [->] (b0) edge (X3);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);
  \draw [<->, out=45, in=135, looseness=2] (b2) to node[above] {$\sigma^2_{b2}$} (b2);

  % Additional paths for A and X interaction effects
  \draw[->, dashed] (b2) -- (1.5,1); % Adjusted x-coordinates
  \draw[->, dashed] (b2) -- (5,1.2); % Adjusted x-coordinates

\end{tikzpicture}
```

### Generative Model 3A

GM3A is the same as GM3, except that the random slope $b_{i2}$ for the treatment $A_{it}$ is removed. The single equation model then becomes:

$$
Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + b_{i0} + A_{it} (\beta_0 + \beta_1 X_{it}) + \epsilon_{it+1}.
$$

```{r}
#| label: fig-GM3A_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-cap: "Path diagram for Generative Model 3A ($t = 1, 2, 3$)"
#| eval: true

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, -2) {$X_{1i}$};
  \node (X1var) at (0, -3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, 2) {$A_{1i}$};
  \node[draw, rectangle] (A1X1) at (0.3, 0) {$A_{1i}\cdot X_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, -2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, -3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, 2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, -2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, -3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2X2) at (5.3, 0) {$A_{2i}\cdot X_{2i}$}; % Increased x-coordinate

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate

  % Arrows with path coefficients
  \draw [->] (b0) -- node[right] {$+$} (Y2);
  \draw [->] (b0) -- node[right] {$+$} (Y3);
  \draw [->] (A2) edge node[below left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[below] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[below] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (A1X1) edge node[above] {$\beta_1$} (Y2);
  \draw [->] (A2X2) edge node[above left] {$\beta_1$} (Y3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);
  \draw [->] (b0) edge (X1);
  \draw [->] (b0) edge (X2);
  \draw [->] (b0) edge (X3);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);

\end{tikzpicture}
```

### Generative Model 3B

GM3B is the same as GM3, except that the interaction term $\beta_1 A_{it} X_{it}$ is removed. The single equation model then becomes:

$$
Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + b_{i0} + A_{it} (\beta_0 + b_{i2}) + \epsilon_{it+1}.
$$

```{r}
#| label: fig-GM3B_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-cap: "Path diagram for Generative Model 3B ($t = 1, 2, 3$)"
#| eval: true

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, -2) {$X_{1i}$};
  \node (X1var) at (0, -3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, 2) {$A_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, -2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, -3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, 2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, -2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, -3) {$\sigma^2_{X}$};

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate
  \node[draw, circle, fill=gray!20] (b2) at (3, 4) {$b_{i2}$}; % Adjusted to align with middle row

  % Arrows with path coefficients
  \draw [->] (b0) -- node[right] {$+$} (Y2);
  \draw [->] (b0) -- node[right] {$+$} (Y3);
  \draw [->] (A2) edge node[below left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[below] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[below] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);
  \draw [->] (b0) edge (X1);
  \draw [->] (b0) edge (X2);
  \draw [->] (b0) edge (X3);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);
  \draw [<->, out=45, in=135, looseness=2] (b2) to node[above] {$\sigma^2_{b2}$} (b2);

  % Additional paths for A and X interaction effects
  \draw[->, dashed] (b2) -- (1.5,1); % Adjusted x-coordinates
  \draw[->, dashed] (b2) -- (5,1.2); % Adjusted x-coordinates

\end{tikzpicture}
```

<!-- ### Generative Model 3C -->

<!-- GM3C is the same as GM3, except that the fixed slope $\alpha_1$ for the covariate $X_{it}$ is removed. The single equation model then becomes: -->

<!-- $$ -->

<!-- Y_{it+1} = \alpha_0 + b_{i0} + A_{it} (\beta_0 + \beta_1 X_{it} + b_{i2}) + \epsilon_{it+1}. -->

<!-- $$ -->

### Parameter Values

The following parameter values were adapted from @qian2020:

$$
\alpha_0 = -2, \quad \alpha_1 = -0.3, \quad \beta_0 = 1, \quad \beta_1 = 0.3,
$$

$$
\sigma_{b0}^2 = 4, \quad \sigma_{b1}^2 = \frac{1}{4}, \quad \sigma_{b2}^2 = 1, \quad \sigma_{b3}^2 = \frac{1}{4}, \quad \sigma_\epsilon^2 = 1.
$$

## Path Diagrams and Conditional Independence

@qian2020 proposes the use of the conditional independence assumption to identify whether bias may occur, which is given by:

$$ X_{it} \perp (b_{i0}, b_{i1}) \mid H_{it-1}, A_{it-1}, Y_{it}. $$

This allows $X_{it}$ to be endogenous, but the endogenous covariate $X_{it}$ can only depend on the random effects through variables observed prior to $X_{it}$. If the only endogenous covariates are functions of prior treatments and prior outcomes, then the assumption automatically holds.

When inspecting @fig-GM1_path and @fig-GM2_path, we may notice that $X_{it}$ becomes independent of the random effects after conditioning on $Y_{it}$. On the other hand, we can see that this assumption is violated in GM3/GM3A/GM3B, as $X_{it}$ depends directly on $b_{i0}$ and can thus not be made independent of the random effects by conditioning on prior variables such as $Y_{it}$ (see @fig-GM3_path, @fig-GM3A_path and @fig-GM3B_path)

## Backdoor Criterion and DAGs

DAGs are a useful tool for representing causal relationships between variables and to evaluate the assumptions needed for causal identification. According to the backdoor criterion [@pearl1988; @pearl2009], a requirement for causal identification, causal effects can be identified by blocking non-causal paths through conditioning on intermediate variables (e.g., controlling or matching). If any non-causal paths cannot be blocked due to omitted variables or measurement error, treatment and outcome remain linked via backdoor paths, leading to biased estimates of the treatment effect [@Kim2021a].

We formulated the DAGs in `dagitty`, where the random disturbance $b_{0i}$ was represented by the node U [e.g., @Kim2021a]. The DAGs for the first three observations of the three data generating models are presented in @fig-DAGs. The red arrows show the biased paths after controlling for the covariate $X_{it}$.

::: {#fig-DAGs layout-ncol="2"}
```{r}
#| label: fig-GM1_DAG
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM 1"
#| eval: true

\begin{tikzpicture}[node distance=1cm]
  % Nodes for observed variables
  \node (X1) at (0, 2) {$X_{1}$};
  \node (A1) at (0, -2) {$A_{1}$};
  \node (Y2) at (3, 0) {$Y_{2}$};
  \node (X2) at (3, 2) {$X_{2}$};
  \node (A2) at (3, -2) {$A_{2}$};
  \node (Y3) at (6, 0) {$Y_{3}$};
  \node (X3) at (6, 2) {$X_{3}$};
  \node (U) at (4.5, 3) {$U$};

  % Directed edges
  \draw [->] (U) -- (Y2);
  \draw [->] (U) -- (Y3);
  \draw [->] (A2) -- (Y3);
  \draw [->] (X2) -- (Y3);
  \draw [->] (Y2) -- (X2);
  \draw [->] (A1) -- (Y2);
  \draw [->] (X1) -- (Y2);
  \draw [->] (Y3) -- (X3);
\end{tikzpicture}
```

```{r}
#| label: fig-GM2_DAG
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM 2"
#| eval: true

\begin{tikzpicture}[node distance=1cm]
  % Nodes for observed variables
  \node (X1) at (0, 2) {$X_{1}$};
  \node (A1) at (0, -2) {$A_{1}$};
  \node (Y2) at (3, 0) {$Y_{2}$};
  \node (X2) at (3, 2) {$X_{2}$};
  \node (A2) at (3, -2) {$A_{2}$};
  \node (Y3) at (6, 0) {$Y_{3}$};
  \node (X3) at (6, 2) {$X_{3}$};
  \node (U) at (4.5, 3) {$U$};

  % Directed edges
  \draw [->] (U) -- (Y2);
  \draw [->] (U) -- (Y3);
  \draw [->, red] (A2) -- (Y3);
  \draw [->] (X2) -- (Y3);
  \draw [->] (Y2) -- (X2);
  \draw [->, red] (A1) -- (Y2);
  \draw [->] (X1) -- (Y2);
  \draw [->] (Y3) -- (X3);

  % Curved edges for X1 -> A1 and X2 -> A2
  \draw [->, bend left=40, red] (X1) to (A1);
  \draw [->, bend left=40, red] (X2) to (A2);
\end{tikzpicture}
```

```{r}
#| label: fig-GM3_DAG
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM 3, 3A, 3B"
#| eval: true

\begin{tikzpicture}[node distance=1cm]
  % Nodes for observed variables
  \node (X1) at (0, 2) {$X_{1}$};
  \node (A1) at (0, -2) {$A_{1}$};
  \node (Y2) at (3, 0) {$Y_{2}$};
  \node (X2) at (3, 2) {$X_{2}$};
  \node (A2) at (3, -2) {$A_{2}$};
  \node (Y3) at (6, 0) {$Y_{3}$};
  \node (X3) at (6, 2) {$X_{3}$};
  \node (U) at (4.5, 3) {$U$};

  % Directed edges
  \draw [->] (U) -- (Y2);
  \draw [->] (U) -- (Y3);
  \draw [->] (U) -- (X1);
  \draw [->] (U) -- (X2);
  \draw [->] (U) -- (X3);
  \draw [->] (A1) -- (Y2);
  \draw [->] (X1) -- (Y2);
  \draw [->] (Y2) -- (X2);
  \draw [->] (A2) -- (Y3);
  \draw [->] (X2) -- (Y3);
  \draw [->] (Y3) -- (X3);
\end{tikzpicture}
```

*Note.* The red arrows show the biased backdoor path(s) in the treatment efffect.

DAGs for Generative Models 1, 2, 3/3A/3B (t = 1, 2, 3)
:::

When applying Pearl's backdoor criterion to GM1/3/3A/3B, it may be observed that there exists no backdoor path in the treatment effect $A_{it} \to Y_{it+1}$, as $A_{it}$ does not have any parents. While we need not control for covariate $X_{it}$ to obtain an unbiased total effect, doing so should not introduce bias.

<!-- In GM1, upon controlling for the covariate $X_{it}$, we block the mediation pathway in $Y_{it} \rightarrow Y_{it+1}$, which would otherwise lead to a biased treatment effect, through the addition of the random disturbance U.  -->

On the other hand, in GM2, there is a backdoor path in the treatment effect: $A_{it} \leftarrow X_{it} \rightarrow Y_{it+1}$ (see @fig-GM2_DAG). More specifically, $X_{it}$ is a confounder in the relationship between $A_{it}$ and $Y_{it+1}$. However, controlling for $X_{it}$ blocks this backdoor path, making the treatment effect unbiased. In other words, the history of covariate $X_{it}$ is a sufficient adjustment set for the treatment effect.

The results of @qian2020 show that GM3 is the only model with bias in the treatment effect. However, the backdoor criterion failed to identify this bias, as there is no backdoor path in the treatment effect. This may be explained by the fact that the DAG does not pose assumptions on (a) the random slopes and (b) interaction effects. Concerns regarding the use of Pearl's backdoor criterion in situations with interaction effects have been voiced by several people (see @weinberg2007; @attia2022).

<!-- Visually, we may notice from @fig-GM3_DAG that when we control for $X_{it}$ in GM3, we block the backdoor path from $X_{ti}$ to $Y_{it+1}$ through $X_{it}$, which is the path that would be biased if we did not control for $X_{it}$. -->

<!-- Alternatively, we can display the data generating models as a path diagram, where latent variables are represented by circles, observed variables by squares and relationships across variables by arrows. The path diagrams of the three data generating models is presented in @fig-pathdiagrams (GM1 in @fig-GM1_pd, GM2 in @fig-GM2_pd, and GM3 in @fig-GM3_pd), which shows the discrepancies between the different generative models more clearly than the DAGs. -->

<!-- SEE TEXT -->

<!-- We can make a couple observations from this path diagram: -->

<!-- -   Contrary to the DAG, this path diagram shows the moderation effect (1) of $X_{it}$ on the relationship between $X_{ti}$ and $Y_{it+1}$ and (2) of $u_{2i}$ on the relationship between $X_{it}$ and $Y_{it+1}$. -->

<!-- -   Similar to the example without treatment in section 2.2, the covariate $X_{it}$ is determined by the previous value of the outcome $Y_{ti}$---which makes it an endogenous time-varying covariate. -->

<!-- -   The path diagram does not display the difference in the randomized treatment assignment probabilities between GM1 and GM2. -->

## Data Analysis

We evaluated the performance of the models across a total of 30 different settings, each replicated 1,000 times, by systematically varying the following factors:

-   **Generative Models (GM):** 1, 2, 3, 3A, 3B

-   **Number of timepoints (T):** 10, 30

-   **Sample size (N):** 30, 100, 200

All data generation and estimation was performed in `R`, version 4.4.2 [@rcoreteam2024]. To fit the standard MLM, the `lmer` function from the R-package `lme4` [@bates2015] was employed with restricted maximum likelihood estimation. For the MLM, the analytical models were equivalent to each of the respective data-generating models. To fit the GEE with the "exchangeable", "independent" and "AR(1)" working correlation structures, the `geeglm` function from the R-package `geepack` [@halekoh2006] was employed with the identity link function. Since the random effects are not explicitly modelled in GEE, the analytical GEE models simply contain only the fixed effects of the generative model at hand.

<!-- the specification of the analytical GEE models is different from their MLM counterparts. More specifically, the analytical models simply contain only the fixed effects of the generative model hand.  -->

<!-- Since the fixed effects modeled in GM1, GM2, GM3, GM3a are the same (the only differences pertains to the modeling of random effects), the analytical *GEE model* is identical across these conditions: -->

<!-- $$ -->

<!-- Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + \beta_0 A_{it} + \beta_1 A_{it} X_{it} + \epsilon_{it+1}. -->

<!-- $$ -->

<!-- In GM3b, the fixed interaction effect is removed, so the analytical *GEE model* is given by: -->

<!-- $$ -->

<!-- Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + \beta_0 A_{it} + \epsilon_{it+1}. -->

<!-- $$ -->

<!-- As a reminder, the analytical *multilevel model* for GM1 and GM3 is given by: -->

<!-- $$ -->

<!-- Y_{it+1} = (\alpha_0 + b_{i0}) + \alpha_1 X_{it} + (\beta_0 + b_{i2}) A_{it} + \beta_1 A_{it} X_{it} + \epsilon_{it+1}. -->

<!-- $$ -->

<!-- which is fitted as `lmer(Y ~ X * A + (1 + A | id), data = data)` in R. -->

<!-- The analytical *multilevel model* for GM2 is given by: -->

<!-- $$ -->

<!-- Y_{it+1} = (\alpha_0 + b_{i0}) + (\alpha_1 + b_{i1}) X_{it} + (\beta_0 + b_{i2}) A_{it} + (\beta_1 + b_{i3}) A_{it} X_{it} + \epsilon_{it+1}. -->

<!-- $$ -->

<!-- which is fitted as `lmer(Y ~ X * A + (X * A | id), data = data)` in R. -->

<!-- The analytical *multilevel model* for GM1A, GM2A, and GM3A is given by: -->

<!-- $$ -->

<!-- Y_{it+1} = \alpha_0 + b_{i0} + \alpha_1 X_{it} + \beta_0 A_{it} + \beta_1 A_{it} X_{it} + \epsilon_{it+1}. -->

<!-- $$ -->

<!-- which is fitted as `lmer(Y ~ X * A + (1 | id), data = data)` in R. -->

<!-- The analytical *multilevel model* for GM1B, GM2B, and GM3B is given by: -->

<!-- $$ -->

<!-- Y_{it+1} = \alpha_0 + b_{i0} + \alpha_1 X_{it} + \beta_0 A_{it} + \epsilon_{it+1}. -->

<!-- $$ -->

<!-- which is fitted as `lmer(Y ~ X + A + (1 | id), data = data)` in R. -->

<!-- The specification of the GEE models related to each of the generative models is unsurprisingly different considering that GEE does not explicitly model random effects. For each of the generative models, we will fit three GEE models: one with an exchangeable correlation structure, one with an independent correlation structure, and one with an AR(1) correlation structure. The GEE models were fitted using the `geeglm` function in R. Since the fixed effects modeled in GM1, GM1a, GM2, GM2a, GM3, GM3a are the same (the only differences pertain to the modeling of random effects), the analytical *GEE model* is identical across these three conditions. The analytical *GEE model* is given by: -->

<!-- $$ -->

<!-- Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + \beta_0 A_{it} + \beta_1 A_{it} X_{it} + \epsilon_{it+1}. -->

<!-- $$ -->

<!-- The GEE models were fitted as `geeglm(Y ~ X * A, id = id, data = data, family = gaussian, corstr = "exchangeable")`, `geeglm(Y ~ X * A, id = id, data = data, family = gaussian, corstr = "independence")`, and `geeglm(Y ~ X * A, id = id, data = data, family = gaussian, corstr = "ar1")` in R. -->

<!-- In GM1b, GM2b, GM3b, the fixed interaction effect is removed, so the analytical *GEE model* is given by: -->

<!-- $$ -->

<!-- Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + \beta_0 A_{it} + \epsilon_{it+1}. -->

<!-- $$ -->

<!-- The GEE models were fitted as `geeglm(Y ~ X + A, id = id, data = data, family = gaussian, corstr = "exchangeable")`, `geeglm(Y ~ X + A, id = id, data = data, family = gaussian, corstr = "independence")`, and `geeglm(Y ~ X + A, id = id, data = data, family = gaussian, corstr = "ar1")` in R. -->

# Results

As shown in Table X, GM3 results in bias

```{=tex}
\begin{table}[H]
    \caption{Simulation results for $N=200$ over $1000$ replications, with $T=10$ and $T=30$.}
    \centering
    \begin{tabular}{@{}p{1.5cm} p{5cm} cc cc@{}}
        \toprule
        GM & Characteristics & \multicolumn{2}{c}{$\hat{\beta}_{0,MLM}-\beta_{0,MLM}$} & \multicolumn{2}{c}{$\hat{\beta}_{0,GEE-ind}-\beta_{0,MLM}$} \\ 
        \cmidrule(lr){3-4} \cmidrule(lr){5-6}
           &                 & \( T = 10 \) & \( T = 30 \) & \( T = 10 \) & \( T = 30 \) \\ \midrule
        1  & Includes random intercept and random slope for treatment & 0.003 & 0.001 & 0.086 & 0.090 \\
        3  & Model 1 with dependency random intercept and covariate       & -0.051 & -0.023 & 0.033 & 0.032 \\
        3a & Model 3 without random slope $b_{2i}$           & 0.002 & -0.000 & -0.001 & -0.000 \\
        3b & Model 3 without interaction effect $\beta_1$ (between treatment and covariate) & 0.005 & 0.001 & 0.003 & 0.001 \\
        \bottomrule
    \end{tabular}
\end{table}
```
# Discussion

Despite a violation of the conditional independence assumption in GM 3, 3A and 3B; biased MLM estimates were only found for GM3.

\newpage

# References

::: {#refs}
:::

<!-- --- -->

<!-- nocite: | -->

<!--   @* -->

<!-- --- -->

# Appendix

## Original Section from Qian et al. (2020): "4. Simulation"

In the simulation, we considered three generative models (GMs), all of which have an endogenous covariate. In the first two GMs, the endogenous covariate $X_{it}$ equals the previous outcome $Y_{it}$ plus some random noise, so the conditional independence assumption (10) is valid. In GM 3, the endogenous covariate depends directly on $b_i$, violating assumption (10). The details of the generative models are described below.

In GM1, we considered a simple case with only a random intercept and a random slope for $A_{it}$, so that $Z_{i(t_0)} = Z_{i(t_2)} = 1$ in model (7). The outcome is generated as:

$$
Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + b_{i0} + A_{it} (\beta_0 + \beta_1 X_{it} + b_{i2}) + \epsilon_{it+1}.
$$

The random effects $b_{i0} \sim N(0, \sigma_{b0}^2)$ and $b_{i2} \sim N(0, \sigma_{b2}^2)$ are independent of each other. The covariate is generated as $X_{i1} \sim N(0, 1)$, and for $t \geq 2$,

$$
X_{it} = Y_{it} + N(0, 1).
$$

The randomization probability $p_t$ is constant at $1/2$. The exogenous noise is $\epsilon_{it+1} \sim N(0, \sigma_\epsilon^2)$.

In GM2, we considered the case where $Z_{i(t_0)} = Z_{i(t_2)} = 1$, with time-varying randomization probability. The outcome is generated as:

$$
Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + b_{i0} + b_{i1} X_{it} + A_{it} (\beta_0 + \beta_1 X_{it} + b_{i2} + b_{i3} X_{it}) + \epsilon_{it+1}.
$$

The random effects $b_{ij} \sim N(0, \sigma_{b_j}^2)$, for $0 \leq j \leq 3$, are independent of each other. The covariate is generated as $X_{i1} \sim N(0, 1)$, and for $t \geq 2$,

$$
X_{it} = Y_{it} + N(0, 1).
$$

The randomization probability depends on $X_{it}$:

$$
p_t = 0.7 \cdot 1(X_{it} > -1.27) + 0.3 \cdot 1(X_{it} \leq -1.27),
$$

where $1(\cdot)$ represents the indicator function, and the cutoff $-1.27$ was chosen so that $p_t$ equals 0.7 or 0.3 for about half of the time. The exogenous noise is $\epsilon_{it+1} \sim N(0, \sigma_\epsilon^2)$.

GM3 is the same as GM 1, except that the covariate $X_{it}$ depends directly on $b_i$:

$$
X_{i1} \sim N(b_{i0}, 1), \quad X_{it} = Y_{it} + N(b_{i0}, 1) \text{ for } t \geq 2.
$$

We chose the following parameter values:

$$
\alpha_0 = -2, \quad \alpha_1 = -0.3, \quad \beta_0 = 1, \quad \beta_1 = 0.3,
$$

$$
\sigma_{b0}^2 = 4, \quad \sigma_{b1}^2 = \frac{1}{4}, \quad \sigma_{b2}^2 = 1, \quad \sigma_{b3}^2 = \frac{1}{4}, \quad \sigma_\epsilon^2 = 1.
$$

\newpage

## Overview of Variations on Generative Model 3

|                  |                                 |                      |                                  |               |
|---------------|---------------|---------------|---------------|---------------|
| Generative Model | random slope treatment $b_{i2}$ | interactie $\beta_1$ | fixed slope covariate $\alpha_1$ | bias          |
| 3                | yes                             | yes                  | yes                              | yes, negative |
| 3a               | no                              | yes                  | yes                              | no            |
| 3d               | yes                             | no                   | yes                              | no            |
| 3h               | yes                             | yes                  | no                               | yes, positive |

: Models with 1 Parameter Less

|                  |                                 |                      |                                  |               |
|---------------|---------------|---------------|---------------|---------------|
| Generative Model | random slope treatment $b_{i2}$ | interactie $\beta_1$ | fixed slope covariate $\alpha_1$ | bias          |
| 3                | yes                             | yes                  | yes                              | yes, negative |
| 3b               | no                              | no                   | yes                              | no            |
| 3i               | no                              | yes                  | no                               |               |
| 3j               | yes                             | no                   | no                               |               |

## Simulation Plan Proposal

To uncover the undesirable effects of endogenous covariates and investigate robustness against these effects, we will carry out simulations in which data will be generated according to several increasingly complex scenarios. These scenarios will be visually represented using directed acyclic graphs and analyzed using GEE, MLM and DSEM. We will start out with a scenario of the basic MLM---where a time-varying outcome $Y$ is regressed on a single time-varying predictor $X$ and in the presence of stable between person differences in the intercept---and increase the complexity until we reach the scenario that includes a time-varying endogenous covariate. The primary interest of this simulation study is the comparative performance of different specifications of the MLM and GEE in terms of bias in the estimation of the effect of $X$ to $Y$. The secondary interest is the efficiency in mean squared error (MSE). We consider settings with timepoints $T = 10,30$ and sample size $N = 30, 100, 200$.

Statistical analyses pertaining to the GEE and basic MLM will be performed in `R`, version 4.4.2 [@rcoreteam2024]. To fit the GEE, the R-package `geepack` [@halekoh2006] will evaluate several different working correlation structures, including independent, exchangeable, AR(1) and unstructured. To fit the basic MLM, the R-package `lme4` [@bates2015] will be employed, where we will use restricted maximum likelihood estimation.

## Trash

```{r}
#| label: fig-DAGs
#| fig-cap: "DAG for Generative Models"
#| layout-ncol: 2
#| fig-subcap:
#|  - "Generative Model 1"
#|  - "Generative Model 2"
#|  - "Generative Model 3, 3A and 3B"
#| echo: false
#| eval: false
#| cache: true

library(dagitty)

GM1_DAG <- dagitty('dag {
bb="0,0,1,1"
U [latent,pos="0.500,0.200"]
A_1 [exposure,pos="0.200,0.600"]
A_2 [exposure,pos="0.400,0.600"]
Y_2 [outcome,pos="0.450,0.450"]
Y_3 [outcome,pos="0.650,0.450"]
X_1 [adjusted,pos="0.200,0.300"]
X_2 [adjusted,pos="0.400,0.300"]
X_3 [pos="0.600,0.300"]
U -> Y_2
U -> Y_3
A_1 -> Y_2
A_2 -> Y_3
Y_2 -> X_2
Y_3 -> X_3
X_1 -> Y_2
X_2 -> Y_3
}')

GM2_DAG <- dagitty('dag {
bb="0,0,1,1"
U [latent,pos="0.500,0.200"]
A_1 [exposure,pos="0.200,0.600"]
A_2 [exposure,pos="0.400,0.600"]
Y_2 [outcome,pos="0.450,0.450"]
Y_3 [outcome,pos="0.650,0.450"]
X_1 [adjusted,pos="0.200,0.300"]
X_2 [adjusted,pos="0.400,0.300"]
X_3 [pos="0.600,0.300"]
U -> Y_2
U -> Y_3
A_1 -> Y_2
A_2 -> Y_3
Y_2 -> X_2
Y_3 -> X_3
X_1 -> A_1
X_1 -> Y_2
X_2 -> A_2
X_2 -> Y_3
}
')

GM3ab_DAG <- dagitty('dag {
bb="0,0,1,1"
U [latent,pos="0.500,0.200"]
A_1 [exposure,pos="0.200,0.600"]
A_2 [exposure,pos="0.400,0.600"]
Y_2 [outcome,pos="0.450,0.450"]
Y_3 [outcome,pos="0.650,0.450"]
X_1 [adjusted,pos="0.200,0.300"]
X_2 [adjusted,pos="0.400,0.300"]
X_3 [pos="0.600,0.300"]
U -> Y_2
U -> Y_3
U -> X_1
U -> X_2
U -> X_3
A_1 -> Y_2
A_2 -> Y_3
Y_2 -> X_2
Y_3 -> X_3
X_1 -> Y_2
X_2 -> Y_3
}')

ggdag::ggdag_status(GM1_DAG) + ggdag::theme_dag()
ggdag::ggdag_status(GM2_DAG) + ggdag::theme_dag()
ggdag::ggdag_status(GM3ab_DAG) + ggdag::theme_dag()
```

<!-- ::: {layout-ncol=2} -->

<!-- ```{r} -->

<!-- #| label: fig-GM1_DAG -->

<!-- #| engine: 'tikz' -->

<!-- #| echo: false -->

<!-- #| cache: true -->

<!-- #| fig-cap: "GM 1" -->

<!-- #| eval: true -->

<!-- \begin{tikzpicture} -->

<!--   % Nodes for observed variables (squares) -->

<!--   \node[draw, rectangle] (X1) at (0, -2) {$X_{1}$}; -->

<!--   \node[draw, rectangle] (A1) at (0, 2) {$A_{1}$}; -->

<!--   \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2}$}; -->

<!--   \node[draw, rectangle] (X2) at (3, -2) {$X_{2}$}; -->

<!--   \node[draw, rectangle] (A2) at (3, 2) {$A_{2}$}; -->

<!--   \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3}$}; -->

<!--   \node[draw, rectangle] (X3) at (8, -2) {$X_{3}$}; -->

<!--   % Node for latent variable (circle) -->

<!--   \node[draw, circle, fill=gray!20] (U) at (6, 4) {$U$}; -->

<!--   % Directed edges -->

<!--   \draw [->] (U) -- (Y2); -->

<!--   \draw [->] (U) -- (Y3); -->

<!--   \draw [->] (A2) -- (Y3); -->

<!--   \draw [->] (X2) -- (Y3); -->

<!--   \draw [->] (Y2) -- (X2); -->

<!--   \draw [->] (A1) -- (Y2); -->

<!--   \draw [->] (X1) -- (Y2); -->

<!--   \draw [->] (Y3) -- (X3); -->

<!-- \end{tikzpicture} -->

<!-- ``` -->

<!-- ```{r} -->

<!-- #| label: fig-GM2_DAG -->

<!-- #| engine: 'tikz' -->

<!-- #| echo: false -->

<!-- #| cache: true -->

<!-- #| fig-cap: "GM 2 ($t = 1, 2, 3$)" -->

<!-- #| eval: true -->

<!-- \begin{tikzpicture} -->

<!--   % Nodes for observed variables (squares) -->

<!--   \node[draw, rectangle] (X1) at (0, -2) {$X_{1}$}; -->

<!--   \node[draw, rectangle] (A1) at (0, 2) {$A_{1}$}; -->

<!--   \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2}$}; -->

<!--   \node[draw, rectangle] (X2) at (3, -2) {$X_{2}$}; -->

<!--   \node[draw, rectangle] (A2) at (3, 2) {$A_{2}$}; -->

<!--   \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3}$}; -->

<!--   \node[draw, rectangle] (X3) at (8, -2) {$X_{3}$}; -->

<!--   % Node for latent variable (circle) -->

<!--   \node[draw, circle, fill=gray!20] (U) at (6, 4) {$U$}; -->

<!--   % Directed edges -->

<!--   \draw [->] (U) -- (Y2); -->

<!--   \draw [->] (U) -- (Y3); -->

<!--   \draw [->] (A2) -- (Y3); -->

<!--   \draw [->] (X2) -- (Y3); -->

<!--   \draw [->] (Y2) -- (X2); -->

<!--   \draw [->] (A1) -- (Y2); -->

<!--   \draw [->] (X1) -- (Y2); -->

<!--   \draw [->] (Y3) -- (X3); -->

<!--   % Curved edges for X1 -> A1 and X2 -> A2 -->

<!--   \draw [->, bend left=40] (X1) to (A1); -->

<!--   \draw [->, bend left=40] (X2) to (A2); -->

<!-- \end{tikzpicture} -->

<!-- ``` -->

<!-- ```{r} -->

<!-- #| label: fig-GM3_DAG -->

<!-- #| engine: 'tikz' -->

<!-- #| echo: false -->

<!-- #| cache: true -->

<!-- #| fig-cap: "GM 3, 3A, 3B" -->

<!-- #| eval: true -->

<!-- \begin{tikzpicture} -->

<!--   % Nodes for observed variables -->

<!--   \node[draw, rectangle] (X1) at (0, -2) {$X_1$}; -->

<!--   \node[draw, rectangle] (A1) at (0, 2) {$A_1$}; -->

<!--   \node[draw, rectangle] (Y2) at (3, 0) {$Y_2$}; -->

<!--   \node[draw, rectangle] (X2) at (3, -2) {$X_2$}; -->

<!--   \node[draw, rectangle] (A2) at (3, 2) {$A_2$}; -->

<!--   \node[draw, rectangle] (Y3) at (8, 0) {$Y_3$}; -->

<!--   \node[draw, rectangle] (X3) at (8, -2) {$X_3$}; -->

<!--   % Latent variable -->

<!--   \node[draw, circle, fill=gray!20] (U) at (6, 4) {$U$}; -->

<!--   % Edges -->

<!--   \draw [->] (U) -- (Y2); -->

<!--   \draw [->] (U) -- (Y3); -->

<!--   \draw [->] (U) -- (X1); -->

<!--   \draw [->] (U) -- (X2); -->

<!--   \draw [->] (U) -- (X3); -->

<!--   \draw [->] (A1) -- (Y2); -->

<!--   \draw [->] (X1) -- (Y2); -->

<!--   \draw [->] (Y2) -- (X2); -->

<!--   \draw [->] (A2) -- (Y3); -->

<!--   \draw [->] (X2) -- (Y3); -->

<!--   \draw [->] (Y3) -- (X3); -->

<!-- \end{tikzpicture} -->

<!-- ``` -->

<!-- DAGs for Generative Models 1, 2, and 3.  -->

<!-- The red arrows indicate the biased paths after controlling for the covariate $X_{it}$. -->

<!-- ::: -->

<!-- ```{r} -->

<!-- #| label: fig-GM_DAG -->

<!-- #| engine: 'tikz' -->

<!-- #| echo: false -->

<!-- #| cache: true -->

<!-- #| fig-cap: "DAGs for Generative Models 1, 2, and 3.  ($t = 1, 2, 3$)" -->

<!-- #| ncols: 2 -->

<!-- #| fig-subcap:  -->

<!-- #|  - "GM1" -->

<!-- #|  - "GM2" -->

<!-- #|  - "GM3" -->

<!-- #| eval: true -->

<!-- \begin{tikzpicture} -->

<!--   % Nodes for observed variables (squares) -->

<!--   \node[draw, rectangle] (X1) at (0, -2) {$X_{1}$}; -->

<!--   \node[draw, rectangle] (A1) at (0, 2) {$A_{1}$}; -->

<!--   \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2}$}; -->

<!--   \node[draw, rectangle] (X2) at (3, -2) {$X_{2}$}; -->

<!--   \node[draw, rectangle] (A2) at (3, 2) {$A_{2}$}; -->

<!--   \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3}$}; -->

<!--   \node[draw, rectangle] (X3) at (8, -2) {$X_{3}$}; -->

<!--   % Node for latent variable (circle) -->

<!--   \node[draw, circle, fill=gray!20] (U) at (6, 4) {$U$}; -->

<!--   % Directed edges -->

<!--   \draw [->] (U) -- (Y2); -->

<!--   \draw [->] (U) -- (Y3); -->

<!--   \draw [->] (A2) -- (Y3); -->

<!--   \draw [->] (X2) -- (Y3); -->

<!--   \draw [->] (Y2) -- (X2); -->

<!--   \draw [->] (A1) -- (Y2); -->

<!--   \draw [->] (X1) -- (Y2); -->

<!--   \draw [->] (Y3) -- (X3); -->

<!-- \end{tikzpicture} -->

<!-- \begin{tikzpicture} -->

<!--   % Nodes for observed variables (squares) -->

<!--   \node[draw, rectangle] (X1) at (0, -2) {$X_{1}$}; -->

<!--   \node[draw, rectangle] (A1) at (0, 2) {$A_{1}$}; -->

<!--   \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2}$}; -->

<!--   \node[draw, rectangle] (X2) at (3, -2) {$X_{2}$}; -->

<!--   \node[draw, rectangle] (A2) at (3, 2) {$A_{2}$}; -->

<!--   \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3}$}; -->

<!--   \node[draw, rectangle] (X3) at (8, -2) {$X_{3}$}; -->

<!--   % Node for latent variable (circle) -->

<!--   \node[draw, circle, fill=gray!20] (U) at (6, 4) {$U$}; -->

<!--   % Directed edges -->

<!--   \draw [->] (U) -- (Y2); -->

<!--   \draw [->] (U) -- (Y3); -->

<!--   \draw [->] (A2) -- (Y3); -->

<!--   \draw [->] (X2) -- (Y3); -->

<!--   \draw [->] (Y2) -- (X2); -->

<!--   \draw [->] (A1) -- (Y2); -->

<!--   \draw [->] (X1) -- (Y2); -->

<!--   \draw [->] (Y3) -- (X3); -->

<!--   % Curved edges for X1 -> A1 and X2 -> A2 -->

<!--   \draw [->, bend left=40] (X1) to (A1); -->

<!--   \draw [->, bend left=40] (X2) to (A2); -->

<!-- \end{tikzpicture} -->

<!-- \begin{tikzpicture} -->

<!--   % Nodes for observed variables -->

<!--   \node[draw, rectangle] (X1) at (0, -2) {$X_1$}; -->

<!--   \node[draw, rectangle] (A1) at (0, 2) {$A_1$}; -->

<!--   \node[draw, rectangle] (Y2) at (3, 0) {$Y_2$}; -->

<!--   \node[draw, rectangle] (X2) at (3, -2) {$X_2$}; -->

<!--   \node[draw, rectangle] (A2) at (3, 2) {$A_2$}; -->

<!--   \node[draw, rectangle] (Y3) at (8, 0) {$Y_3$}; -->

<!--   \node[draw, rectangle] (X3) at (8, -2) {$X_3$}; -->

<!--   % Latent variable -->

<!--   \node[draw, circle, fill=gray!20] (U) at (6, 4) {$U$}; -->

<!--   % Edges -->

<!--   \draw [->] (U) -- (Y2); -->

<!--   \draw [->] (U) -- (Y3); -->

<!--   \draw [->] (U) -- (X1); -->

<!--   \draw [->] (U) -- (X2); -->

<!--   \draw [->] (U) -- (X3); -->

<!--   \draw [->] (A1) -- (Y2); -->

<!--   \draw [->] (X1) -- (Y2); -->

<!--   \draw [->] (Y2) -- (X2); -->

<!--   \draw [->] (A2) -- (Y3); -->

<!--   \draw [->] (X2) -- (Y3); -->

<!--   \draw [->] (Y3) -- (X3); -->

<!-- \end{tikzpicture} -->

<!-- ``` -->

<!-- a -->

<!-- ```{=tex} -->

<!-- \begin{figure}[htbp] -->

<!--     % Subfigure 1 -->

<!--     \begin{subfigure}[t]{0.48\textwidth} -->

<!--         \centering -->

<!--         \begin{tikzpicture} -->

<!--           % Nodes for observed variables (squares) -->

<!--           \node[draw, rectangle] (X1) at (0, -2) {$X_{1}$}; -->

<!--           \node[draw, rectangle] (A1) at (0, 2) {$A_{1}$}; -->

<!--           \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2}$}; -->

<!--           \node[draw, rectangle] (X2) at (3, -2) {$X_{2}$}; -->

<!--           \node[draw, rectangle] (A2) at (3, 2) {$A_{2}$}; -->

<!--           \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3}$}; -->

<!--           \node[draw, rectangle] (X3) at (8, -2) {$X_{3}$}; -->

<!--           % Node for latent variable (circle) -->

<!--           \node[draw, circle, fill=gray!20] (U) at (6, 4) {$U$}; -->

<!--           % Directed edges -->

<!--           \draw [->] (U) -- (Y2); -->

<!--           \draw [->] (U) -- (Y3); -->

<!--           \draw [->] (A2) -- (Y3); -->

<!--           \draw [->] (X2) -- (Y3); -->

<!--           \draw [->] (Y2) -- (X2); -->

<!--           \draw [->] (A1) -- (Y2); -->

<!--           \draw [->] (X1) -- (Y2); -->

<!--           \draw [->] (Y3) -- (X3); -->

<!--         \end{tikzpicture} -->

<!--         \caption{Generative Model 1} -->

<!--         \label{fig:GM1_DAG} -->

<!--     \end{subfigure} -->

<!--     \hfill -->

<!--     % Subfigure 2 -->

<!--     \begin{subfigure}[t]{0.48\textwidth} -->

<!--         \centering -->

<!--         \begin{tikzpicture} -->

<!--           % Nodes for observed variables (squares) -->

<!--           \node[draw, rectangle] (X1) at (0, -2) {$X_{1}$}; -->

<!--           \node[draw, rectangle] (A1) at (0, 2) {$A_{1}$}; -->

<!--           \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2}$}; -->

<!--           \node[draw, rectangle] (X2) at (3, -2) {$X_{2}$}; -->

<!--           \node[draw, rectangle] (A2) at (3, 2) {$A_{2}$}; -->

<!--           \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3}$}; -->

<!--           \node[draw, rectangle] (X3) at (8, -2) {$X_{3}$}; -->

<!--           % Node for latent variable (circle) -->

<!--           \node[draw, circle, fill=gray!20] (U) at (6, 4) {$U$}; -->

<!--           % Directed edges -->

<!--           \draw [->] (U) -- (Y2); -->

<!--           \draw [->] (U) -- (Y3); -->

<!--           \draw [->] (A2) -- (Y3); -->

<!--           \draw [->] (X2) -- (Y3); -->

<!--           \draw [->] (Y2) -- (X2); -->

<!--           \draw [->] (A1) -- (Y2); -->

<!--           \draw [->] (X1) -- (Y2); -->

<!--           \draw [->] (Y3) -- (X3); -->

<!--           % Curved edges for X1 -> A1 and X2 -> A2 -->

<!--           \draw [->, bend left=40] (X1) to (A1); -->

<!--           \draw [->, bend right=40] (X2) to (A2); -->

<!--         \end{tikzpicture} -->

<!--         \caption{Generative Model 2} -->

<!--         \label{fig:GM2_DAG} -->

<!--     \end{subfigure} -->

<!--     \vspace{1em} -->

<!--     % Subfigure 3 -->

<!--     \begin{subfigure}[t]{0.48\textwidth} -->

<!--         \centering -->

<!--         \begin{tikzpicture} -->

<!--           % Nodes for observed variables -->

<!--           \node[draw, rectangle] (X1) at (0, -2) {$X_1$}; -->

<!--           \node[draw, rectangle] (A1) at (0, 2) {$A_1$}; -->

<!--           \node[draw, rectangle] (Y2) at (3, 0) {$Y_2$}; -->

<!--           \node[draw, rectangle] (X2) at (3, -2) {$X_2$}; -->

<!--           \node[draw, rectangle] (A2) at (3, 2) {$A_2$}; -->

<!--           \node[draw, rectangle] (Y3) at (8, 0) {$Y_3$}; -->

<!--           \node[draw, rectangle] (X3) at (8, -2) {$X_3$}; -->

<!--           % Latent variable -->

<!--           \node[draw, circle, fill=gray!20] (U) at (6, 4) {$U$}; -->

<!--           % Edges -->

<!--           \draw [->] (U) -- (Y2); -->

<!--           \draw [->] (U) -- (Y3); -->

<!--           \draw [->] (U) -- (X1); -->

<!--           \draw [->] (U) -- (X2); -->

<!--           \draw [->] (U) -- (X3); -->

<!--           \draw [->] (A1) -- (Y2); -->

<!--           \draw [->] (X1) -- (Y2); -->

<!--           \draw [->] (Y2) -- (X2); -->

<!--           \draw [->] (A2) -- (Y3); -->

<!--           \draw [->] (X2) -- (Y3); -->

<!--           \draw [->] (Y3) -- (X3); -->

<!--         \end{tikzpicture} -->

<!--         \caption{Generative Model 3} -->

<!--         \label{fig:GM3_DAG} -->

<!--     \end{subfigure} -->

<!--     \caption{DAGs for Generative Models 1, 2, and 3. Each subfigure corresponds to a distinct model configuration.} -->

<!--     \label{fig:combined_DAGs} -->

<!-- \end{figure} -->

<!-- ``` -->
