---
title: "Estimation of Effects of Endogenous Time-Varying Covariates: A Comparison Of Multilevel Linear Modeling and Generalized Estimating Equations"
subtitle: "Research Report" 
author: 
  - name: "Ward B. Eiling (9294163)"
    orcid: "0009-0007-8114-9497"
    affiliation: 
      - name: "Utrecht University"
      - department: "Methodology and Statistics"
date: last-modified # deadline: "Dec 22, 2024"
date-format: long
format: 
  pdf:
    papersize: a4
    fig-pos: 'H'
    tbl-pos: 'H'
    keep-tex: true
    # toc: true
    number-sections: true
    colorlinks: true
    indent: true
    header-includes:
      - \usepackage{fancyhdr}
      - \usepackage{amsmath}
      - \usepackage{multirow}
      - \usepackage{booktabs}
      - \usepackage{pifont}
      # - \usepackage{float}
      # - \usepackage{tikz}
      # - \usepackage{subcaption}
    geometry:
      - top=2.5cm
      - bottom=2.5cm
      - left=2.5cm
      - right=2.5cm
    mainfont: Latin Modern Roman
    sansfont: Latin Modern Roman
    linestretch: 2
    fontsize: 11pt
    template-partials:
      - "before-body.tex"
bibliography: references.bib
link-citations: true
csl: apa.csl
execute:
  message: false
  warning: false
fig-cap-location: top
---

```{r}
#| label: Packages
#| echo: false

# for knitting the document
library(papaja)
library(rlang)
library(rmarkdown)
library(knitr)
library(kableExtra)
library(here)
# for general data manipulation and plotting
library(tidyverse)
# for estimation
library(lme4)
library(geepack)
# for diagrams
library(dagitty)
library(ggdag)
library(DiagrammeR)
library(magick)
library(pdftools)
```

\newpage

# Introduction

<!-- -   Start with a paragraph describing a problem in the real-life world (so that a BoS member not familiar with statistics understands why you are pursuing research in this direction); -->

Across a wide range of disciplines, researchers analyze clustered longitudinal, observational data to investigate prospective causal relationships between variables. When analyzing such data, psychological researchers most commonly use the multilevel linear model[^1] [MLM, @bauer2011], which—in the context of longitudinal data analysis—partitions observed variance into stable between-person differences and within-person fluctuations [@hamaker2020]. In the application of the MLM, time invariant and time-varying covariates, the latter measured repeatedly over time, are often available. Including these covariates is often beneficial, as it may improve the precision of parameter estimates and mitigate bias caused by confounders [@daniel2013]. However, in some cases, the inclusion of covariates can introduce bias into estimates of the treatment effect, for instance, when conditioning on a covariate that acts as a collider in the causal pathway [@elwert2014].

[^1]: The MLM is known by various names, including: linear mixed model, hierarchical linear model, random-effect model and mixed-effects model.

<!-- -   Then, add a paragraph describing what is known in the literature; -->

<!-- -   Also, describe what is not known and which gap you will address in your thesis; End with a clear research question and, if applicable, your hypothesis (for confirmatory research questions – it should be a testable hypothesis) or expectations (for exploratory research questions – can be much vaguer because of the explorative nature of the question). -->

A recent paper by @qian2020 highlighted a potential issue with the inclusion of covariates in the MLM that has not yet been widely recognized in the psychological literature. In their paper, the authors considered the appropriateness of the MLM for estimating the causal effect of a time-varying exposure/treatment, when this exposure is *randomly assigned* at each occasion within each person, on an outcome. While randomized assignment may seem to ensure that the presence or absence of a causal effect can be easily determined, @qian2020 showed that model fitting issues and parameter bias can arise in the presence of a *time-varying endogenous covariate*. A time-varying covariate is *endogenous* if it is directly or indirectly determined by prior treatment or outcome [@qian2020].

<!-- The issues described in @qian2020 can be categorized into two main problems: (1) model interpretation and (2) model fitting. In this investigation, we focus on the latter issue of model fitting of the MLM  -->

However, due to a divide between the disciplines that employ the MLM, such critiques appear to have largely failed to reach the applied researcher in psychology. One specific reason might be that the technical jargon in other disciplines makes it difficult for researchers to recognize when and how these issues emerge. Therefore, this report aims to understand why @qian2020 found biased estimates of the treatment effect for some generative models containing endogenous covariates and not for others; and to explain this issue to an audience of psychologists. To achieve this aim, the current investigation employs (a) graphical tools to evaluate key assumptions and (b) data simulations with additional scenarios to pinpoint the issue. Accordingly, the following research question will be addressed: *When does the inclusion of endogenous variables in multilevel linear models result in biased estimates of the treatment effect?*

<!-- -   Also, describe what is not known and which gap you will address in your thesis; End with a clear research question and, if applicable, your hypothesis (for confirmatory research questions – it should be a testable hypothesis) or expectations (for exploratory research questions – can be much vaguer because of the explorative nature of the question). -->

# Methods

To obtain a better understanding of the issue exposed by @qian2020, two methods were employed. First, graphical methods were used provide insight into the presence and extent of bias with potential violation of assumptions: (a) path diagrams were used to evaluate the conditional independence assumption and (b) directed acyclic graphs (DAGs) were used to evaluate the backdoor criterion [@pearl1988; @pearl2009]. Second, a simulation study was performed to reproduce the results for the generative models (GMs) from @qian2020 and to further isolate the issue using additional GMs. In this simulation, bias in the treatment effect was quantified using analytical multilevel models identical to the generative models.

## Data Generation

We consider 2 generative models (GMs) from @qian2020, one (GM A) being a special case of the general model (GM G) where bias was detected. To further isolate the source of bias, we introduce two additional special cases, labeled GM B and C. @tbl-gm-differences summarizes the differences between the generative models. Compared to the general model G, GM A is not directly determined by the random intercept $b_{i0}$; GM B is does not have a random slope $b_{i2}$ for treatment; and GM C does not have a fixed interaction effect $\beta_1$ between covariate and treatment.

| Generative Model | Name in @qian2020 | dependency $b_{i0}$ and $X_{it}$ | random slope treatment $b_{i2}$ | interaction $\beta_1$ |
|---------------|---------------|---------------|---------------|---------------|
| G(eneral) | 3 | $\checkmark$ | $\checkmark$ | $\checkmark$ |
| A | 1 | $\times$ | $\checkmark$ | $\checkmark$ |
| B | NA | $\checkmark$ | $\times$ | $\checkmark$ |
| C | NA | $\checkmark$ | $\checkmark$ | $\times$ |

: Generative Models: Summary of Differences {#tbl-gm-differences}

The details of the generative models are described below. We follow the symbol notation of @qian2020 to allow for direct comparison, but rewrite the equations into within- and between-person models [see @raudenbush2002; @Schoot2017].

### Generative Model G

Following the original notation of @qian2020, the outcome of GM G was generated according to the following model:

$$
Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + b_{i0} + A_{it} (\beta_0 + \beta_1 X_{it} + b_{i2}) + \epsilon_{it+1}
$$

where $Y_{it+1}$ is the outcome at time $t+1$, $X_{it}$ is the covariate at time $t$, $A_{it}$ is the treatment at time $t$, $b_{i0}$ is the random intercept, $b_{i2}$ is the random slope for the treatment, and $\epsilon_{it+1}$ is the error term. We may rewrite this model into the repeated-observations or within-person model in the following steps:

$$ 
\begin{aligned} Y_{it+1} &= \alpha_0 + \alpha_1 X_{it} + b_{i0} + A_{it} (\beta_0 + \beta_1 X_{it} + b_{i2}) + \epsilon_{it+1} \\ &= \alpha_0 + \alpha_1 X_{it} + b_{i0} + \beta_0 A_{it} +  \beta_1 A_{it} X_{it} + A_{it} b_{i2} + \epsilon_{it+1} \\ &= \alpha_0 + b_{i0} + \alpha_1 X_{it} + \beta_0 A_{it} + A_{it} b_{i2} + \beta_1 A_{it} X_{it} + \epsilon_{it+1} \\ &= (\alpha_0 + b_{i0}) + \alpha_1 X_{it} + (\beta_0 + b_{i2}) A_{it} + \beta_1 A_{it} X_{it} + \epsilon_{it+1} \\ &= \pi_{0i} + \pi_{1i} X_{it} + \pi_{2i} A_{it} + \pi_{3i} A_{it} X_{it} + \epsilon_{it+1}. \end{aligned}
$$

with the person-level or between-person model (level 2):

$$
\begin{aligned}
\pi_{0i} &= \alpha_0 + b_{i0}, & \text{where} \quad b_{i0} &\sim \mathcal{N}(0, \sigma_{b0}^2), \\
\pi_{1i} &= \alpha_1, \\
\pi_{2i} &= \beta_0 + b_{i2}, & \text{where} \quad b_{i2} &\sim \mathcal{N}(0, \sigma_{b2}^2), \\
\pi_{3i} &= \beta_1.
\end{aligned}
$$

We model fixed effects $\alpha_0$, $\alpha_1$, $\beta_0$, and $\beta_1$ as constants across individuals, while random effects $b_{i0}$ and $b_{i2}$ capture individual-specific deviations. Specifically, $b_{i0}$ represents deviations from the population intercept $\alpha_0$, and $b_{i2}$ represents deviations from the population slope $\beta_0$. Compared to the population average, a higher $b_{i0}$ indicates a higher initial outcome, while a higher $b_{i2}$ indicates a stronger treatment effect. Following @qian2020, the random effects $b_{i0}$ and $b_{i2}$ are modeled independent of each other.

The covariate is generated as:

$$
X_{it} = 
\begin{cases} 
b_{i0} + \epsilon_{X_{it}}, & \text{if } t = 1, \\
b_{i0} + Y_{it} + \epsilon_{X_{it}}, & \text{if } t \geq 2,
\end{cases}
\quad \text{where} \quad \epsilon_{X_{it}} \sim \mathcal{N}(0, 1)
$$

The randomization probability of treatment $p_t = P(A_{it} = 1 \mid H_{it})$ is constant at $1/2$. Thus, $A_{it} \sim \text{Bernoulli}(0.5)$ for $i = 1, \ldots, N$ and $t = 1, \ldots, T$. In other words, for every given person $i$ and every timepoint $t$, the probability that treatment is assigned is equivalent to a fair coinflip. The exogenous noise is $\epsilon_{it+1} \sim \mathcal{N}(0, \sigma_\epsilon^2)$.

@fig-GMG_path shows the path diagram for the first couple observations of GM G.

### Generative Model A

GM A is a special case of GM G, where the covariate $X_{it}$ is not directly determined by the random intercept $b_{i0}$ (see @fig-GMA_path). Instead, the endogenous covariate $X_{it}$ equals the previous outcome $Y_{it}$ plus some random noise:

$$
X_{it} = 
\begin{cases} 
\epsilon_{X_{it}}, & \text{if } t = 1, \\
Y_{it} + \epsilon_{X_{it}}, & \text{if } t \geq 2,
\end{cases}
\quad \text{where} \quad \epsilon_{X_{it}} \sim \mathcal{N}(0, 1)
$$

### Generative Model B

GM B is a special case of GM G, where the random slope $b_{i2}$ for the treatment $A_{it}$ is removed (see @fig-GMB_path). While the within-person model is the same as GM G, there is a slight alteration in the between-person model:

$$ \pi_{2i} = \beta_0. $$

The single equation model then becomes:

$$
Y_{it+1} = (\alpha_0 + b_{i0}) + \alpha_1 X_{it} + \beta_0 A_{it} + \beta_1 A_{it} X_{it} + \epsilon_{it+1}
$$

### Generative Model C

GM C is a special case of GM G, where the interaction term $\beta_1 A_{it} X_{it}$ is removed (see @fig-GMC_path). Accordingly, due to the removal of $\pi_{3i}$, the within-person model of GM C differs from GM G:

$$Y_{it+1} = \pi_{0i} + \pi_{1i} X_{it} + \pi_{2i} A_{it} + \epsilon_{it+1}.$$

Nevertheless, the between-person model of $\pi_{0i}$, $\pi_{1i}$ and $\pi_{2i}$ remains the same as GM G. The single equation model then becomes:

$$
Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + b_{i0} + A_{it} (\beta_0 + b_{i2}) + \epsilon_{it+1}.
$$

### Parameter Values

The following parameter values were adapted from @qian2020:

$$
\alpha_0 = -2, \quad \alpha_1 = -0.3, \quad \beta_0 = 1, \quad \beta_1 = 0.3,
$$

$$
\sigma_{b0}^2 = 4, \quad \sigma_{b2}^2 = 1, \quad \sigma_\epsilon^2 = 1.
$$

## Data Analysis

In the simulation study, we evaluated the performance of the models across a total of `r 4*2*3` different settings, each replicated 1,000 times, by systematically varying the following factors:

-   **Generative Models (GM):** G, A, B, C

-   **Number of timepoints (T):** 10, 30

-   **Sample size (N):** 30, 100, 200

All data generation and estimation was performed in `R`, version 4.4.2 [@rcoreteam2024]. After the generation of data generation for any given setting, analytical multilevel linear models were fit that are equivalent to each of the respective data-generating models. To fit the standard MLM, the `lmer` function from the R-package `lme4` [@bates2015] was employed with restricted maximum likelihood estimation.

# Results

## Conditional Independence and Path Diagrams

@qian2020 proposes the use of the conditional independence assumption to identify whether estimators of the treatment effect are consistent and unbiased under randomized treatment, which is given by:

$$ X_{it} \perp (b_{i0}, b_{i1}) \mid H_{it-1}, A_{it-1}, Y_{it}. $$

where $H_{it-1}$ refers to the history of the set of covariates, which in this case are all observations of covariate $X_{it}$ prior to the current timepoint $t$. This allows $X_{it}$ to be endogenous, but the endogenous covariate $X_{it}$ can only depend on the random effects through variables observed prior to $X_{it}$. If the only endogenous covariates are functions of prior treatments and prior outcomes, then the assumption automatically holds. According to @qian2020, this assumption must be verified based on theory and domain knowledge.

To make the application of the assumption more insightful, we accompany the equations of the GMs with path diagrams of the first three observations $t$ for each generative model (see @fig-Pathdiagrams).

**In GM1, the endogenous covariate** $X_{it}$ **equals the previous outcome** $Y_{it}$ **plus some random noiseo isolate the issue. In GM3, the endogenous covariate depends directly on** $b_{i0}$**, violating the assumption.** **We consider two variations on this model: GM3A, where the random slope** $b_{i2}$ **for the treatment** $A_{it}$ **is removed; GM3B, where the interaction term** $\beta_1 A_{it} X_{it}$ **is removed. Note that the conditional independence assumption is violated in either of these variations,**

When inspecting @fig-GMA_path, we may notice that $X_{it}$ becomes independent of the random effects after conditioning on $Y_{it}$. On the other hand, we can see that this assumption is violated only in GM G/B/C, as $X_{it}$ depends directly on $b_{i0}$ and can thus not be made independent of the random effects by conditioning on prior variables such as $Y_{it}$ (see @fig-GMG_path, @fig-GMB_path and @fig-GMC_path). Thus, all things considered, we would expect biased estimates of the treatment effect for GM G/B/C but not for GM A.

::: {#fig-Pathdiagrams layout-ncol="2"}
```{r}
#| label: fig-GMG_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM G"
#| eval: true
#| out.width: "100%"

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, 2) {$X_{1i}$};
  \node (X1var) at (0, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, -2) {$A_{1i}$};
  \node[draw, rectangle] (A1X1) at (0.3, 0) {$A_{1i}\cdot X_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, 2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, -2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, 2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2X2) at (5.3, 0) {$A_{2i}\cdot X_{2i}$}; % Increased x-coordinate

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate
  \node[draw, circle, fill=gray!20] (b2) at (3, -4) {$b_{i2}$}; % Adjusted to align with middle row

  % Arrows with path coefficients
  \draw [->] (b0) -- node[right] {$+$} (Y2);
  \draw [->] (b0) -- node[right] {$+$} (Y3);
  \draw [->] (A2) edge node[above left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[above] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[above left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[above] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (A1X1) edge node[above] {$\beta_1$} (Y2);
  \draw [->] (A2X2) edge node[above left] {$\beta_1$} (Y3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);
  \draw [->] (b0) edge (X1);
  \draw [->] (b0) edge (X2);
  \draw [->] (b0) edge (X3);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);
  \draw [<->, out=315, in=225, looseness=2] (b2) to node[below] {$\sigma^2_{b2}$} (b2);

  % Additional paths for random effects
  \draw[->, dashed] (b2) -- (1.5,-1); 
  \draw[->, dashed] (b2) -- (5,-1.2); 

\end{tikzpicture}
```

```{r}
#| label: fig-GMA_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM A"
#| eval: true
#| out.width: "100%"

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, 2) {$X_{1i}$};
  \node (X1var) at (0, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, -2) {$A_{1i}$};
  \node[draw, rectangle] (A1X1) at (0.3, 0) {$A_{1i}\cdot X_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, 2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, -2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, 2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2X2) at (5.3, 0) {$A_{2i}\cdot X_{2i}$}; % Increased x-coordinate

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate
  \node[draw, circle, fill=gray!20] (b2) at (3, -4) {$b_{i2}$}; % Adjusted to align with middle row

  % Arrows with path coefficients
  \draw [->] (b0) -- node[right] {$+$} (Y2);
  \draw [->] (b0) -- node[right] {$+$} (Y3);
  \draw [->] (A2) edge node[above left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[above] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[above left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[above] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (A1X1) edge node[above] {$\beta_1$} (Y2);
  \draw [->] (A2X2) edge node[above left] {$\beta_1$} (Y3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);
  \draw [<->, out=315, in=225, looseness=2] (b2) to node[below] {$\sigma^2_{b2}$} (b2);

  % Additional paths for random effects
  \draw[->, dashed] (b2) -- (1.5,-1); 
  \draw[->, dashed] (b2) -- (5,-1.2); 

\end{tikzpicture}
```

```{r}
#| label: fig-GMB_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM B"
#| eval: true
#| out.width: "100%"

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, 2) {$X_{1i}$};
  \node (X1var) at (0, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, -2) {$A_{1i}$};
  \node[draw, rectangle] (A1X1) at (0.3, 0) {$A_{1i}\cdot X_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, 2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, -2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, 2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2X2) at (5.3, 0) {$A_{2i}\cdot X_{2i}$}; % Increased x-coordinate

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate

  % Arrows with path coefficients
  \draw [->] (b0) -- node[right] {$+$} (Y2);
  \draw [->] (b0) -- node[right] {$+$} (Y3);
  \draw [->] (A2) edge node[above left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[above] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[above left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[above] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (A1X1) edge node[above] {$\beta_1$} (Y2);
  \draw [->] (A2X2) edge node[above left] {$\beta_1$} (Y3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);
  \draw [->] (b0) edge (X1);
  \draw [->] (b0) edge (X2);
  \draw [->] (b0) edge (X3);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);

\end{tikzpicture}
```

```{r}
#| label: fig-GMC_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM C"
#| eval: true
#| out.width: "100%"

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, 2) {$X_{1i}$};
  \node (X1var) at (0, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, -2) {$A_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, 2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, -2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, 2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, 3) {$\sigma^2_{X}$};

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate
  \node[draw, circle, fill=gray!20] (b2) at (3, -4) {$b_{i2}$}; % Adjusted to align with middle row

  % Arrows with path coefficients
  \draw [->] (b0) -- node[right] {$+$} (Y2);
  \draw [->] (b0) -- node[right] {$+$} (Y3);
  \draw [->] (A2) edge node[above left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[above] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[above left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[above] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);
  \draw [->] (b0) edge (X1);
  \draw [->] (b0) edge (X2);
  \draw [->] (b0) edge (X3);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);
  \draw [<->, out=315, in=225, looseness=2] (b2) to node[below] {$\sigma^2_{b2}$} (b2);

  % Additional paths for random effects
  \draw[->, dashed] (b2) -- (1.5,-1); 
  \draw[->, dashed] (b2) -- (5,-1.2); 

\end{tikzpicture}
```

*Note.* Random effects are represented by grey circles, observed variables by squares and relationships across variables by arrows, where dashed lines are reserved for random slopes.

Path Diagrams for Generative Models G, A, B and C (t = 1, 2, 3)
:::

## Backdoor Criterion and DAGs

According to the backdoor criterion [@pearl1988; @pearl2009], a requirement for causal identification, causal effects can be identified by blocking non-causal paths through conditioning on intermediate variables (e.g., controlling or matching). If any non-causal paths cannot be blocked due to omitted variables or measurement error, treatment and outcome remain linked via backdoor paths, leading to biased estimates of the treatment effect [@Kim2021a].

DAGs are a useful tool for representing causal relationships between variables and to evaluate the assumptions needed for causal identification [see @elwert2014 for a psychological example]. We formulated the DAGs for the first three observations of each generative model, where the random disturbance $b_{0i}$ was represented by the node U [e.g., @Kim2021a, see @fig-DAGs].

When applying Pearl's backdoor criterion to the GMs, it may be observed that there exists no backdoor path in the treatment effect $A_{it} \to Y_{it+1}$, as $A_{it}$ does not have any parents. While we need not control for covariate $X_{it}$ to obtain an unbiased total effect, doing so should not introduce bias. All things considered, according to the backdoor criterion, controlling for the covariate $X_{it}$ should not result in biased estimates of the treatment effect for any of the generative models.

**Note that these limitations of the DAG (not random effects or interactions explicit) would have prevented us from evaluating the conditional independence assumption.**

::: {#fig-DAGs layout-ncol="2"}
```{r}
#| label: fig-GMG_DAG
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM G, B, C"
#| eval: true

\begin{tikzpicture}[node distance=1cm]
  % Nodes for observed variables
  \node (X1) at (0, 2) {$X_{1}$};
  \node (A1) at (0, -2) {$A_{1}$};
  \node (Y2) at (3, 0) {$Y_{2}$};
  \node (X2) at (3, 2) {$X_{2}$};
  \node (A2) at (3, -2) {$A_{2}$};
  \node (Y3) at (6, 0) {$Y_{3}$};
  \node (X3) at (6, 2) {$X_{3}$};
  \node (U) at (4.5, 3) {$U$};

  % Directed edges
  \draw [->] (U) -- (Y2);
  \draw [->] (U) -- (Y3);
  \draw [->] (U) -- (X1);
  \draw [->] (U) -- (X2);
  \draw [->] (U) -- (X3);
  \draw [->] (A1) -- (Y2);
  \draw [->] (X1) -- (Y2);
  \draw [->] (Y2) -- (X2);
  \draw [->] (A2) -- (Y3);
  \draw [->] (X2) -- (Y3);
  \draw [->] (Y3) -- (X3);
\end{tikzpicture}
```

```{r}
#| label: fig-GMA_DAG
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM A"
#| eval: true

\begin{tikzpicture}[node distance=1cm]
  % Nodes for observed variables
  \node (X1) at (0, 2) {$X_{1}$};
  \node (A1) at (0, -2) {$A_{1}$};
  \node (Y2) at (3, 0) {$Y_{2}$};
  \node (X2) at (3, 2) {$X_{2}$};
  \node (A2) at (3, -2) {$A_{2}$};
  \node (Y3) at (6, 0) {$Y_{3}$};
  \node (X3) at (6, 2) {$X_{3}$};
  \node (U) at (4.5, 3) {$U$};

  % Directed edges
  \draw [->] (U) -- (Y2);
  \draw [->] (U) -- (Y3);
  \draw [->] (A2) -- (Y3);
  \draw [->] (X2) -- (Y3);
  \draw [->] (Y2) -- (X2);
  \draw [->] (A1) -- (Y2);
  \draw [->] (X1) -- (Y2);
  \draw [->] (Y3) -- (X3);
\end{tikzpicture}
```

*Note.* The red arrows show the biased backdoor path(s) in the treatment efffect (before controlling for $X_{it}$).

DAGs for Generative Models G, A, B and C (t = 1, 2, 3)
:::

<!-- Visually, we may notice from @fig-GM3_DAG that when we control for $X_{it}$ in GM3, we block the backdoor path from $X_{ti}$ to $Y_{it+1}$ through $X_{it}$, which is the path that would be biased if we did not control for $X_{it}$. -->

<!-- Alternatively, we can display the data generating models as a path diagram, where latent variables are represented by circles, observed variables by squares and relationships across variables by arrows. The path diagrams of the three data generating models is presented in @fig-pathdiagrams (GM1 in @fig-GM1_pd, GM2 in @fig-GM2_pd, and GM3 in @fig-GM3_pd), which shows the discrepancies between the different generative models more clearly than the DAGs. -->

<!-- SEE TEXT -->

<!-- We can make a couple observations from this path diagram: -->

<!-- -   Contrary to the DAG, this path diagram shows the moderation effect (1) of $X_{it}$ on the relationship between $X_{ti}$ and $Y_{it+1}$ and (2) of $u_{2i}$ on the relationship between $X_{it}$ and $Y_{it+1}$. -->

<!-- -   Similar to the example without treatment in section 2.2, the covariate $X_{it}$ is determined by the previous value of the outcome $Y_{ti}$---which makes it an endogenous time-varying covariate. -->

<!-- -   The path diagram does not display the difference in the randomized treatment assignment probabilities between GM1 and GM2. -->

## Simulation Study

@tbl-simulation-results presents the simulation results for each of the generative and analytical models. The estimates for the analytical MLM may be interpreted in terms of bias, where given the value of the treatment effect $\beta_0 = 1$, absolute bias of .05 would imply $5\%$ relative bias. Here we find the greatest absolute bias of $.02-.06$ for GM3, $\leq .015$ for GM1/2, $\leq .010$ for GM3B and , $\leq .005$ for GM3A. While the bias found for the original GMs 1, 2 and 3 was slightly larger here compared to @qian2020, the overall pattern remained the same. To conclude, once we remove either the dependency of the random intercept with the covariate (GM1), the random slope $b_{i2}$ (GM3A) or the interaction $\beta_1$ (GM3B) from GM3, the bias dissapears or becomes very small. The bias in GM3 decreases as the number of timepoints $T$ increases from 10 to 30. Note that the MLM model fitting success rates are particularly poor for GM2, where in the worst case, only 87 of the 1000 models were fitted.

::: {#tbl-simulation-results}
```{r}
#| label: simulation-results
#| echo: false
#| cache: true
#| escape: false

results <- readRDS(here("simulation_results/GM123ad-1000reps-researchreport-cleanscript/results_beta0_bias_sd_success.rds"))

# remove gee_ex and gee_ar1 and gee_ind
results <- results[, !grepl("gee_ex|gee_ar1|gee_ind", colnames(results))]
# remove GM2
results <- results[results$GM != 2, ]
# rename GM = 3 to "G", GM = 1 to "A", GM = 3a to "B" and GM = 3d to "C"
results$GM <- ifelse(results$GM == 3, "G", 
                     ifelse(results$GM == 1, "A", 
                            ifelse(results$GM == "3a", "B", 
                                   ifelse(results$GM == "3d", "C", NA))))

# reset row numbers
rownames(results) <- NULL
# reorder so that model GM is in the order G, A, B, C

results <- results[c(7:12, 1:6, 13:24), ]

# Create a kableExtra table with a second header
kableExtra::kbl(results, "latex", booktabs = T,  escape = F, digits = c(0, 0, 0, 3, 3, 3), col.names = c("GM", "T", "N", "Bias", "SD", "SR"), row.names = FALSE) %>%
  add_header_above(c(" " = 3, '$\\\\beta_0$' = 2, " " = 1), escape = F) %>%
  kable_styling(full_width = TRUE) %>%
  column_spec(1, width = "5em") %>%
  collapse_rows(columns = 1:2, valign = "middle") %>%
  kableExtra::kable_classic_2()
```

*Note.* GM: generative model. T: number of timepoints. N: sample size. SD: standard deviation of estimates across replications. SR: model fitting success rate. Bias: $\hat{\beta}_{0,MLM} - \beta_{0,MLM}$.

Treatment effect bias for Generative Models G, A, B and C over 1000 replications
:::

```{r}
#| label: fig-simulation-results
#| cache: true
#| echo: false
#| fig-cap: "Estimation bias for the fixed treatment effect $\\beta_0$ of each generative model for different combinations of sample size N and number of timepoints T over 1000 simulation replications"
#| fig-subcap: 
#| - "T=10, N=30"
#| - "T=10, N=100"
#| - "T=10, N=200"
#| - "T=30, N=30"
#| - "T=30, N=100"
#| - "T=30, N=200"
#| warning: false
#| layout-ncol: 2
#| fig-height: 4.8
#| escape: false

runname <- "GM123ad-1000reps-researchreport-cleanscript" # set a runname

# set the number of simulations
nsim <- 1000

# simulation for Research Report
design <- expand.grid(sample_size = c(30, 100, 200), total_T = c(10, 30), dgm_type = c(1,2,3,"3a","3d"))

# make sure dgm_type is not a factor
design$dgm_type <- as.character(design$dgm_type)

# create dataframe to store results
design$mlm_beta_0_bias <- rep(list(numeric(1000)), nrow(design))

for (idesign in 1:nrow(design)) {
  
    results_list <- readRDS(here(paste0("simulation_results/", runname, "/", idesign, ".RDS")))
    
    dgm_type <- design$dgm_type[idesign]
    
    ### Restate the true values of the parameters ###
    
    beta_0_true <- 1 # was originallly 0.5 in the code but is 1 in the paper
    
    result_lmm <- results_list[grep("solution_lmm", names(results_list))]
    
    ### Extract the Results ###
    
    mlm_beta_0 <- sapply(result_lmm, function(l) l$coef["A", "Estimate"])
    design$mlm_beta_0_bias[[idesign]] <- mlm_beta_0 - beta_0_true
    
}

# remove dgm_type = 2
design <- design[design$dgm_type != 2, ]

# rename dgm_type = 3 to "G", dgm_type = 1 to "A", dgm_type = 3a to "B" and dgm_type = 3d to "C"
design$dgm_type <- ifelse(design$dgm_type == 3, "G", 
                          ifelse(design$dgm_type == 1, "A", 
                                 ifelse(design$dgm_type == "3a", "B", 
                                        ifelse(design$dgm_type == "3d", "C", NA))))

# Convert T and N to factors if they are numeric
design$total_T <- as.factor(design$total_T)
design$sample_size <- as.factor(design$sample_size)

data_t10_n30 <- design[design$total_T == 10 & design$sample_size == 30, ]
data_t10_n100 <- design[design$total_T == 10 & design$sample_size == 100, ]
data_t10_n200 <- design[design$total_T == 10 & design$sample_size == 200, ]
data_t30_n30 <- design[design$total_T == 30 & design$sample_size == 30, ]
data_t30_n100 <- design[design$total_T == 30 & design$sample_size == 100, ]
data_t30_n200 <- design[design$total_T == 30 & design$sample_size == 200, ]

# Function to unnest and plot
plot_boxplot <- function(data, dataset_name) {
  # Unnest the bias values (assuming mlm_beta_0_bias is the list column)
  data_long <- data %>%
    unnest(mlm_beta_0_bias) %>%
    mutate(T_N = interaction(total_T, sample_size))  # Create combined T-N factor
  
  # Reorder the levels of dgm_type so that "G" comes first
  data_long$dgm_type <- factor(data_long$dgm_type, levels = c("G", "A", "B", "C"))
  
  # Create the boxplot
  p <- ggplot(data_long, aes(x = dgm_type, y = mlm_beta_0_bias, col = dgm_type)) + 
    geom_boxplot() +
    geom_hline(yintercept = 0, linetype = "dashed") +  # Dashed horizontal line at 0
    labs(x = "Generative Model", # title = paste("Boxplot for", dataset_name), 
         y = "Bias") +
    theme(
      axis.text.x = element_text(size = 14, vjust = -0.8),  # Increase font size of x-axis category labels
      axis.text.y = element_text(size = 14),
      axis.title.x = element_text(size = 16, vjust = -0.8),  # Increase font size of x-axis label
      axis.title.y = element_text(size = 16),  # Increase font size of y-axis label
      legend.position = "none",   # Remove legend
      panel.border = element_rect(colour = "black", fill = NA, size = 1),  # Add black border
      plot.margin = margin(5, 5, 12, 5),  # Add space around the plot
      panel.background = element_rect(fill = "white"),  # White background instead of light grey
      panel.grid.major = element_blank(),  # Remove major gridlines
      panel.grid.minor = element_blank(),  # Remove minor gridlines
      axis.line = element_line(colour = "black"),  # Black axis lines
      axis.ticks = element_line(size = 1, color = "black") # Black ticks
    ) 
  print(p)
}

# Example for each dataset:
plot_boxplot(data_t10_n30, "T=10, N=30")
plot_boxplot(data_t10_n100, "T=10, N=100")
plot_boxplot(data_t10_n200, "T=10, N=200")
plot_boxplot(data_t30_n30, "T=30, N=30")
plot_boxplot(data_t30_n100, "T=30, N=100")
plot_boxplot(data_t30_n200, "T=30, N=200")
```

# Discussion

This report employed both graphical methods and data simulations to understand and explain the issue of endogenous covariates. Now we first discuss the expected results based on the backdoor criterion [@pearl1988, @pearl2009] and the conditional independence assumption [@qian2020], whereafter we discuss the findings relating to the two research questions.

Using the conditional independence assumption of @qian2020, we would expect, based on the path diagrams, that the treatment effect would be biased for GM3, 3A and 3B. On the other hand, the backdoor criterion suggested the absence of bias for all generative models. While @qian2020 show that GM3 is the only model with bias in the treatment effect, the backdoor criterion failed to identify this bias, as there is no backdoor path in the treatment effect. This may be explained by the fact that the classic DAG does not impose restrictions based on (a) the random slopes and (b) interaction effects.

<!-- (1) When does the inclusion of endogenous variables in multilevel linear models result in biased estimates of the treatment effect? -->

<!--     (a) How is the bias in the treatment effect affected by the removal of the interaction $\beta_1$ from generative model 3? -->

<!--     (b) How is the bias in the treatment effect affected by the removal of the random slope $b_{i2}$ from generative model 3? -->

The first research question—pertaining to the extent of treatment effect bias in MLM estimates of generative model that were nested in GM3---was investigated using the analytical multilevel model. First, we reproduced the findings by @qian2020 who found consistent estimators for GM1 and and inconsistent ones for GM3. Using additional generative models, we found that bias became indiscernable when removing from GM3 either the dependency between the random intercept and covariate (GM1), the random slope for treatment (GM3A) or the interaction effect (GM3B). This finding is in sharp contrast to the suggestion of the conditional independence assumption that the treatment effect would be biased for GM3, 3A and 3B.

<!-- What next -->

The current research report leaves several avenues unexplored. First, it is unclear whether the simulation findings pertaining the generative models in @qian2020 and here generalize to other generative models. For instance, we found here that removal of a random slope or interaction from GM3 got rid of most if not all of the treatment effect bias. Thus, it is important to establish how this generalizes, so that practical recommendations can be formulated. This is particularly important, since while violations of model assumptions are never desired, the robustness against and the practical implications of a violation is what matters. Second, it is unclear how exactly the divide between the literatures pertaining to the focus of the MLM on different centering methods and within- and between-person interpretations and the focus of the GEE on marginal and conditional interpretations may be bridged. Consequently, future research could assess the implications of centering methods in MLMs on the extent to which the marginal interpretation of MLM breaks down. Third, we found that the classical DAG may not be sufficient to identify bias in the treatment effect for GM3, especially due to its lack of specification of interaction effects. Concerns regarding the use of Pearl's backdoor criterion in situations with interaction effects have been voiced by several people (see @weinberg2007; @attia2022). Future research could explore to what extent proposed extensions of the DAG may be useful in identifying bias in the treatment effect for GM3. Finally, it may be interesting to investigate the implications of endogenous covariates in MLMs for other types of longitudinal data analysis methods, such as dynamic structural equation modelling (DSEM; a widely used framework in the social sciences based on MLM).

Third, since the issue extends to all longitudinal data analysis methods according to @diggle2002, in future research it may be interesting to investigate the implications of endogenous covariates in MLMs for other types of longitudinal data analysis methods, such as dynamic structural equation modelling (DSEM; a widely used framework in the social sciences based on MLM).

<!-- # Other ideas -->

<!-- -   Initially, it seemed that the issue of endogenous covariates meant that, for unbiased estimation of the treatment effect, we should rely on GEE with independence. However, it is important not to conflate the issues mentioned by @qian2020. The first issue pertains to model interpretation: should we expect covariates to be endogenous and be primarily interested in marginal interpretations of the parameters rather than the person-specific (conditional-on-the-random-effect) interpretation, we should indeed employ GEE with working independence (OR STRUCTURAL MARGINAL MODELS??? CHECK THIS OUT IN ZOTERO). The second issue pertains to model fitting: once we conclude that person-specific interpretation aligns with our interest but we fear the presence of endogenous covariates, we have to assess the conditional independence assumption. -->

<!-- -   Across all generative model, we generally found that the estimated fixed treatment effect differed more from the specified MLM effect for the analytical GEE models than for the analytical MLM model. This is rather unsurprising, considering that the MLM is analyzing the exact same model as was specified, thereby putting it at an advantage over the GEE. -->

<!-- -   While GEE with independence may indeed yield unbiased estimators of the marginal effect as mentioned by @qian2020, they do not reflect the value of the model parameter. And since the endogeneity of a covariate implies that it is determined by the random effect, thereby making the marginal relationship between any given $X_{it}$ and $Y_{it+1}$ different as shown by @qian2020 (section 2.2), it is unclear what utility the combined marginal effect may serve in this context. -->

<!-- -   They set the same seed for every setting, potentially making the first dataset the same for every setting. Nevertheless, properly randomizing does not seem to drastically impact the results. The settings with GM2, $T = 30$ and $N \geq 100$ of @qian2020 results in very implausible values for the covariate $X_{it}$ and outcome $Y_{it+1}$, often exceeding a million. This may imply the presence of a non-stationary process (e.g., unit root). In the case of GEE analytical models, these settings result in very large estimates. -->

<!-- -   wasn't working exchangeability in GEE equivalent to MLM? then use this to argue why using this GEE variation. Because for the difference of GEE with independence from the actual specified conditional parameter estimates should indicate a difference between the marginal and conditional effect, but this is not bias. Can we speak of bias with exchangeability? -->

<!--     -   no it depends on the random effects, and it was compound symmetry. Leave this for next time. -->

\newpage

# References

::: {#refs}
:::

<!-- --- -->

<!-- nocite: | -->

<!--   @* -->

<!-- --- -->

<!-- # Appendix -->
