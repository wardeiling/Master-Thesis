---
title: "Treatment Effect Bias in Multilevel Linear Models under Time-Varying Endogeneity: A New Look at Qian et al. (2020)"
# The Impact of Endogenous Time-Varying Covariates on Treatment Effect Estimates in Multilevel Linear Models: Revisiting Qian et al. (2020)
# "Untangling Bias in Multilevel Linear Models: The Role of Endogenous Time-Varying Covariates"
# "The Dangers of Including Time-Varying Endogenous Covariates in the Multilevel Linear Model"
# Estimation of Effects of Endogenous Time-Varying Covariates: A Comparison Of Multilevel Linear Modeling and Generalized Estimating Equations"
subtitle: "Research Report" 
author: 
  - name: "Ward B. Eiling (9294163)"
    orcid: "0009-0007-8114-9497"
    affiliation: 
      - name: "Utrecht University"
      - department: "Methodology and Statistics"
date: last-modified # deadline: "Dec 22, 2024"
date-format: long
format: # docx
  pdf:
    papersize: a4
    fig-pos: 'H'
    tbl-pos: 'H'
    # keep-tex: true
    # toc: true
    number-sections: true
    colorlinks: true
    indent: true
    header-includes:
      - \usepackage{fancyhdr}
      - \usepackage{amsmath}
      - \usepackage{multirow}
      - \usepackage{booktabs}
      - \usepackage{pifont}
      # - \usepackage{float}
      # - \usepackage{tikz}
      # - \usepackage{subcaption}
    geometry:
      - top=2.5cm
      - bottom=2.5cm
      - left=2.5cm
      - right=2.5cm
    mainfont: Latin Modern Roman
    sansfont: Latin Modern Roman
    linestretch: 2
    fontsize: 11pt
    template-partials:
      - "before-body.tex"
bibliography: references.bib
link-citations: true
csl: apa.csl
execute:
  message: false
  warning: false
fig-cap-location: top
---

```{r}
#| label: Packages
#| echo: false

# for knitting the document
library(papaja)
library(rlang)
library(rmarkdown)
library(knitr)
library(kableExtra)
library(here)
# for general data manipulation and plotting
library(tidyverse)
# for estimation
# library(lme4)
# library(geepack)
# for diagrams
library(dagitty)
library(ggdag)
library(DiagrammeR)
library(magick)
library(pdftools)
```

# Introduction

<!-- -   Start with a paragraph describing a problem in the real-life world (so that a BoS member not familiar with statistics understands why you are pursuing research in this direction); -->

Across a wide range of disciplines, researchers analyze clustered longitudinal, observational data to investigate prospective causal relationships between variables. When analyzing such data, psychological researchers most commonly use the multilevel linear model[^1] [MLM, @bauer2011], which—in the context of longitudinal data analysis—partitions observed variance into stable between-person differences and within-person fluctuations [@hamaker2020]. Research questions explored with the MLM commonly lead to the availability of time invariant and/or time-varying covariates, the latter measured repeatedly over time. The inclusion of covariates is a common strategy to improve parameter precision [@boruvka2018] and address bias introduced by (time-varying) confounders [@daniel2013; @Wodtke2020; @Robins2000]. Nevertheless, this approach is not universally beneficial, as conditioning on endogenous covariates—those influenced by (prior) treatment/exposure or outcome—can create challenges for standard methods like MLMs, which implicitly assume the exogeneity of covariates [@Erler2019].

[^1]: The MLM is known by various names in different substantive fields, including: linear mixed model, hierarchical linear model, random-effect model and mixed-effects model.

<!-- -   Then, add a paragraph describing what is known in the literature; -->

<!-- -   Also, describe what is not known and which gap you will address in your thesis; End with a clear research question and, if applicable, your hypothesis (for confirmatory research questions – it should be a testable hypothesis) or expectations (for exploratory research questions – can be much vaguer because of the explorative nature of the question). -->

Originating from the biostatistics literature and dating back to the work of @pepe1994, this assumption has been shown to be non-trivial when endogenous covariates vary over time. In fact, their inclusion in longitudinal studies can lead to biased treatment effect estimates, an issue that, despite its significance, has received limited attention in psychological research. Building on this foundation, a recent paper by @qian2020 examined the suitability of MLM for estimating the causal effect of a time-varying exposure or treatment. Specifically, they focused on settings where the exposure is randomly assigned at each occasion within individuals. Such randomized exposures may include, for example, prompts delivered through push notifications to remind participants of cognitive or mindfulness-based strategies [@nahum-shani2021; @walton2018]. While random assignment with a constant probability might seem sufficient to identify (the presence and absence of) causal effects, @qian2020 showed that model fitting issues and parameter bias can arise when a *time-varying endogenous covariate* is present.

<!-- The issues described in @qian2020 can be categorized into two main problems: (1) model interpretation and (2) model fitting. In this investigation, we focus on the latter issue of model fitting of the MLM  -->

However, due to a divide between the disciplines that employ the MLM, such critiques appear to have largely failed to reach the applied researcher in psychology. One specific reason might be that the technical jargon in other disciplines makes it difficult for researchers to recognize when and how these issues emerge. This report aims to explore why @qian2020 observed biased estimates of the treatment effect in certain data-generating mechanisms containing endogenous covariates, while not for others. Additionally, it seeks to explain this issue to an audience of psychologists. The study will first employ graphical diagrams to assess two criteria across various scenarios involving an endogenous time-varying covariate and randomized treatment: (a) path diagrams to evaluate the conditional independence assumption introduced by @qian2020 and (b) directed acyclic graphs (DAGs) to assess the backdoor criterion [@pearl1988; @pearl2009]. Subsequently, data simulations based on @qian2020's original scenarios, along with additional ones, will be performed to reproduce and isolate the underlying issue and evaluate whether these criteria can effectively detect bias in the treatment effect. The following research question will be addressed: *When does the inclusion of endogenous variables in multilevel linear models result in biased estimates of the treatment effect?*

<!-- -   Also, describe what is not known and which gap you will address in your thesis; End with a clear research question and, if applicable, your hypothesis (for confirmatory research questions – it should be a testable hypothesis) or expectations (for exploratory research questions – can be much vaguer because of the explorative nature of the question). -->

# Methods

In this section, we first formulate four generative models (GMs) that incorporate a time-varying endogenous covariate and a randomized treatment. We then outline the methodology used to investigate treatment effect bias across different settings.

## Data Generation

We consider two GMs from @qian2020, one (GM A) being a special case of the general model (GM G) where bias was detected. To further isolate the source of bias, we introduce two additional special cases, labeled GM B and C. We first describe the general generative model (GM G) in detail, and then proceed to its three special cases: GM A, B, and C.

### General Model: Generative Model G

Following the original notation of @qian2020, the outcome of GM G was generated as:

$$
Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + b_{i0} + A_{it} (\beta_0 + \beta_1 X_{it} + b_{i2}) + \epsilon_{it+1}
$$ {#eq-modelG}

where $Y_{it+1}$ is the outcome for person $i$ at time $t+1$, $X_{it}$ is the covariate for person $i$ at time $t$, $A_{it}$ is the treatment for person $i$ at time $t$, $b_{i0}$ is the random intercept, $b_{i2}$ is the random slope for the treatment, and $\epsilon_{it+1}$ is the error term. Alternatively, the model can be rewritten in the multilevel notation of @raudenbush2002, with at the within-person level (level 1):

$$ 
\begin{aligned} Y_{it+1} &= \alpha_0 + \alpha_1 X_{it} + b_{i0} + A_{it} (\beta_0 + \beta_1 X_{it} + b_{i2}) + \epsilon_{it+1} \\ &= (\alpha_0 + b_{i0}) + \alpha_1 X_{it} + (\beta_0 + b_{i2}) A_{it} + \beta_1 A_{it} X_{it} + \epsilon_{it+1} \\ &= \pi_{0i} + \pi_{1i} X_{it} + \pi_{2i} A_{it} + \pi_{3i} A_{it} X_{it} + \epsilon_{it+1}. \end{aligned}
$$

and at the between-person level (level 2):

$$
\begin{aligned}
\pi_{0i} &= \alpha_0 + b_{i0}, & \text{where} \quad b_{i0} &\sim \mathcal{N}(0, \sigma_{b0}^2), \\
\pi_{1i} &= \alpha_1, \\
\pi_{2i} &= \beta_0 + b_{i2}, & \text{where} \quad b_{i2} &\sim \mathcal{N}(0, \sigma_{b2}^2), \\
\pi_{3i} &= \beta_1.
\end{aligned}
$$

The parameters $\alpha_0 = -2$, $\alpha_1 = -0.3$, $\beta_0 = 1$, and $\beta_1 = 0.3$ are fixed effects that are constant across individuals, while $b_{i0}$ and $b_{i2}$ are independent random effects that capture individual-specific deviations from population parameters. The presence of the interaction term $\beta_1$ implies treatment heterogeneity: the effect of the treatment $A_{it}$ on the outcome depends on the value of the covariate $X_{it}$. The random intercept $b_{i0}$ and random slope $b_{i2}$ are assumed to be normally distributed with mean zero and variance $\sigma_{b0}^2 = 4$ and $\sigma_{b2}^2 = 1$, respectively. $b_{i0}$ represents deviations from the population intercept $\alpha_0$, and $b_{i2}$ represents deviations from the population slope $\beta_0$. The exogenous noise $\epsilon_{it+1}$ in the outcome is normally distributed with mean zero and variance $\sigma_\epsilon^2 = 1$.

The covariate is generated as:

$$
X_{it} = 
\begin{cases} 
b_{i0} + \epsilon_{X_{it}} & \text{if } t = 1, \\
b_{i0} + Y_{it} + \epsilon_{X_{it}} & \text{if } t \geq 2,
\end{cases}
\quad \text{where} \quad \epsilon_{X_{it}} \sim \mathcal{N}(0, 1)
$$

The treatment randomization probability is constant at $p_t = 0.5$, so $A_{it} \sim \text{Bernoulli}(0.5)$ for all $i$ and $t$. In other words, for every given person $i$ and every timepoint $t$, the probability that treatment is assigned is equivalent to a fair coinflip. Relationships between the (observed and latent) variables are illustrated in @fig-GMG_path.

### Special Cases: Generative Model A, B and C

We consider three special cases of GM G, namely GM A, B and C. The relation of each special case to GM G is summarized in @tbl-gm-differences. The specifics of each special case are described below.

<!-- Compared to the general model G, GM A is not directly determined by the random intercept $b_{i0}$; GM B does not have a random slope $b_{i2}$ for treatment; and GM C does not have a fixed interaction effect $\beta_1$ between covariate and treatment. Below we discuss the specifics of each special case. -->

| Generative Model | Name in @qian2020 | dependency $b_{i0}$ and $X_{it}$ | random slope treatment $b_{i2}$ | interaction $\beta_1$ |
|---------------|---------------|---------------|---------------|---------------|
| G(eneral) | 3 | $\checkmark$ | $\checkmark$ | $\checkmark$ |
| A | 1 | $\times$ | $\checkmark$ | $\checkmark$ |
| B | NA | $\checkmark$ | $\times$ | $\checkmark$ |
| C | NA | $\checkmark$ | $\checkmark$ | $\times$ |

: Summary of Differences Between Generative Models {#tbl-gm-differences}

GM A is a special case of GM G, where the effect of the random intercept $b_{i0}$ on the covariate $X_{it}$ is set to zero. This results in a model where the covariate $X_{it}$ is not directly determined by the random intercept $b_{i0}$ (see @fig-GMA_path). Instead, the endogenous covariate $X_{it}$ equals the previous outcome $Y_{it}$ plus some random noise:

$$
X_{it} = 
\begin{cases} 
\epsilon_{X_{it}} & \text{if } t = 1, \\
Y_{it} + \epsilon_{X_{it}} & \text{if } t \geq 2,
\end{cases}
\quad \text{where} \quad \epsilon_{X_{it}} \sim \mathcal{N}(0, 1)
$$

GM B is a special case of GM G in which the random slope $b_{i2}$ was removed (see @fig-GMB_path) by setting the random slope variance $\sigma_{b2}^2$ to zero. While the within-person model is the same as GM G, there is a slight alteration in the between-person model:

$$ \pi_{2i} = \beta_0. $$

The composite model then becomes:

$$
Y_{it+1} = (\alpha_0 + b_{i0}) + \alpha_1 X_{it} + \beta_0 A_{it} + \beta_1 A_{it} X_{it} + \epsilon_{it+1}
$$ {#eq-modelB}

GM C is a special case of GM G, where the fixed interaction parameter $\beta_1$ is set to zero, which implies the removal of the interaction term $\beta_1 A_{it} X_{it}$ (see @fig-GMC_path). This, in turn, removed $\pi_{3i}$, thereby creating a discrepancy in within-person model of GM C and GM G:

$$Y_{it+1} = \pi_{0i} + \pi_{1i} X_{it} + \pi_{2i} A_{it} + \epsilon_{it+1}.$$

Nevertheless, the between-person model of $\pi_{0i}$, $\pi_{1i}$ and $\pi_{2i}$ remains the same as GM G. The composite model then becomes:

$$
Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + b_{i0} + A_{it} (\beta_0 + b_{i2}) + \epsilon_{it+1}.
$$ {#eq-modelC}

## Data Analysis

Data generation and estimation were performed in `R`, version 4.4.2 [@rcoreteam2024], following @qian2020's methodology for consistency. After data generation, analytical MLMs with restricted maximum likelihood estimation were fit using the `lmer` function from the `lme4` package [@bates2015]. Specifically, the MLM from @eq-modelG was fit for GM G and A, the MLM from @eq-modelB for GM B, and the MLM from @eq-modelC for GM C[^2].

[^2]: Contrary to the data generating models, the analytical models do not model the covariate $X_{it}$ and the treatment $A_{it}$.

In the simulation study, we evaluated the bias of the analytical models across different settings by systematically varying the following 3 factors: Generative Model (GM) with the levels G, A, B, and C; number of timepoints (T) with the levels 10 and 30; and sample size (N) with the levels 30, 100, and 200. By varying these factors, we created a total of 24 unique settings, each of which was replicated 10,000 times. Bias was calculated as the difference between the estimated treatment effect and the true treatment effect, averaged across replications. 

<!-- The uncertainty in the bias estimates for each setting was quantified using the standard deviation and the Monte Carlo standard error [see @morris2019]. -->

<!-- For each setting, the performance was based on bias of the treatment effect estimates. The bias was calculated as the difference between the estimated treatment effect and the true treatment effect and averaged across the replications: $\frac{1}{n_{\text{sim}}} \sum\limits_{i = 1}^{n_{\text{sim}}} \hat{\beta}_{0i} - \beta_{0}$ [@morris2019]. Uncertainty in the bias estimates for each setting was quantified using the standard deviation and Monte Carlo standard error. The standard deviation was calculated as $\sqrt{\frac{1}{(n_{\text{sim}} - 1)} \sum\limits_{i=1}^{n_{\text{sim}}} \left(\hat{\beta}_{0i} - \bar{\beta}_{0}\right)^2}$ and the Monte Carlo standard error was calculated as $\frac{\text{SD}}{\sqrt{n_{\text{sim}}}}$ [@morris2019]. -->

# Results

This section begins by constructing predictions about treatment effect bias for each GM, guided by the conditional independence assumption and the backdoor criterion. These predictions are then compared with the simulation study results, which present the bias across the different GMs.

## Conditional Independence and Path Diagrams

The first criterion for evaluating the presence of bias in treatment effect estimates is the *conditional independence assumption*, introduced by @qian2020 and based on the work of @sitlani2012. According to @qian2020, this assumption should identify whether estimators of the treatment effect are consistent and unbiased under randomized treatment assignment. The conditional independence assumption states that the covariate at time $t$ ($X_{it}$) should be independent of the individual’s random effects (intercept $b_{i0}$ and slope(s) $b_{i1}$) once we account for their history of covariates up to timepoint $t-1$ ($H_{it-1}$), previous treatment ($A_{it-1}$), and prior outcome ($Y_{it}$).

$$
X_{it} \perp (b_{i0}, b_{i1}) \mid H_{it-1}, A_{it-1}, Y_{it}.
$$

<!-- where $b_{i0}$ and $b_{i1}$ represent the random intercept and random slope(s), respectively, and $H_{it-1}$ comprises all observations of that covariate before timepoint $t$.  -->

This assumption allows for $X_{it}$ to be influenced by earlier variables (e.g., outcomes or treatments) but not directly by unobserved individual characteristics (i.e., random effects). However, as @qian2020 highlights, ensuring this assumption holds requires careful consideration of theory and domain knowledge. To clarify the application of the conditional independence assumption, we pair the equations of the generative models (GMs) with path diagrams [@duncan1966; @wright1934a] illustrating the first three timepoints ($t$) for each model (see @fig-Pathdiagrams). We can observe that all models contain a time-varying *endogenous* covariate $X_{it}$, which is determined by previous outcome $Y_{it}$.

<!-- If the included endogenous covariates are only affected by prior outcomes and treatments, the assumption is automatically satisfied.  -->

::: {#fig-Pathdiagrams layout-ncol="2"}
```{r}
#| label: fig-GMG_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM G"
#| eval: true
#| out.width: "100%"

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, 2) {$X_{1i}$};
  \node (X1var) at (0, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, -2) {$A_{1i}$};
  \node[draw, rectangle] (A1X1) at (0.3, 0) {$A_{1i}\cdot X_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, 2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, -2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, 2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2X2) at (5.3, 0) {$A_{2i}\cdot X_{2i}$}; % Increased x-coordinate

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate
  \node[draw, circle, fill=gray!20] (b2) at (3, -4) {$b_{i2}$}; % Adjusted to align with middle row

  % Arrows with path coefficients
  \draw [->] (b0) -- (Y2);
  \draw [->] (b0) -- (Y3);
  \draw [->] (A2) edge node[above left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[above] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[above left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[above] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (A1X1) edge node[above] {$\beta_1$} (Y2);
  \draw [->] (A2X2) edge node[above left] {$\beta_1$} (Y3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);
  \draw [->] (b0) edge (X1);
  \draw [->] (b0) edge (X2);
  \draw [->] (b0) edge (X3);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);
  \draw [<->, out=315, in=225, looseness=2] (b2) to node[below] {$\sigma^2_{b2}$} (b2);

  % Additional paths for random effects
  \draw[->, dashed] (b2) -- (1.5,-1); 
  \draw[->, dashed] (b2) -- (5,-1.2); 

\end{tikzpicture}
```

```{r}
#| label: fig-GMA_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM A"
#| eval: true
#| out.width: "100%"

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, 2) {$X_{1i}$};
  \node (X1var) at (0, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, -2) {$A_{1i}$};
  \node[draw, rectangle] (A1X1) at (0.3, 0) {$A_{1i}\cdot X_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, 2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, -2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, 2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2X2) at (5.3, 0) {$A_{2i}\cdot X_{2i}$}; % Increased x-coordinate

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate
  \node[draw, circle, fill=gray!20] (b2) at (3, -4) {$b_{i2}$}; % Adjusted to align with middle row

  % Arrows with path coefficients
  \draw [->] (b0) -- (Y2);
  \draw [->] (b0) -- (Y3);
  \draw [->] (A2) edge node[above left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[above] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[above left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[above] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (A1X1) edge node[above] {$\beta_1$} (Y2);
  \draw [->] (A2X2) edge node[above left] {$\beta_1$} (Y3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);
  \draw [<->, out=315, in=225, looseness=2] (b2) to node[below] {$\sigma^2_{b2}$} (b2);

  % Additional paths for random effects
  \draw[->, dashed] (b2) -- (1.5,-1); 
  \draw[->, dashed] (b2) -- (5,-1.2); 

\end{tikzpicture}
```

```{r}
#| label: fig-GMB_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM B"
#| eval: true
#| out.width: "100%"

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, 2) {$X_{1i}$};
  \node (X1var) at (0, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, -2) {$A_{1i}$};
  \node[draw, rectangle] (A1X1) at (0.3, 0) {$A_{1i}\cdot X_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, 2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, -2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, 2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2X2) at (5.3, 0) {$A_{2i}\cdot X_{2i}$}; % Increased x-coordinate

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate

  % Arrows with path coefficients
  \draw [->] (b0) -- (Y2);
  \draw [->] (b0) -- (Y3);
  \draw [->] (A2) edge node[above left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[above] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[above left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[above] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (A1X1) edge node[above] {$\beta_1$} (Y2);
  \draw [->] (A2X2) edge node[above left] {$\beta_1$} (Y3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);
  \draw [->] (b0) edge (X1);
  \draw [->] (b0) edge (X2);
  \draw [->] (b0) edge (X3);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);

\end{tikzpicture}
```

```{r}
#| label: fig-GMC_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM C"
#| eval: true
#| out.width: "100%"

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, 2) {$X_{1i}$};
  \node (X1var) at (0, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, -2) {$A_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, 2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, -2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, 2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, 3) {$\sigma^2_{X}$};

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate
  \node[draw, circle, fill=gray!20] (b2) at (3, -4) {$b_{i2}$}; % Adjusted to align with middle row

  % Arrows with path coefficients
  \draw [->] (b0) -- (Y2);
  \draw [->] (b0) -- (Y3);
  \draw [->] (A2) edge node[above left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[above] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[above left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[above] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);
  \draw [->] (b0) edge (X1);
  \draw [->] (b0) edge (X2);
  \draw [->] (b0) edge (X3);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);
  \draw [<->, out=315, in=225, looseness=2] (b2) to node[below] {$\sigma^2_{b2}$} (b2);

  % Additional paths for random effects
  \draw[->, dashed] (b2) -- (1.5,-1); 
  \draw[->, dashed] (b2) -- (5,-1.2); 

\end{tikzpicture}
```

Path Diagrams for Generative Models G, A, B and C (t = 1, 2, 3)
:::

*Note.* Random effects are represented by grey circles, observed variables by squares and relationships across variables by arrows, where dashed lines are reserved for random slopes.

\vspace{2em}

In GM G, the covariate $X_{it}$ is directly influenced by unobserved individual factors (represented by the random effects, $b_{i0}$). Consequently, conditioning on prior variables, such as the outcome at the previous timepoint $Y_{it}$, does not fully block or eliminate the influence of these unobserved factors. As a result, $X_{it}$ remains dependent on the random effects, violating the assumption. This violation of the conditional independence assumption aligns with the biased estimates of the treatment effect observed in GM G, as identified by @qian2020.

<!-- that $X_{it}$ should be independent of these unobserved factors once we account for prior variables -->

In contrast, GM A, a special case of GM G where no bias was found by @qian2020, removes the direct link between $X_{it}$ and the random effects $b_{i0}$. In this case, $X_{it}$ is simply the previous outcome $Y_{it}$ plus some random noise. While there remains an indirect connection between $X_{it}$ and $b_{i0}$ through $Y_{it}$, conditioning on $Y_{it}$ effectively "breaks the link" between $X_{it}$ and the random effects, satisfying the conditional independence assumption.

For GM B and GM C, the direct link between the random effects and $X_{it}$ remains, as in GM G. As a result, these models also violate the conditional independence assumption, suggesting the presence of bias in treatment effect estimates.

In summary, GM G, B, and C violate the conditional independence assumption, which suggests that we would expect biased treatment effect estimates for these models. In contrast, GM A satisfies the assumption, supporting unbiased estimates of the treatment effect.

<!-- causal effects can be identified by blocking non-causal paths through conditioning on appropriate variables, such as relevant confounders—common causes that induce spurious relationships. If spurious backdoor paths remain unblocked, bias persists in treatment effect estimates [@Kim2021a]. -->

<!-- This addresses the classical problem in causal inference: causal effects cannot be directly observed and must be inferred from associations, which often include both causal and non-causal or *spurious* components [@Holland1986]. When causal effects can be isolated under ideal conditions (e.g., no measurement error, infinite sample size), they are said to be *identified*. According to the backdoor criterion, causal effects can be identified by blocking non-causal paths through conditioning on appropriate variables, such as relevant confounders—common causes that induce spurious relationships. If spurious backdoor paths remain unblocked, bias persists in treatment effect estimates [@Kim2021a]. -->

<!-- The second criterion for evaluating the presence of bias in treatment effect estimates is the *backdoor criterion* [@pearl1988; @pearl2009]. This criterion provides a partial solution to the classical problem in causal inference: *causal effects* cannot be directly observed and must instead be inferred from observed associations, which often represent a mixture of causal effects and various undesirable non-causal, or *spurious*, components [@Holland1986]. When it is possible, under ideal conditions (e.g., no measurement error, infinite sample size), to isolate the causal effect from a combination of causal and spurious components, the causal effect is said to be identified. According to the backdoor criterion [@pearl1988; @pearl2009], causal effects can be identified by blocking non-causal paths through conditioning on appropriate variables (e.g., controlling or matching). For instance, a causal effect of exposure on outcome can be identified by including in the analysis (i.e., controlling for) all relevant confounders—common causes that would otherwise induce spurious relationships. However, if spurious paths remain unblocked due to unmeasured variables or measurement error, the treatment and outcome remain linked via backdoor paths, leading to biased estimates of the treatment effect [@Kim2021a]. -->

<!-- To detect the presence of such backdoor paths, directed acyclic graphs (DAGs) [@pearl1995; @pearl2009] are invaluable tools. DAGs generalize conventional linear path diagrams [@wright1934a; @duncan1966] and operate in a fully nonparametric framework. Unlike traditional path diagrams, DAGs make no assumptions about distributional properties (e.g., multivariate normality) or functional forms (e.g., linearity). Instead, they encode qualitative causal assumptions about the data-generating process in the population. Arrows connecting nodes indicate direct causal effects, which may vary in magnitude across individuals (effect heterogeneity) or depend on the values of other variables (effect interaction or modification) [@elwert2014]. Notably, random slopes from random-effects models and interaction effects are not explicitly represented in DAGs, which precludes their use for evaluating the conditional independence assumption. -->

## Backdoor Criterion and DAGs

The second criterion for evaluating bias in treatment effect estimates is the *backdoor criterion* [@pearl1988; @pearl2009]. When estimating the treatment effect, the backdoor criterion can help us decide which variables to control for to ensure that we do not obtain a biased estimate of the treatment effect.

To detect backdoor paths, directed acyclic graphs (DAGs) [@pearl1995; @pearl2009] are invaluable tools[^3]. DAGs generalize conventional path diagrams [@wright1934a; @duncan1966] within a fully nonparametric framework. Unlike conventional linear path diagrams and structural equation models, DAGs make no assumptions about distributional properties (e.g., multivariate normality) or functional forms (e.g., linearity). They encode qualitative causal assumptions about the data-generating process, where arrows indicate direct causal effects that may vary across individuals (effect heterogeneity) or depend on other variables (effect interaction or modification) [@elwert2014]. Notably, random slopes from random-effects models and interaction effects are not explicitly represented in DAGs, which precluded their use for evaluating the conditional independence assumption.

[^3]: An accessible introduction into DAGs and backdoor paths can be found in @rohrer2018.

Using the direct causal effects specified in each generative model (GM), we can formulate DAGs for the first three observations, representing the random disturbance $b_{0i}$ as the node $U$ [e.g., @Kim2021a, see @fig-DAGs]. These diagrams confirm that random slopes and fixed interaction effects are absent. Indeed, this absence explains why the DAGs for GMs G, B, and C are equivalent.

::: {#fig-DAGs layout-ncol="2"}
```{r}
#| label: fig-GMG_DAG
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM G, B, C"
#| eval: true

\begin{tikzpicture}[node distance=1cm]
  % Nodes for observed variables
  \node (X1) at (0, 2) {$X_{1}$};
  \node (A1) at (0, -2) {$A_{1}$};
  \node (Y2) at (3, 0) {$Y_{2}$};
  \node (X2) at (3, 2) {$X_{2}$};
  \node (A2) at (3, -2) {$A_{2}$};
  \node (Y3) at (6, 0) {$Y_{3}$};
  \node (X3) at (6, 2) {$X_{3}$};
  \node (U) at (4.5, 3) {$U$};

  % Directed edges
  \draw [->] (U) -- (Y2);
  \draw [->] (U) -- (Y3);
  \draw [->] (U) -- (X1);
  \draw [->] (U) -- (X2);
  \draw [->] (U) -- (X3);
  \draw [->] (A1) -- (Y2);
  \draw [->] (X1) -- (Y2);
  \draw [->] (Y2) -- (X2);
  \draw [->] (A2) -- (Y3);
  \draw [->] (X2) -- (Y3);
  \draw [->] (Y3) -- (X3);
\end{tikzpicture}
```

```{r}
#| label: fig-GMA_DAG
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM A"
#| eval: true

\begin{tikzpicture}[node distance=1cm]
  % Nodes for observed variables
  \node (X1) at (0, 2) {$X_{1}$};
  \node (A1) at (0, -2) {$A_{1}$};
  \node (Y2) at (3, 0) {$Y_{2}$};
  \node (X2) at (3, 2) {$X_{2}$};
  \node (A2) at (3, -2) {$A_{2}$};
  \node (Y3) at (6, 0) {$Y_{3}$};
  \node (X3) at (6, 2) {$X_{3}$};
  \node (U) at (4.5, 3) {$U$};

  % Directed edges
  \draw [->] (U) -- (Y2);
  \draw [->] (U) -- (Y3);
  \draw [->] (A2) -- (Y3);
  \draw [->] (X2) -- (Y3);
  \draw [->] (Y2) -- (X2);
  \draw [->] (A1) -- (Y2);
  \draw [->] (X1) -- (Y2);
  \draw [->] (Y3) -- (X3);
\end{tikzpicture}
```

DAGs for Generative Models G, A, B and C (t = 1, 2, 3)
:::

*Note.* The node $U$ represents the random disturbance $b_{0i}$ in the generative models.

\vspace{2em}

We now apply the backdoor criterion to these DAGs to assess potential violations. Application of the backdoor criterion begins by identifying all paths that connect the treatment $A_t$ to the outcome $Y_{t+1}$, excluding the direct treatment effect itself. Next, we determine whether these paths can transmit association by applying the d-separation rules[^4]. A path that contains an arrow pointing to the treatment $A_t$ is considered a *backdoor path*.

[^4]: For an accessible introduction to d-separation, see [@hayduk2003].

<!-- Note that if the treatment has a parent, an indirect pathway may exist through the treatment, linking it to the outcome.  -->

<!-- To block all backdoor paths, we identify a set of variables, $Z$, that, when conditioned on, renders the treatment $A_t$ and the outcome $Y_{t+1}$ independent. This set of variables is known as a *backdoor set* [@pearl2009]. -->

Examining the DAGs presented in @fig-DAGs, we observe that none of the GMs contains any path connecting $A_t$ to $Y_{t+1}$ other than the direct treatment effect itself. This confirms that there are no backdoor paths between $A_t$ and $Y_{t+1}$, as $A_t$ lacks any parent nodes. Indeed, the random assignment of the treatment ensures, by design, the absence of backdoor paths and any shared common cause with the outcome. Consequently, the covariate $X_t$ does not need to be controlled for to obtain an unbiased estimate of the total effect.

Additionally, the lack of any pathways connecting $A_t$ to $Y_{t+1}$ other than the direct treatment effect ensures that including $X_t$ does not introduce identification issues. Its inclusion may increase the statistical power to detect the treatment effect. Therefore, according to the backdoor criterion, the inclusion of the time-varying covariate $X_{it}$ should not lead to biased estimates of the treatment effect in any of the generative models.

## Simulation Study

@fig-simulation-results present the simulation results for each of the generative models[^5], where bias refers to the difference between the mean of the estimated parameter values $\bar{{\beta}}_{0}$ and the prespecified treatment effect $\beta_{0}$. As $\beta_{0} = 1$, an absolute bias of 0.05 implies a $5\%$ relative bias.

[^5]: The supplemental table with additional information can be found at the OSF repository: <https://osf.io/8xawt/?view_only=aad6a13b7a4a4d36aed76ed8aac584c4>

```{r}
#| label: fig-simulation-results
#| cache: true
#| echo: false
#| fig-cap: "Estimation bias for the fixed treatment effect $\\beta_0$ of each generative model for different combinations of sample size $N$ and number of timepoints $T$ over 10,000 simulation replications"
#| warning: false
#| fig-height: 13
#| fig-width: 9
#| escape: false

runname <- "GM123ad-1000reps-researchreport-cleanscript_nrep10000" # set a runname

# set the number of simulations
nsim <- 10000

# simulation for Research Report
design <- expand.grid(sample_size = c(30, 100, 200), total_T = c(10, 30), dgm_type = c(1,2,3,"3a","3d"))

# make sure dgm_type is not a factor
design$dgm_type <- as.character(design$dgm_type)

# create dataframe to store results
design$mlm_beta_0_bias <- rep(list(numeric(1000)), nrow(design))

for (idesign in 1:nrow(design)) {
  
    results_list <- readRDS(here(paste0("simulation_results/", runname, "/", idesign, ".RDS")))
    
    dgm_type <- design$dgm_type[idesign]
    
    ### Restate the true values of the parameters ###
    
    beta_0_true <- 1 # was originallly 0.5 in the code but is 1 in the paper
    
    result_lmm <- results_list[grep("solution_lmm", names(results_list))]
    
    ### Extract the Results ###
    
    mlm_beta_0 <- sapply(result_lmm, function(l) l$coef["A", "Estimate"])
    design$mlm_beta_0_bias[[idesign]] <- mlm_beta_0 - beta_0_true
    
}

# remove dgm_type = 2
design <- design[design$dgm_type != 2, ]

# rename dgm_type = 3 to "G", dgm_type = 1 to "A", dgm_type = 3a to "B" and dgm_type = 3d to "C"
design$dgm_type <- ifelse(design$dgm_type == 3, "G", 
                          ifelse(design$dgm_type == 1, "A", 
                                 ifelse(design$dgm_type == "3a", "B", 
                                        ifelse(design$dgm_type == "3d", "C", NA))))

# Combine datasets into one
all_data <- design %>%
  mutate(total_T = as.factor(total_T), 
         sample_size = as.factor(sample_size)) %>%
  unnest(mlm_beta_0_bias) %>%
  mutate(dgm_type = factor(dgm_type, levels = c("G", "A", "B", "C")))

# Turn N and T into strings with an underscore
all_data <- all_data %>%
  mutate(total_T2 = as.character(total_T),
         sample_size2 = as.character(sample_size)) %>%
  mutate(total_T2 = str_replace_all(total_T2, "10", "T = 10"),
         total_T2 = str_replace_all(total_T2, "30", "T = 30"),
         sample_size2 = str_replace_all(sample_size2, "30", "N = 30"),
         sample_size2 = str_replace_all(sample_size2, "100", "N = 100"),
         sample_size2 = str_replace_all(sample_size2, "200", "N = 200")) %>%
  # Set factor levels to ensure correct order in the plot
  mutate(
    total_T2 = factor(total_T2, levels = c("T = 10", "T = 30")),
    sample_size2 = factor(sample_size2, levels = c("N = 30", "N = 100", "N = 200"))
  )

# Create the matrix-style plot
ggplot(all_data, aes(x = dgm_type, y = mlm_beta_0_bias, col = dgm_type)) +
  geom_boxplot() +
  geom_hline(yintercept = 0, linetype = "dashed") +  # Dashed horizontal line at 0
  ylim(-1.0, 0.7) +  # Set y-axis limits
  labs(x = "Generative Model", y = "Bias") +
  facet_grid(sample_size2 ~ total_T2) +  # Show T and N values in labels
  theme(
      axis.text.x = element_text(size = 14, vjust = -0.8),  # Increase font size of x-axis category labels
      axis.text.y = element_text(size = 14),
      axis.title.x = element_text(size = 16, vjust = -1.4),  # Increase font size of x-axis label
      axis.title.y = element_text(size = 16),  # Increase font size of y-axis label
      legend.position = "none",   # Remove legend
      panel.border = element_rect(colour = "black", fill = NA, size = 1),  # Add black border
      plot.margin = margin(5, 5, 12, 5),  # Add space around the plot
      panel.background = element_rect(fill = "white"),  # White background instead of light grey
      panel.grid.major = element_blank(),  # Remove major gridlines
      panel.grid.minor = element_blank(),  # Remove minor gridlines
      axis.line = element_line(colour = "black"),  # Black axis lines
      axis.ticks = element_line(size = 1, color = "black"), # Black ticks
      strip.text.x = element_text(face = "bold", color = "white", size = 15),
      strip.text.y = element_text(face = "bold", color = "white", size = 15),
      strip.background.x = element_rect(fill = "dodgerblue3", linetype = "solid",
                                          color = "black", linewidth = 1),
      strip.background.y = element_rect(fill = "firebrick2", linetype = "solid",
                                          color = "gray30", linewidth = 1)
  )
```

In this reproduction of @qian2020, the overall pattern was consistent with the original study: we observed substantial absolute bias ranging from $0.023$ ($2.3\%$) to $0.056$ ($5.6\%$) for the most general generative model (GM G) and no discernible bias ($\leq 0.002$ or $0.2\%$) for GM A. These results align with expectations based on the conditional independence assumption, but contradict the backdoor criterion, which predicted the absence of bias for all the GMs. As shown in @fig-simulation-results, the magnitude of the bias in GM G decreased as the number of time points ($T$) increased. While a larger sample size ($N$) reduced the variability of the bias across replications, it did not reduce the bias itself. This indicates the bias is systematic, and the estimates are inconsistent, meaning they do not converge to the true value as $N$ increases.

Similar to GM A, we observed no discernible bias for GM B ($\leq 0.005$ or $0.5\%$) and GM C ($\leq 0.003$ or $0.3\%$). These findings align with the backdoor criterion’s prediction of no bias but contradict the expectations based on the conditional independence assumption, which suggests the presence of bias. Additionally, GM B showed much smaller variability across simulation replications compared to all other GMs. In contrast, the remaining models exhibited comparable levels of variability (see @fig-simulation-results).

In summary, these findings suggest that if the underlying GM did not include the direct dependency of the random intercept on the covariate (GM A), the random slope $b_{i2}$ (GM B), or the interaction term $\beta_1$ (GM C), as in GM G, the bias disappears. However, neither the backdoor criterion nor the conditional independence assumption provided consistent predictions of treatment effect bias across all models.

# Discussion

## Main Findings

In this research report, we evaluated several GMs to investigate when endogenous time-varying covariates bias treatment effect estimates in MLMs under randomized treatment. We also assessed the ability of the conditional independence assumption and the backdoor criterion to predict when this bias will occur. We observed biased and inconsistent estimates in the most general model (GM G), but not in the three special cases, that either did not contain a direct effect of the random intercept on the covariate (GM A), a random slope for treatment (GM B), or an interaction term between treatment and covariate (GM C). The conditional independence assumption correctly predicted bias in GM G and GM A, but not in GM B and GM C. The backdoor criterion predicted no bias in any of the GMs, which was inconsistent with the bias found in GM G. These findings suggest that, at least in this instance, the dependency between the covariate and treatment, the random slope for treatment, and the interaction effect are all essential for bias to occur.

This naturally raises important questions: why was no discernible bias observed in GM B and GM C, as predicted by the conditional independence assumption, and can this pattern generalize beyond the current generative models? One possibility is that heterogeneity in the treatment effect—both explained by the covariate and influenced by the random slope—is necessary for bias to occur. Both factors appear to be essential in driving the bias, rather than either factor alone. This aligns with @qian2020’s assertion that “applying linear mixed models is problematic because potential moderators of the treatment effect are frequently endogenous” (p. 375). If treatment effect moderation by a covariate is indeed a prerequisite for bias, it could explain the absence of bias in GM C, where no interaction term was included. However this would not explain why the bias disappears in the model without the random slope (GM B). More generally, note of caution is due here since the current study only considered a specific set of GMs. Further research is needed to determine whether these findings generalize or are specific to the evaluated GMs, thereby informing practical recommendations for using MLMs with endogenous time-varying covariates.

Regarding the backdoor criterion and DAGs [@pearl1988; @pearl2009], our results suggest that the classical non-parametric DAG may be insufficient to identify bias in GM G. While the arrows in DAGs may contain interaction effects[^6] and effect heterogeneity, these are not explicitly defined as such in the DAG. This precluded their use for evaluating the conditional independence assumption. Similar concerns regarding the use of the DAG with Pearl's backdoor criterion in situations with interaction effects have been raised [@weinberg2007; @attia2022]. Future research could explore to what extent proposed extensions of the DAG---that incorporate interaction effects---may allow the backdoor criterion to identify bias in the treatment effect estimates.

[^6]: Note that the term "effect modification", while often used interchangeably with "interaction", has a distinct definition in the counterfactual framework [@vanderweele2009].

Another avenue for future investigation is the role of centering approaches (see @hamaker2020 for an overview). According to @antonakis2021, the assumption of uncorrelatedness between the random effects and level 1 covariates can be relaxed by using Mundlak's contextual model[^7] [@mundlak1978]: adding the cluster means of each covariate as predictor of the random intercept. Such an approach of explicitly modeling the source of endogeneity, as advocated by @bell2015, may help to gain new points of contact  for understanding the source of treatment effect bias in GM G, and why it is absent in GMs A, B And C.

[^7]: This is referred to as the Correlated Random Effects (CRE) approach by @wooldridge2002.

Finally, @qian2020 only considered independence between the random intercept and random slope, and perfect correlation between the random intercept of the outcome and the covariate. These two assumptions are restrictive and may be violated in practice. Exploring cases where the random intercept and random slope are correlated and/or where the random intercepts are not identical across covariate and outcome could provide further insight.

<!-- Similarly, it would be valuable to examine the implications of endogenous covariates for other longitudinal data analysis methods, such as dynamic structural equation modeling (DSEM; a widely used approach in the social sciences based on the MLM). -->

## Conclusion

Dating all the way back to the work of @pepe1994 it has been known that the endogeneity of time-varying covariates can result in biased parameter estimates. This research report is a first step towards understanding the work of @qian2020, who illustrated that this issue can also affect MLMs with randomized treatment. To recognize and understand completely when and why endogenous covariates may trouble an empirical investigation, further research is needed.

<!-- Future studies should aim to generalize these findings, clarify the role of treatment effect moderation, and explore solutions such as DAG extensions, centering approaches, and alternative longitudinal modeling frameworks.  -->

<!-- With regard to the special case without random slope for treatment, the bias, as well as the variability across simulations, was noticably smaller compared to the other GMs. -->

<!-- The current research report leaves several avenues unexplored. -->

<!-- **Maar bij GEE kun je ook centreren per persoon, en dan krijg je ook weer iets anders, dus ik zou het iets anders opschrijven hier** Second, it is unclear how exactly the divide between the literatures pertaining to the focus of the MLM on different centering methods and within- and between-person interpretations and the focus of the GEE on marginal and conditional interpretations may be bridged. Consequently, future research could assess the implications of centering methods in MLMs on the extent to which the marginal interpretation of MLM breaks down. -->

<!-- # Other ideas -->

<!-- -   Initially, it seemed that the issue of endogenous covariates meant that, for unbiased estimation of the treatment effect, we should rely on GEE with independence. However, it is important not to conflate the issues mentioned by @qian2020. The first issue pertains to model interpretation: should we expect covariates to be endogenous and be primarily interested in marginal interpretations of the parameters rather than the person-specific (conditional-on-the-random-effect) interpretation, we should indeed employ GEE with working independence (OR STRUCTURAL MARGINAL MODELS??? CHECK THIS OUT IN ZOTERO). The second issue pertains to model fitting: once we conclude that person-specific interpretation aligns with our interest but we fear the presence of endogenous covariates, we have to assess the conditional independence assumption. -->

<!-- -   Across all generative model, we generally found that the estimated fixed treatment effect differed more from the specified MLM effect for the analytical GEE models than for the analytical MLM model. This is rather unsurprising, considering that the MLM is analyzing the exact same model as was specified, thereby putting it at an advantage over the GEE. -->

<!-- -   While GEE with independence may indeed yield unbiased estimators of the marginal effect as mentioned by @qian2020, they do not reflect the value of the model parameter. And since the endogeneity of a covariate implies that it is determined by the random effect, thereby making the marginal relationship between any given $X_{it}$ and $Y_{it+1}$ different as shown by @qian2020 (section 2.2), it is unclear what utility the combined marginal effect may serve in this context. -->

<!-- -   They set the same seed for every setting, potentially making the first dataset the same for every setting. Nevertheless, properly randomizing does not seem to drastically impact the results. The settings with GM2, $T = 30$ and $N \geq 100$ of @qian2020 results in very implausible values for the covariate $X_{it}$ and outcome $Y_{it+1}$, often exceeding a million. This may imply the presence of a non-stationary process (e.g., unit root). In the case of GEE analytical models, these settings result in very large estimates. -->

<!-- -   wasn't working exchangeability in GEE equivalent to MLM? then use this to argue why using this GEE variation. Because for the difference of GEE with independence from the actual specified conditional parameter estimates should indicate a difference between the marginal and conditional effect, but this is not bias. Can we speak of bias with exchangeability? -->

<!--     -   no it depends on the random effects, and it was compound symmetry. Leave this for next time. -->

\newpage

# References

::: {#refs}
:::

<!-- --- -->

<!-- nocite: | -->

<!--   @* -->

<!-- --- -->
