---
title: "Untangling Bias in Multilevel Linear Models: The Role of Endogenous Time-Varying Covariates"

# "The Dangers of Including Time-Varying Endogenous Covariates in the Multilevel Linear Model"
# Estimation of Effects of Endogenous Time-Varying Covariates: A Comparison Of Multilevel Linear Modeling and Generalized Estimating Equations"
subtitle: "Research Report" 
author: 
  - name: "Ward B. Eiling (9294163)"
    orcid: "0009-0007-8114-9497"
    affiliation: 
      - name: "Utrecht University"
      - department: "Methodology and Statistics"
date: last-modified # deadline: "Dec 22, 2024"
date-format: long
format: # docx
  pdf:
    papersize: a4
    fig-pos: 'H'
    tbl-pos: 'H'
    # keep-tex: true
    # toc: true
    number-sections: true
    colorlinks: true
    indent: true
    header-includes:
      - \usepackage{fancyhdr}
      - \usepackage{amsmath}
      - \usepackage{multirow}
      - \usepackage{booktabs}
      - \usepackage{pifont}
      # - \usepackage{float}
      # - \usepackage{tikz}
      # - \usepackage{subcaption}
    geometry:
      - top=2.5cm
      - bottom=2.5cm
      - left=2.5cm
      - right=2.5cm
    mainfont: Latin Modern Roman
    sansfont: Latin Modern Roman
    linestretch: 2
    fontsize: 11pt
    template-partials:
      - "before-body.tex"
bibliography: references.bib
link-citations: true
csl: apa.csl
execute:
  message: false
  warning: false
fig-cap-location: top
---

```{r}
#| label: Packages
#| echo: false

# for knitting the document
library(papaja)
library(rlang)
library(rmarkdown)
library(knitr)
library(kableExtra)
library(here)
# for general data manipulation and plotting
library(tidyverse)
# for estimation
# library(lme4)
# library(geepack)
# for diagrams
library(dagitty)
library(ggdag)
library(DiagrammeR)
library(magick)
library(pdftools)
```

# Introduction

<!-- -   Start with a paragraph describing a problem in the real-life world (so that a BoS member not familiar with statistics understands why you are pursuing research in this direction); -->

Across a wide range of disciplines, researchers analyze clustered longitudinal, observational data to investigate prospective causal relationships between variables. When analyzing such data, psychological researchers most commonly use the multilevel linear model[^1] [MLM, @bauer2011], which—in the context of longitudinal data analysis—partitions observed variance into stable between-person differences and within-person fluctuations [@hamaker2020]. Research questions explored with the MLM commonly lead to the availability of time invariant and/or time-varying covariates, the latter measured repeatedly over time. The inclusion of covariates is a common strategy to improve parameter precision [@boruvka2018] and address bias introduced by (time-varying) confounders [@daniel2013; @Wodtke2020; @Robins2000]. Nevertheless, this approach is not universally beneficial, as conditioning on endogenous covariates—those influenced by (prior) treatment/exposure or outcome—can create challenges for standard methods like MLMs, which implicitly assume the exogeneity of covariates [@Erler2019].

[^1]: The MLM is known by various names in different substantive fields, including: linear mixed model, hierarchical linear model, random-effect model and mixed-effects model.

<!-- -   Then, add a paragraph describing what is known in the literature; -->

<!-- -   Also, describe what is not known and which gap you will address in your thesis; End with a clear research question and, if applicable, your hypothesis (for confirmatory research questions – it should be a testable hypothesis) or expectations (for exploratory research questions – can be much vaguer because of the explorative nature of the question). -->

Dating back to the work of @pepe1994, this assumption has proven to be non-trivial when endogenous covariates vary over time. In fact, their inclusion in longitudinal studies can lead to biased treatment effect estimates, an issue that, despite its significance, has received limited attention in psychological research. Building on this foundation, a recent paper by @qian2020 examined the suitability of MLM for estimating the causal effect of a time-varying exposure or treatment. Specifically, they focused on settings where the exposure is randomly assigned at each occasion within individuals. Such randomized exposures may include, for example, prompts delivered through push notifications to remind participants of cognitive or mindfulness-based strategies [@nahum-shani2021; @walton2018]. While random assignment with a constant probability might seem sufficient to identify (the presence and absence of) causal effects, @qian2020 showed that model fitting issues and parameter bias can arise when a *time-varying endogenous covariate* is present.

<!-- The issues described in @qian2020 can be categorized into two main problems: (1) model interpretation and (2) model fitting. In this investigation, we focus on the latter issue of model fitting of the MLM  -->

However, due to a divide between the disciplines that employ the MLM, such critiques appear to have largely failed to reach the applied researcher in psychology. One specific reason might be that the technical jargon in other disciplines makes it difficult for researchers to recognize when and how these issues emerge. This report aims to explore why @qian2020 observed biased estimates of the treatment effect in certain data-generating mechanisms containing endogenous covariates, while not for others. Additionally, it seeks to explain this issue to an audience of psychologists. The study will first employ graphical diagrams to assess two criteria across various scenarios involving an endogenous time-varying covariate and randomized treatment: (a) path diagrams to evaluate the conditional independence assumption introduced by @qian2020 and (b) directed acyclic graphs (DAGs) to assess the backdoor criterion [@pearl1988; @pearl2009]. Subsequently, data simulations based on @qian2020's original scenarios, along with additional ones, will be performed to reproduce and isolate the underlying issue and evaluate whether these criteria can effectively detect bias in the treatment effect. The following research question will be addressed: *When does the inclusion of endogenous variables in multilevel linear models result in biased estimates of the treatment effect?*

<!-- -   Also, describe what is not known and which gap you will address in your thesis; End with a clear research question and, if applicable, your hypothesis (for confirmatory research questions – it should be a testable hypothesis) or expectations (for exploratory research questions – can be much vaguer because of the explorative nature of the question). -->

# Methods

In this section, the GMs will be formulated and specifications will be defined for the simulation study.

## Data Generation

We consider two GMs from @qian2020, one (GM A) being a special case of the general model (GM G) where bias was detected. To further isolate the source of bias, we introduce two additional special cases, labeled GM B and C. @tbl-gm-differences summarizes the differences between the generative models. Compared to the general model G, GM A is not directly determined by the random intercept $b_{i0}$; GM B is does not have a random slope $b_{i2}$ for treatment; and GM C does not have a fixed interaction effect $\beta_1$ between covariate and treatment.

| Generative Model | Name in @qian2020 | dependency $b_{i0}$ and $X_{it}$ | random slope treatment $b_{i2}$ | interaction $\beta_1$ |
|---------------|---------------|---------------|---------------|---------------|
| G(eneral)        | 3                 | $\checkmark$                     | $\checkmark$                    | $\checkmark$          |
| A                | 1                 | $\times$                         | $\checkmark$                    | $\checkmark$          |
| B                | NA                | $\checkmark$                     | $\times$                        | $\checkmark$          |
| C                | NA                | $\checkmark$                     | $\checkmark$                    | $\times$              |

: Generative Models: Summary of Differences {#tbl-gm-differences}

The details of the generative models are described below.

### General Generative Model

Following the original notation of @qian2020, the outcome of GM G was generated as:

$$
Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + b_{i0} + A_{it} (\beta_0 + \beta_1 X_{it} + b_{i2}) + \epsilon_{it+1}
$$

where $Y_{it+1}$ is the outcome for person $i$ at time $t+1$, $X_{it}$ is the covariate for person $i$ at time $t$, $A_{it}$ is the treatment for person $i$ at time $t$, $b_{i0}$ is the random intercept, $b_{i2}$ is the random slope for the treatment, and $\epsilon_{it+1}$ is the error term. Alternatively, the model can be represented in the multilevel notation of @raudenbush2002 with at the within-person level (level 1)

$$ 
Y_{it+1} = \pi_{0i} + \pi_{1i} X_{it} + \pi_{2i} A_{it} + \pi_{3i} A_{it} X_{it} + \epsilon_{it+1}
$$

and at the between-person level (level 2)

$$
\begin{aligned}
\pi_{0i} &= \alpha_0 + b_{i0}, & \text{where} \quad b_{i0} &\sim \mathcal{N}(0, \sigma_{b0}^2), \\
\pi_{1i} &= \alpha_1, \\
\pi_{2i} &= \beta_0 + b_{i2}, & \text{where} \quad b_{i2} &\sim \mathcal{N}(0, \sigma_{b2}^2), \\
\pi_{3i} &= \beta_1.
\end{aligned}
$$

The parameters $\alpha_0$, $\alpha_1$, $\beta_0$, and $\beta_1$ are fixed effects that are constant across individuals, while $b_{i0}$ and $b_{i2}$ are independent random effects that capture individual-specific deviations from population parameters. The presence of the interaction term $\beta_1$ implies treatment heterogeneity: the effect of the treatment $A_{it}$ on the outcome depends on the value of the covariate $X_{it}$. $b_{i0}$ represents deviations from the population intercept $\alpha_0$, and $b_{i2}$ represents deviations from the population slope $\beta_0$.

The covariate is generated as:

$$
X_{it} = 
\begin{cases} 
b_{i0} + \epsilon_{X_{it}} & \text{if } t = 1, \\
b_{i0} + Y_{it} + \epsilon_{X_{it}} & \text{if } t \geq 2,
\end{cases}
\quad \text{where} \quad \epsilon_{X_{it}} \sim \mathcal{N}(0, 1)
$$

The treatment randomization probability is constant at $p_t = 0.5$, so $A_{it} \sim \text{Bernoulli}(0.5)$ for all $i$ and $t$. In other words, for every given person $i$ and every timepoint $t$, the probability that treatment is assigned is equivalent to a fair coinflip. The exogenous noise is $\epsilon_{it+1} \sim \mathcal{N}(0, \sigma_\epsilon^2)$.

@fig-GMG_path shows the path diagram for GM G.

The following parameter values were adapted from @qian2020:

$$ \alpha_0 = -2, \quad \alpha_1 = -0.3, \quad \beta_0 = 1, \quad \beta_1 = 0.3, $$

$$ \sigma_{b0}^2 = 4, \quad \sigma_{b2}^2 = 1, \quad \sigma_\epsilon^2 = 1. $$

### Special Cases

GM A is a special case of GM G, where the effect of the random intercept $b_{i0}$ on the covariate $X_{it}$ is set to zero. This results in a model where the covariate $X_{it}$ is not directly determined by the random intercept $b_{i0}$ (see @fig-GMA_path). Instead, the endogenous covariate $X_{it}$ equals the previous outcome $Y_{it}$ plus some random noise:

$$
X_{it} = 
\begin{cases} 
\epsilon_{X_{it}} & \text{if } t = 1, \\
Y_{it} + \epsilon_{X_{it}} & \text{if } t \geq 2,
\end{cases}
\quad \text{where} \quad \epsilon_{X_{it}} \sim \mathcal{N}(0, 1)
$$

GM B is a special case of GM G in which the random slope $b_{i2}$ was removed (see @fig-GMB_path) by setting the random slope variance $\sigma_{b2}^2$ to zero. While the within-person model is the same as GM G, there is a slight alteration in the between-person model:

$$ \pi_{2i} = \beta_0. $$

The composite model then becomes:

$$
Y_{it+1} = (\alpha_0 + b_{i0}) + \alpha_1 X_{it} + \beta_0 A_{it} + \beta_1 A_{it} X_{it} + \epsilon_{it+1}
$$

GM C is a special case of GM G, where the fixed interaction parameter $\beta_1$ is set to zero, which implies the removal of the interaction term $\beta_1 A_{it} X_{it}$ (see @fig-GMC_path). This, in turn, removed $\pi_{3i}$, thereby creating a discrepancy in within-person model of GM C and GM G:

$$Y_{it+1} = \pi_{0i} + \pi_{1i} X_{it} + \pi_{2i} A_{it} + \epsilon_{it+1}.$$

Nevertheless, the between-person model of $\pi_{0i}$, $\pi_{1i}$ and $\pi_{2i}$ remains the same as GM G. The single equation model then becomes:

$$
Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + b_{i0} + A_{it} (\beta_0 + b_{i2}) + \epsilon_{it+1}.
$$

## Data Analysis

All data generation and estimation was performed in `R`, version 4.4.2 [@rcoreteam2024]. After the generation of data generation for any given setting, analytical MLMs were fit that contain all the fixed and random parameters present in each of the respective data generating models. To fit the MLM, the `lmer` function from the R-package `lme4` [@bates2015] was employed with restricted maximum likelihood estimation.

In the simulation study, we evaluated the performance of the analytical models across a total of `r 4*2*3` different settings, each replicated 1,000 times, by systematically varying the following factors:

-   **Generative Models (GM):** G, A, B, C

-   **Number of timepoints (T):** 10, 30

-   **Sample size (N):** 30, 100, 200

# Results

## Conditional Independence and Path Diagrams

The first criterion for evaluating the presence of bias in treatment effect estimates is the *conditional independence assumption*, introduced by @qian2020 and based on the work of @sitlani2012. According to @qian2020, this assumption should identify whether estimators of the treatment effect are consistent and unbiased under randomized treatment assignment. The conditional independence assumption states that the covariate at time $t$ ($X_{it}$) should be independent of the individual’s random effects (intercept $b_{i0}$ and slope(s) $b_{i1}$) once we account for their history of covariates up to timepoint $t-1$ ($H_{it-1}$), previous treatments ($A_{it-1}$), and prior outcomes ($Y_{it}$).

$$
X_{it} \perp (b_{i0}, b_{i1}) \mid H_{it-1}, A_{it-1}, Y_{it}.
$$

<!-- where $b_{i0}$ and $b_{i1}$ represent the random intercept and random slope(s), respectively, and $H_{it-1}$ comprises all observations of that covariate before timepoint $t$.  -->

This assumption allows for $X_{it}$ to be influenced by earlier variables (e.g., outcomes or treatments) but not directly by unobserved individual characteristics (i.e., random effects). However, as @qian2020 highlights, ensuring this assumption holds requires careful consideration of theory and domain knowledge. To clarify the application of the conditional independence assumption, we pair the equations of the generative models (GMs) with path diagrams [@duncan1966; @wright1934a] illustrating the first three timepoints ($t$) for each model (see @fig-Pathdiagrams).

<!-- If the included endogenous covariates are only affected by prior outcomes and treatments, the assumption is automatically satisfied.  -->

::: {#fig-Pathdiagrams layout-ncol="2"}
```{r}
#| label: fig-GMG_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM G"
#| eval: true
#| out.width: "100%"

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, 2) {$X_{1i}$};
  \node (X1var) at (0, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, -2) {$A_{1i}$};
  \node[draw, rectangle] (A1X1) at (0.3, 0) {$A_{1i}\cdot X_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, 2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, -2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, 2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2X2) at (5.3, 0) {$A_{2i}\cdot X_{2i}$}; % Increased x-coordinate

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate
  \node[draw, circle, fill=gray!20] (b2) at (3, -4) {$b_{i2}$}; % Adjusted to align with middle row

  % Arrows with path coefficients
  \draw [->] (b0) -- (Y2);
  \draw [->] (b0) -- (Y3);
  \draw [->] (A2) edge node[above left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[above] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[above left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[above] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (A1X1) edge node[above] {$\beta_1$} (Y2);
  \draw [->] (A2X2) edge node[above left] {$\beta_1$} (Y3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);
  \draw [->] (b0) edge (X1);
  \draw [->] (b0) edge (X2);
  \draw [->] (b0) edge (X3);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);
  \draw [<->, out=315, in=225, looseness=2] (b2) to node[below] {$\sigma^2_{b2}$} (b2);

  % Additional paths for random effects
  \draw[->, dashed] (b2) -- (1.5,-1); 
  \draw[->, dashed] (b2) -- (5,-1.2); 

\end{tikzpicture}
```

```{r}
#| label: fig-GMA_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM A"
#| eval: true
#| out.width: "100%"

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, 2) {$X_{1i}$};
  \node (X1var) at (0, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, -2) {$A_{1i}$};
  \node[draw, rectangle] (A1X1) at (0.3, 0) {$A_{1i}\cdot X_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, 2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, -2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, 2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2X2) at (5.3, 0) {$A_{2i}\cdot X_{2i}$}; % Increased x-coordinate

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate
  \node[draw, circle, fill=gray!20] (b2) at (3, -4) {$b_{i2}$}; % Adjusted to align with middle row

  % Arrows with path coefficients
  \draw [->] (b0) -- (Y2);
  \draw [->] (b0) -- (Y3);
  \draw [->] (A2) edge node[above left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[above] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[above left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[above] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (A1X1) edge node[above] {$\beta_1$} (Y2);
  \draw [->] (A2X2) edge node[above left] {$\beta_1$} (Y3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);
  \draw [<->, out=315, in=225, looseness=2] (b2) to node[below] {$\sigma^2_{b2}$} (b2);

  % Additional paths for random effects
  \draw[->, dashed] (b2) -- (1.5,-1); 
  \draw[->, dashed] (b2) -- (5,-1.2); 

\end{tikzpicture}
```

```{r}
#| label: fig-GMB_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM B"
#| eval: true
#| out.width: "100%"

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, 2) {$X_{1i}$};
  \node (X1var) at (0, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, -2) {$A_{1i}$};
  \node[draw, rectangle] (A1X1) at (0.3, 0) {$A_{1i}\cdot X_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, 2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, -2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, 2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2X2) at (5.3, 0) {$A_{2i}\cdot X_{2i}$}; % Increased x-coordinate

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate

  % Arrows with path coefficients
  \draw [->] (b0) -- (Y2);
  \draw [->] (b0) -- (Y3);
  \draw [->] (A2) edge node[above left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[above] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[above left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[above] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (A1X1) edge node[above] {$\beta_1$} (Y2);
  \draw [->] (A2X2) edge node[above left] {$\beta_1$} (Y3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);
  \draw [->] (b0) edge (X1);
  \draw [->] (b0) edge (X2);
  \draw [->] (b0) edge (X3);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);

\end{tikzpicture}
```

```{r}
#| label: fig-GMC_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM C"
#| eval: true
#| out.width: "100%"

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, 2) {$X_{1i}$};
  \node (X1var) at (0, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, -2) {$A_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, 2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, -2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, 2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, 3) {$\sigma^2_{X}$};

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate
  \node[draw, circle, fill=gray!20] (b2) at (3, -4) {$b_{i2}$}; % Adjusted to align with middle row

  % Arrows with path coefficients
  \draw [->] (b0) -- (Y2);
  \draw [->] (b0) -- (Y3);
  \draw [->] (A2) edge node[above left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[above] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[above left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[above] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);
  \draw [->] (b0) edge (X1);
  \draw [->] (b0) edge (X2);
  \draw [->] (b0) edge (X3);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);
  \draw [<->, out=315, in=225, looseness=2] (b2) to node[below] {$\sigma^2_{b2}$} (b2);

  % Additional paths for random effects
  \draw[->, dashed] (b2) -- (1.5,-1); 
  \draw[->, dashed] (b2) -- (5,-1.2); 

\end{tikzpicture}
```

Path Diagrams for Generative Models G, A, B and C (t = 1, 2, 3)
:::

*Note.* Random effects are represented by grey circles, observed variables by squares and relationships across variables by arrows, where dashed lines are reserved for random slopes.

\vspace{2em}

In GM G, the covariate $X_{it}$ is directly influenced by unobserved individual factors (represented by the random effects, $b_{i0}$). Consequently, conditioning on prior variables, such as the outcome at the previous timepoint $Y_{it}$, does not fully block or eliminate the influence of these unobserved factors. As a result, $X_{it}$ remains dependent on the random effects, violating the assumption. This violation of the conditional independence assumption aligns with the biased estimates of the treatment effect observed in GM G, as identified by @qian2020.

<!-- that $X_{it}$ should be independent of these unobserved factors once we account for prior variables -->

In contrast, GM A, a special case of GM G where no bias was found by @qian2020, removes the direct link between $X_{it}$ and the random effects $b_{i0}$. In this case, $X_{it}$ is simply the previous outcome $Y_{it}$ plus some random noise. While there remains an indirect connection between $X_{it}$ and $b_{i0}$ through $Y_{it}$, conditioning on $Y_{it}$ effectively "breaks the link" between $X_{it}$ and the random effects, satisfying the conditional independence assumption.

For GM B and GM C, the direct link between the random effects and $X_{it}$ remains, as in GM G. As a result, these models also violate the conditional independence assumption, suggesting the presence of bias in treatment effect estimates.

<!-- In summary, GM G, B, and C violate the conditional independence assumption, which suggests that we would expect biased treatment effect estimates for these models. In contrast, GM A satisfies the assumption, supporting unbiased estimates of the treatment effect. -->

## Backdoor Criterion and DAGs

The second criterion for evaluating bias in treatment effect estimates is the *backdoor criterion* [@pearl1988; @pearl2009]. This addresses the classical problem in causal inference: causal effects cannot be directly observed and must be inferred from associations, which often include both causal and non-causal or *spurious* components [@Holland1986]. When causal effects can be isolated under ideal conditions (e.g., no measurement error, infinite sample size), they are said to be *identified*. According to the backdoor criterion, causal effects can be identified by blocking non-causal paths through conditioning on appropriate variables, such as relevant confounders—common causes that induce spurious relationships. If spurious backdoor paths remain unblocked, bias persists in treatment effect estimates [@Kim2021a].

<!-- The second criterion for evaluating the presence of bias in treatment effect estimates is the *backdoor criterion* [@pearl1988; @pearl2009]. This criterion provides a partial solution to the classical problem in causal inference: *causal effects* cannot be directly observed and must instead be inferred from observed associations, which often represent a mixture of causal effects and various undesirable non-causal, or *spurious*, components [@Holland1986]. When it is possible, under ideal conditions (e.g., no measurement error, infinite sample size), to isolate the causal effect from a combination of causal and spurious components, the causal effect is said to be identified. According to the backdoor criterion [@pearl1988; @pearl2009], causal effects can be identified by blocking non-causal paths through conditioning on appropriate variables (e.g., controlling or matching). For instance, a causal effect of exposure on outcome can be identified by including in the analysis (i.e., controlling for) all relevant confounders—common causes that would otherwise induce spurious relationships. However, if spurious paths remain unblocked due to unmeasured variables or measurement error, the treatment and outcome remain linked via backdoor paths, leading to biased estimates of the treatment effect [@Kim2021a]. -->

To detect backdoor paths, directed acyclic graphs (DAGs) [@pearl1995; @pearl2009] are invaluable tools. DAGs generalize conventional path diagrams [@wright1934a; @duncan1966] within a fully nonparametric framework. Unlike traditional diagrams, DAGs make no assumptions about distributional properties (e.g., multivariate normality) or functional forms (e.g., linearity). They encode qualitative causal assumptions about the data-generating process, where arrows indicate direct causal effects that may vary across individuals (effect heterogeneity) or depend on other variables (effect interaction or modification) [@elwert2014]. Notably, random slopes from random-effects models and interaction effects are not explicitly represented in DAGs, which precludes their use for evaluating the conditional independence assumption.

<!-- To detect the presence of such backdoor paths, directed acyclic graphs (DAGs) [@pearl1995; @pearl2009] are invaluable tools. DAGs generalize conventional linear path diagrams [@wright1934a; @duncan1966] and operate in a fully nonparametric framework. Unlike traditional path diagrams, DAGs make no assumptions about distributional properties (e.g., multivariate normality) or functional forms (e.g., linearity). Instead, they encode qualitative causal assumptions about the data-generating process in the population. Arrows connecting nodes indicate direct causal effects, which may vary in magnitude across individuals (effect heterogeneity) or depend on the values of other variables (effect interaction or modification) [@elwert2014]. Notably, random slopes from random-effects models and interaction effects are not explicitly represented in DAGs, which precludes their use for evaluating the conditional independence assumption. -->

Using the direct causal effects specified in each generative model (GM), we can formulate DAGs for the first three observations, representing the random disturbance $b_{0i}$ as the node $U$ [e.g., @Kim2021a, see @fig-DAGs]. These diagrams confirm that random slopes and fixed interaction effects are absent. Indeed, this absence explains why the DAGs for GMs G, B, and C are equivalent.

::: {#fig-DAGs layout-ncol="2"}
```{r}
#| label: fig-GMG_DAG
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM G, B, C"
#| eval: true

\begin{tikzpicture}[node distance=1cm]
  % Nodes for observed variables
  \node (X1) at (0, 2) {$X_{1}$};
  \node (A1) at (0, -2) {$A_{1}$};
  \node (Y2) at (3, 0) {$Y_{2}$};
  \node (X2) at (3, 2) {$X_{2}$};
  \node (A2) at (3, -2) {$A_{2}$};
  \node (Y3) at (6, 0) {$Y_{3}$};
  \node (X3) at (6, 2) {$X_{3}$};
  \node (U) at (4.5, 3) {$U$};

  % Directed edges
  \draw [->] (U) -- (Y2);
  \draw [->] (U) -- (Y3);
  \draw [->] (U) -- (X1);
  \draw [->] (U) -- (X2);
  \draw [->] (U) -- (X3);
  \draw [->] (A1) -- (Y2);
  \draw [->] (X1) -- (Y2);
  \draw [->] (Y2) -- (X2);
  \draw [->] (A2) -- (Y3);
  \draw [->] (X2) -- (Y3);
  \draw [->] (Y3) -- (X3);
\end{tikzpicture}
```

```{r}
#| label: fig-GMA_DAG
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM A"
#| eval: true

\begin{tikzpicture}[node distance=1cm]
  % Nodes for observed variables
  \node (X1) at (0, 2) {$X_{1}$};
  \node (A1) at (0, -2) {$A_{1}$};
  \node (Y2) at (3, 0) {$Y_{2}$};
  \node (X2) at (3, 2) {$X_{2}$};
  \node (A2) at (3, -2) {$A_{2}$};
  \node (Y3) at (6, 0) {$Y_{3}$};
  \node (X3) at (6, 2) {$X_{3}$};
  \node (U) at (4.5, 3) {$U$};

  % Directed edges
  \draw [->] (U) -- (Y2);
  \draw [->] (U) -- (Y3);
  \draw [->] (A2) -- (Y3);
  \draw [->] (X2) -- (Y3);
  \draw [->] (Y2) -- (X2);
  \draw [->] (A1) -- (Y2);
  \draw [->] (X1) -- (Y2);
  \draw [->] (Y3) -- (X3);
\end{tikzpicture}
```

DAGs for Generative Models G, A, B and C (t = 1, 2, 3)
:::

*Note.* The red arrows show the biased backdoor path(s) in the treatment efffect (before controlling for $X_{it}$).

\vspace{2em}

We now apply the backdoor criterion to these DAGs to assess potential bias in the treatment effect. For all GMs, there are no backdoor paths in the treatment effect $A_t \to Y_{t+1}$, as $A_t$ lacks any parent nodes. Consequently, covariate $X_t$ need not be controlled to obtain an unbiased total effect. Importantly, including $X_{it}$ does not introduce identification issues, as it is neither a mediator (i.e., on the pathway from $A_t$ to $Y_{t+1}$) nor a collider (i.e., a common effect of $A_t$ and $Y_{t+1}$) in the treatment effect. Therefore, according to the backdoor criterion, the inclusion of the time-varying covariate $X_{it}$ should not result in biased estimates of the treatment effect in any of the generative models.

## Simulation Study

@tbl-simulation-results and @fig-simulation-results present the simulation results for each of the generative models. The bias in @tbl-simulation-results refers to the difference between the mean of the estimated parameter values $\bar{\hat{\beta}}_{0}$ and the prespecified treatment effect $\beta_{0}$. As $\beta_{0} = 1$, an absolute bias of 0.05 implies a $5\%$ relative bias.

```{r}
#| label: fig-simulation-results
#| cache: true
#| echo: false
#| fig-cap: "Estimation bias for the fixed treatment effect $\\beta_0$ of each generative model for different combinations of sample size N and number of timepoints T over 1000 simulation replications"
#| fig-subcap: 
#| - "T=10, N=30"
#| - "T=10, N=100"
#| - "T=10, N=200"
#| - "T=30, N=30"
#| - "T=30, N=100"
#| - "T=30, N=200"
#| warning: false
#| layout-ncol: 2
#| fig-height: 4.8
#| escape: false

runname <- "GM123ad-1000reps-researchreport-cleanscript" # set a runname

# set the number of simulations
nsim <- 1000

# simulation for Research Report
design <- expand.grid(sample_size = c(30, 100, 200), total_T = c(10, 30), dgm_type = c(1,2,3,"3a","3d"))

# make sure dgm_type is not a factor
design$dgm_type <- as.character(design$dgm_type)

# create dataframe to store results
design$mlm_beta_0_bias <- rep(list(numeric(1000)), nrow(design))

for (idesign in 1:nrow(design)) {
  
    results_list <- readRDS(here(paste0("simulation_results/", runname, "/", idesign, ".RDS")))
    
    dgm_type <- design$dgm_type[idesign]
    
    ### Restate the true values of the parameters ###
    
    beta_0_true <- 1 # was originallly 0.5 in the code but is 1 in the paper
    
    result_lmm <- results_list[grep("solution_lmm", names(results_list))]
    
    ### Extract the Results ###
    
    mlm_beta_0 <- sapply(result_lmm, function(l) l$coef["A", "Estimate"])
    design$mlm_beta_0_bias[[idesign]] <- mlm_beta_0 - beta_0_true
    
}

# remove dgm_type = 2
design <- design[design$dgm_type != 2, ]

# rename dgm_type = 3 to "G", dgm_type = 1 to "A", dgm_type = 3a to "B" and dgm_type = 3d to "C"
design$dgm_type <- ifelse(design$dgm_type == 3, "G", 
                          ifelse(design$dgm_type == 1, "A", 
                                 ifelse(design$dgm_type == "3a", "B", 
                                        ifelse(design$dgm_type == "3d", "C", NA))))

# Convert T and N to factors if they are numeric
design$total_T <- as.factor(design$total_T)
design$sample_size <- as.factor(design$sample_size)

data_t10_n30 <- design[design$total_T == 10 & design$sample_size == 30, ]
data_t10_n100 <- design[design$total_T == 10 & design$sample_size == 100, ]
data_t10_n200 <- design[design$total_T == 10 & design$sample_size == 200, ]
data_t30_n30 <- design[design$total_T == 30 & design$sample_size == 30, ]
data_t30_n100 <- design[design$total_T == 30 & design$sample_size == 100, ]
data_t30_n200 <- design[design$total_T == 30 & design$sample_size == 200, ]

# Function to unnest and plot
plot_boxplot <- function(data, dataset_name) {
  # Unnest the bias values (assuming mlm_beta_0_bias is the list column)
  data_long <- data %>%
    unnest(mlm_beta_0_bias) %>%
    mutate(T_N = interaction(total_T, sample_size))  # Create combined T-N factor
  
  # Reorder the levels of dgm_type so that "G" comes first
  data_long$dgm_type <- factor(data_long$dgm_type, levels = c("G", "A", "B", "C"))
  
  # Create the boxplot
  p <- ggplot(data_long, aes(x = dgm_type, y = mlm_beta_0_bias, col = dgm_type)) + 
    geom_boxplot() +
    ylim(-1.0, 0.7) + # Set y-axis limits to match each subfigure
    geom_hline(yintercept = 0, linetype = "dashed") +  # Dashed horizontal line at 0
    labs(x = "Generative Model", # title = paste("Boxplot for", dataset_name), 
         y = "Bias") +
    theme(
      axis.text.x = element_text(size = 14, vjust = -0.8),  # Increase font size of x-axis category labels
      axis.text.y = element_text(size = 14),
      axis.title.x = element_text(size = 16, vjust = -0.8),  # Increase font size of x-axis label
      axis.title.y = element_text(size = 16),  # Increase font size of y-axis label
      legend.position = "none",   # Remove legend
      panel.border = element_rect(colour = "black", fill = NA, size = 1),  # Add black border
      plot.margin = margin(5, 5, 12, 5),  # Add space around the plot
      panel.background = element_rect(fill = "white"),  # White background instead of light grey
      panel.grid.major = element_blank(),  # Remove major gridlines
      panel.grid.minor = element_blank(),  # Remove minor gridlines
      axis.line = element_line(colour = "black"),  # Black axis lines
      axis.ticks = element_line(size = 1, color = "black") # Black ticks
    ) 
  print(p)
}

# Example for each dataset:
plot_boxplot(data_t10_n30, "T=10, N=30")
plot_boxplot(data_t10_n100, "T=10, N=100")
plot_boxplot(data_t10_n200, "T=10, N=200")
plot_boxplot(data_t30_n30, "T=30, N=30")
plot_boxplot(data_t30_n100, "T=30, N=100")
plot_boxplot(data_t30_n200, "T=30, N=200")
```

In this reproduction of @qian2020, the overall pattern was consistent with the original study (see @fig-simulation-results): we observed substantial absolute bias ranging from $0.023$ ($2.3\%$) to $0.064$ ($6.4\%$) for the most general generative model (GM G), and much smaller bias of $\leq 0.015$ ($1.5\%$) for GM A. These results align with expectations based on the conditional independence assumption, which predicts that the treatment effect would be unbiased for GM A and biased for GM G. However, the findings contradict the backdoor criterion, which predicts no bias for any of the generative models (GMs). Notably, we found 4 times greater treatment effect bias for GM A in the scenario with $T = 10$ and $N = 100$ than reported by @qian2020, with a maximum bias of $-0.012$ ($1.2\%$; (see @tbl-simulation-results), compared to $-0.003$ ($0.3\%$) found by @qian2020. For GM G, the size of the bias decreased as the number of time points increased.

For the two additional special cases of GM G, namely GM B and GM C, we observed even smaller absolute bias than for GM A: $\leq 0.010$ ($1\%$) for GM C and $\leq 0.005$ ($0.5\%$) for GM B (see @tbl-simulation-results). These findings align with the backdoor criterion’s prediction of no bias but contradict the expectations based on the conditional independence assumption, which suggests the presence of bias. Additionally, GM B exhibited the smallest absolute bias overall and showed much smaller variability across simulation replications compared to all other GMs. In contrast, the remaining models exhibited comparable levels of variability (see @fig-simulation-results and @tbl-simulation-results).

In summary, these findings suggest that if the underlying GM did not include the direct dependency of the random intercept on the covariate (GM A), the random slope $b_{i2}$ (GM B), or the interaction term $\beta_1$ (GM C), as in GM G, the bias either disappears or becomes negligible. However, neither the backdoor criterion nor the conditional independence assumption provided consistent predictions of treatment effect bias across all models.

# Discussion

## Main Findings

In this research report, we evaluated several GMs to investigate when endogenous time-varying covariates bias treatment effect estimates in MLMs under randomized treatment. We also assessed the ability of the conditional independence assumption and the backdoor criterion to predict this bias. Consistent with @qian2020 and the conditional independence assumption, we observed substantially greater bias in the most general model (GM G) compared to a special case (GM A), where the direct effect of the random intercept on the covariate was removed. However, one unexpected result was the fourfold increase in maximum bias for GM A compared to @qian2020. This discrepancy may stem from differences in the simulation setup (e.g., random number generation or handling of warnings/errors). The two additional special cases of GM G, where the random slope for treatment (GM B) or the interaction term between treatment and covariate (GM C) was removed, exhibited even smaller bias in the treatment effect---despite violations of the conditional independence assumption. These findings tell us that—at least in this particular instance---the dependency between the covariate and treatment, the random slope for treatment, and the interaction effect are all essential for bias to occur.

This naturally raises important questions: why was no discernible bias observed in GM B and GM C, as predicted by the conditional independence assumption, and can this pattern generalize beyond the current generative models? One possibility is that, while bias may exist as predicted, it was canceled out by parameter removal in these specific models. Alternatively, heterogeneity in the treatment effect---whether unexplained via the random slope or explained via the covariate---may both be necessary for bias to occur. This explanation aligns with the statement by @qian2020 that "applying linear mixed models is problematic because potential moderators of the treatment effect are frequently endogenous" (p. 375). If treatment effect moderation by a covariate is indeed a prerequisite for bias, it could explain the absence of bias in GM C, where no interaction term was included. However, it remains unclear why @qian2020 does not explicitly mention this condition (e.g., when introducing the conditional independence assumption). Further research is needed to determine whether these findings generalize or are specific to the evaluated GMs, thereby informing practical recommendations for using MLMs with endogenous time-varying covariates.

Regarding the backdoor criterion and DAGs [@pearl1988; @pearl2009], our results suggest that the classical non-parametric DAG may be insufficient to identify bias in GM G. While DAGs account for direct causal effects, they do not impose restrictions on random slopes or interaction effects[^2], which are central to the conditional independence assumption. Similar concerns regarding the use of the DAG with Pearl's backdoor criterion in situations with interaction effects have been raised [@weinberg2007; @attia2022]. Future research could explore to what extent proposed extensions of the DAG---that incorperate interaction effects---may allow the backdoor criterion to identify bias in the treatment effect estimates.

[^2]: Note that the term "effect modification", while often used interchangeably with "interaction", has a distinct definition in the counterfactual framework [@vanderweele2009].

It should also be noted that the current investigation takes for granted that the marginal (population-averaged) interpretation of the treatment effect estimators of the MLM may not be valid due to the presence of endogenous time-varying covariates [@diggle2002, @pepe1994]. While, the conditional-on-the-random-effect interpretation of the parameters is often aligned with the scientific interest in psychology, future research would benefit from a more comprehensive intergration of insights from the literature on marginal effects.

Another avenue for future investigation is the role of centering approaches (see @hamaker2020 for an overview). Namely, @antonakis2021 notes that the assumption of uncorrelatedness between the random effects and level 1 covariates can be relaxed by using Mundlak's contextual model[^3] [@mundlak1978]: adding the cluster means of each covariate as predictor of the random intercept. Such an approach of explicitly modeling the source of endogeneity, as advocated by @bell2015, may further clarify the treatment effect bias in GM G.

[^3]: This is referred to as the Correlated Random Effects (CRE) approach by @wooldridge2002.

Finally, @qian2020 only considered independent random effects, a restrictive assumption that may be violated in practice. Exploring correlated random effects using structural equation modeling frameworks [@rovine2000] could provide further insight.

<!-- Similarly, it would be valuable to examine the implications of endogenous covariates for other longitudinal data analysis methods, such as dynamic structural equation modeling (DSEM; a widely used approach in the social sciences based on the MLM). -->

## Conclusion

This report is a first step towards understanding the implications of endogenous covariates in multilevel linear models. However, to recognize and understand completely when and why endogenous covariates may trouble an empirical investigation, further research is needed.

<!-- Future studies should aim to generalize these findings, clarify the role of treatment effect moderation, and explore solutions such as DAG extensions, centering approaches, and alternative longitudinal modeling frameworks.  -->

<!-- With regard to the special case without random slope for treatment, the bias, as well as the variability across simulations, was noticably smaller compared to the other GMs. -->

<!-- The current research report leaves several avenues unexplored. -->

<!-- **Maar bij GEE kun je ook centreren per persoon, en dan krijg je ook weer iets anders, dus ik zou het iets anders opschrijven hier** Second, it is unclear how exactly the divide between the literatures pertaining to the focus of the MLM on different centering methods and within- and between-person interpretations and the focus of the GEE on marginal and conditional interpretations may be bridged. Consequently, future research could assess the implications of centering methods in MLMs on the extent to which the marginal interpretation of MLM breaks down. -->

<!-- # Other ideas -->

<!-- -   Initially, it seemed that the issue of endogenous covariates meant that, for unbiased estimation of the treatment effect, we should rely on GEE with independence. However, it is important not to conflate the issues mentioned by @qian2020. The first issue pertains to model interpretation: should we expect covariates to be endogenous and be primarily interested in marginal interpretations of the parameters rather than the person-specific (conditional-on-the-random-effect) interpretation, we should indeed employ GEE with working independence (OR STRUCTURAL MARGINAL MODELS??? CHECK THIS OUT IN ZOTERO). The second issue pertains to model fitting: once we conclude that person-specific interpretation aligns with our interest but we fear the presence of endogenous covariates, we have to assess the conditional independence assumption. -->

<!-- -   Across all generative model, we generally found that the estimated fixed treatment effect differed more from the specified MLM effect for the analytical GEE models than for the analytical MLM model. This is rather unsurprising, considering that the MLM is analyzing the exact same model as was specified, thereby putting it at an advantage over the GEE. -->

<!-- -   While GEE with independence may indeed yield unbiased estimators of the marginal effect as mentioned by @qian2020, they do not reflect the value of the model parameter. And since the endogeneity of a covariate implies that it is determined by the random effect, thereby making the marginal relationship between any given $X_{it}$ and $Y_{it+1}$ different as shown by @qian2020 (section 2.2), it is unclear what utility the combined marginal effect may serve in this context. -->

<!-- -   They set the same seed for every setting, potentially making the first dataset the same for every setting. Nevertheless, properly randomizing does not seem to drastically impact the results. The settings with GM2, $T = 30$ and $N \geq 100$ of @qian2020 results in very implausible values for the covariate $X_{it}$ and outcome $Y_{it+1}$, often exceeding a million. This may imply the presence of a non-stationary process (e.g., unit root). In the case of GEE analytical models, these settings result in very large estimates. -->

<!-- -   wasn't working exchangeability in GEE equivalent to MLM? then use this to argue why using this GEE variation. Because for the difference of GEE with independence from the actual specified conditional parameter estimates should indicate a difference between the marginal and conditional effect, but this is not bias. Can we speak of bias with exchangeability? -->

<!--     -   no it depends on the random effects, and it was compound symmetry. Leave this for next time. -->

\newpage

# References

::: {#refs}
:::

<!-- --- -->

<!-- nocite: | -->

<!--   @* -->

<!-- --- -->

# Appendix (MAAK SUPPLEMTAL MATERIALS AAN!!!)

::: {#tbl-simulation-results}
```{r}
#| label: simulation-results
#| echo: false
#| cache: true
#| escape: false

results <- readRDS(here("simulation_results/GM123ad-1000reps-researchreport-cleanscript/results_beta0_bias_sd_success.rds"))

# remove gee_ex and gee_ar1 and gee_ind
results <- results[, !grepl("gee_ex|gee_ar1|gee_ind", colnames(results))]
# remove GM2
results <- results[results$GM != 2, ]
# rename GM = 3 to "G", GM = 1 to "A", GM = 3a to "B" and GM = 3d to "C"
results$GM <- ifelse(results$GM == 3, "G", 
                     ifelse(results$GM == 1, "A", 
                            ifelse(results$GM == "3a", "B", 
                                   ifelse(results$GM == "3d", "C", NA))))

# compute MC-SE
results <- results %>%
  mutate(mc_se = mlm_beta0_sd / sqrt(1000))

# reorder columns
results <- results[, c(1:5,7,6)]

# reset row numbers
rownames(results) <- NULL
# reorder so that model GM is in the order G, A, B, C

results <- results[c(7:12, 1:6, 13:24), ]

# Create a kableExtra table with a second header
kableExtra::kbl(results, "latex", booktabs = T,  escape = F, digits = c(0, 0, 0, 3, 3, 3, 3), col.names = c("GM", "T", "N", "Bias", "SD", "MC-SE", "SR"), row.names = FALSE) %>%
  add_header_above(c(" " = 3, '$\\\\beta_0$' = 3, " " = 1), escape = F) %>%
  kable_styling(full_width = TRUE) %>%
  column_spec(1, width = "5em") %>%
  collapse_rows(columns = 1:2, valign = "middle") %>%
  kableExtra::kable_classic_2()
```

\vspace{2em}

*Note.* GM: generative model. T: number of timepoints. N: sample size. SD: $\sqrt{\frac{1}{(n_{\text{sim}} - 1)} \sum\limits_{i=1}^{n_{\text{sim}}} \left(\hat{\beta}_{0i} - \bar{\beta}_{0}\right)^2}$, which is the standard deviation of estimates across replications. SR: model fitting success rate. Bias: $\frac{1}{n_{\text{sim}}} \sum\limits_{i = 1}^{n_{\text{sim}}} \hat{\beta}_{0i} - \beta_{0}$ [@morris2019], which represents the difference between the mean of the estimated parameter values $\hat{\beta}_{0}$ and the prespecified treatment effect $\beta_{0} = 1$. MC-SE: $\sqrt{\frac{1}{n_{\text{sim}}(n_{\text{sim}}-1)} \sum\limits_{i = 1}^{n_{\text{sim}}} \left(\hat{\beta}_{0i} - \bar{\beta}_{0}\right)^2} = \frac{\text{SD}}{\sqrt{n_{\text{sim}}}}$, which represents the Monte Carlo SE of bias [@morris2019].

Treatment effect bias for Generative Models G, A, B and C over 1000 replications
:::
