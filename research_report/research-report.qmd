---
title: "Treatment Effect Bias in Multilevel Linear Models under Time-Varying Endogeneity: A New Look at Qian et al. (2020)"
# The Impact of Endogenous Time-Varying Covariates on Treatment Effect Estimates in Multilevel Linear Models: Revisiting Qian et al. (2020)
# "Untangling Bias in Multilevel Linear Models: The Role of Endogenous Time-Varying Covariates"
# "The Dangers of Including Time-Varying Endogenous Covariates in the Multilevel Linear Model"
# Estimation of Effects of Endogenous Time-Varying Covariates: A Comparison Of Multilevel Linear Modeling and Generalized Estimating Equations"
subtitle: "Research Report" 
author: 
  - name: "Ward B. Eiling (9294163)"
    orcid: "0009-0007-8114-9497"
    affiliation: 
      - name: "Utrecht University"
      - department: "Methodology and Statistics"
date: last-modified # deadline: "Dec 22, 2024"
date-format: long
format: #docx
  pdf:
    papersize: a4
    fig-pos: 'H'
    tbl-pos: 'H'
    # keep-tex: true
    # toc: true
    number-sections: true
    colorlinks: true
    indent: true
    header-includes:
      - \usepackage{fancyhdr}
      - \usepackage{amsmath}
      - \usepackage{multirow}
      - \usepackage{booktabs}
      - \usepackage{pifont}
      # - \usepackage{float}
      # - \usepackage{tikz}
      # - \usepackage{subcaption}
    geometry:
      - top=2.5cm
      - bottom=2.5cm
      - left=2.5cm
      - right=2.5cm
    mainfont: Latin Modern Roman
    sansfont: Latin Modern Roman
    linestretch: 2
    fontsize: 11pt
    template-partials:
      - "before-body.tex"
bibliography: references.bib
link-citations: true
csl: apa7.csl
execute:
  message: false
  warning: false
fig-cap-location: top
---

```{r}
#| label: Packages
#| echo: false

# for knitting the document
library(papaja)
library(rlang)
library(rmarkdown)
library(knitr)
library(kableExtra)
library(here)
# for general data manipulation and plotting
library(tidyverse)
# for estimation
# library(lme4)
# library(geepack)
# for diagrams
library(dagitty)
library(ggdag)
library(DiagrammeR)
library(magick)
library(pdftools)
```

# Introduction

<!-- -   Start with a paragraph describing a problem in the real-life world (so that a BoS member not familiar with statistics understands why you are pursuing research in this direction); -->

Across a wide range of disciplines, researchers analyze clustered longitudinal, observational data to investigate prospective causal relationships between variables. When analyzing such data, psychological researchers most commonly use the multilevel linear model[^1] [MLM, @bauer2011], which—in the context of longitudinal data analysis—partitions observed variance into stable between-person differences and within-person fluctuations [@hamaker2020]. Research questions explored with the MLM often result in the availability of time invariant and/or time-varying covariates, the latter measured repeatedly over time. The inclusion of covariates is a common strategy to improve parameter precision [@boruvka2018] and address bias introduced by confounders [@daniel2013; @Robins2000]. Nevertheless, this approach is not universally beneficial, as conditioning on endogenous covariates—those influenced by (prior) treatment/exposure or outcome—can create challenges for standard methods like MLMs, which implicitly assume the exogeneity of covariates [@Erler2019].

[^1]: The MLM is known by various names in different substantive fields, including: linear mixed model, hierarchical linear model, random-effect model and mixed-effects model.

<!-- -   Then, add a paragraph describing what is known in the literature; -->

<!-- -   Also, describe what is not known and which gap you will address in your thesis; End with a clear research question and, if applicable, your hypothesis (for confirmatory research questions – it should be a testable hypothesis) or expectations (for exploratory research questions – can be much vaguer because of the explorative nature of the question). -->

Dating back to the work of Pepe and Anderson [-@pepe1994] in the biostatistics literature, this assumption has been shown to be non-trivial when endogenous covariates vary over time. In fact, their inclusion in longitudinal studies can lead to biased treatment effect estimates, an issue that, despite its significance, has received limited attention in psychological research. Building on this foundation, a recent paper by @qian2020 examined the suitability of MLM for estimating the causal effect of a time-varying exposure or treatment. Specifically, they focused on settings where the exposure is randomly assigned at each occasion within individuals. Such randomized exposures may include, for example, prompts delivered through push notifications to remind participants of cognitive or mindfulness-based strategies [@nahum-shani2021; @walton2018]. While random assignment with a constant probability might seem sufficient to identify (the presence and absence of) causal effects, @qian2020 showed that model fitting issues and parameter bias can arise when a *time-varying endogenous covariate* is present.

<!-- The issues described in @qian2020 can be categorized into two main problems: (1) model interpretation and (2) model fitting. In this investigation, we focus on the latter issue of model fitting of the MLM  -->

However, due to a divide between the disciplines that employ the MLM, such critiques appear to have largely failed to reach the applied researcher in psychology. One specific reason might be that the technical jargon in other disciplines makes it difficult for researchers to recognize when and how these issues emerge. This report aims to explore why @qian2020 observed biased estimates of the treatment effect in certain data-generating mechanisms containing endogenous covariates, while not for others. Additionally, it seeks to explain this issue to an audience of psychologists. The study will first employ graphical diagrams to assess two criteria across various models involving an endogenous time-varying covariate and randomized treatment: (a) path diagrams to evaluate the conditional independence assumption introduced by @qian2020 and (b) directed acyclic graphs (DAGs) to assess the backdoor criterion [@pearl1988; @pearl2009]. Subsequently, data simulations based on @qian2020's original scenarios, along with additional ones, will be performed to reproduce and isolate the underlying issue and evaluate whether these criteria can predict the presence of bias in the treatment effect. The following research question will be addressed: *When does the inclusion of endogenous variables in multilevel linear models result in biased estimates of the treatment effect?*

<!-- -   Also, describe what is not known and which gap you will address in your thesis; End with a clear research question and, if applicable, your hypothesis (for confirmatory research questions – it should be a testable hypothesis) or expectations (for exploratory research questions – can be much vaguer because of the explorative nature of the question). -->

# Methods

In this section, we describe four generative models (GMs) that incorporate a time-varying endogenous covariate and randomized treatment, followed by the methodology used to evaluate treatment effect bias across settings.

## Data Generation

We consider two GMs from @qian2020, one (GM-A) being a special case of the general model (GM-G) where bias was detected. To further isolate the source of bias, we introduce two additional special cases, GM-B and GM-C. We first describe the GM-G in detail, and then proceed to its three special cases.

### General Model: GM-G

Following the original notation of @qian2020, the outcome of GM-G was generated as:

$$
Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + b_{i0} + A_{it} (\beta_0 + \beta_1 X_{it} + b_{i2}) + \epsilon_{it+1}
$$ {#eq-modelG}

where $Y_{it+1}$ is the outcome, $X_{it}$ the covariate, $A_{it}$ the treatment, $b_{i0}$ the random intercept, $b_{i2}$ the random slope for treatment, and $\epsilon_{it+1}$ the error term. The observed variables vary across individuals $i$ and timepoints $t$. Alternatively, the model can be rewritten in the multilevel notation of Raudenbush and Bryk [-@raudenbush2002], with at the within-person level (level 1):

$$ 
\begin{aligned} Y_{it+1} &= \alpha_0 + \alpha_1 X_{it} + b_{i0} + A_{it} (\beta_0 + \beta_1 X_{it} + b_{i2}) + \epsilon_{it+1} \\ &= (\alpha_0 + b_{i0}) + \alpha_1 X_{it} + (\beta_0 + b_{i2}) A_{it} + \beta_1 A_{it} X_{it} + \epsilon_{it+1} \\ &= \pi_{0i} + \pi_{1i} X_{it} + \pi_{2i} A_{it} + \pi_{3i} A_{it} X_{it} + \epsilon_{it+1}. \end{aligned}
$$

and at the between-person level (level 2):

$$
\begin{aligned}
\pi_{0i} &= \alpha_0 + b_{i0}, & \text{where} \quad b_{i0} &\sim \mathcal{N}(0, \sigma_{b0}^2), \\
\pi_{1i} &= \alpha_1, \\
\pi_{2i} &= \beta_0 + b_{i2}, & \text{where} \quad b_{i2} &\sim \mathcal{N}(0, \sigma_{b2}^2), \\
\pi_{3i} &= \beta_1.
\end{aligned}
$$

The parameters $\alpha_0 = -2$, $\alpha_1 = -0.3$, $\beta_0 = 1$, and $\beta_1 = 0.3$ are fixed effects that are constant across individuals. Conversely, $b_{i0}$ and $b_{i2}$ are independent random effects that capture individual-specific deviations from population parameters $\alpha_0$ and $\beta_0$ respectively. The presence of the interaction term $\beta_1$ implies treatment heterogeneity: the effect of $A_{it}$ on $Y_{it+1}$ depends on the value of $X_{it}$. The random intercept $b_{i0}$, random slope $b_{i2}$ and exogenous noise $\epsilon_{it+1}$ are assumed to be normally distributed with mean zero and variance $\sigma_{b0}^2 = 4$,  $\sigma_{b2}^2 = 1$ and $\sigma_\epsilon^2 = 1$, respectively.

The covariate is generated as:

$$
X_{it} = 
\begin{cases} 
b_{i0} + \epsilon_{X_{it}} & \text{if } t = 1, \\
b_{i0} + Y_{it} + \epsilon_{X_{it}} & \text{if } t \geq 2,
\end{cases}
\quad \text{where} \quad \epsilon_{X_{it}} \sim \mathcal{N}(0, 1)
$$

The treatment randomization probability is constant at $p_t = 0.5$, so $A_{it} \sim \text{Bernoulli}(0.5)$, implying that for every given person $i$ and every timepoint $t$, the probability that treatment is assigned is equivalent to a fair coinflip. Relationships between variables are illustrated in @fig-GMG_path.

### Special Cases: GM-A, GM-B and GM-C

We consider three special cases of GM-G, namely GM-A, GM-B and GM-C. The relation of each special case to GM-G is summarized in @tbl-gm-differences and the specifics are described below.

<!-- Compared to the general model G, GM-A is not directly determined by the random intercept $b_{i0}$; GM-B does not have a random slope $b_{i2}$ for treatment; and GM-C does not have a fixed interaction effect $\beta_1$ between covariate and treatment. Below we discuss the specifics of each special case. -->

| Generative Model | Name in @qian2020 | dependency $b_{i0}$ and $X_{it}$ | random slope treatment $b_{i2}$ | interaction $\beta_1$ |
|---------------|---------------|---------------|---------------|---------------|
| G(eneral)        | 3                 | $\checkmark$                     | $\checkmark$                    | $\checkmark$          |
| A                | 1                 | $\times$                         | $\checkmark$                    | $\checkmark$          |
| B                | NA                | $\checkmark$                     | $\times$                        | $\checkmark$          |
| C                | NA                | $\checkmark$                     | $\checkmark$                    | $\times$              |

: Summary of Differences Between Generative Models {#tbl-gm-differences}

GM-A is a special case of GM-G, where the effect of $b_{i0}$ on $X_{it}$ is set to zero, which implies that $X_{it}$ is not directly determined by $b_{i0}$ (see @fig-GMA_path). Instead, $X_{it}$ equals the previous outcome $Y_{it}$ plus some random noise:

$$
X_{it} = 
\begin{cases} 
\epsilon_{X_{it}} & \text{if } t = 1, \\
Y_{it} + \epsilon_{X_{it}} & \text{if } t \geq 2,
\end{cases}
\quad \text{where} \quad \epsilon_{X_{it}} \sim \mathcal{N}(0, 1)
$$

GM-B is a special case of GM-G in which $b_{i2}$ was removed (see @fig-GMB_path) by setting $\sigma_{b2}^2 = 0$. While the within-person model is the same as GM-G, there is a slight alteration at the between-person level:

$$ \pi_{2i} = \beta_0. $$

The composite model then becomes:

$$
Y_{it+1} = (\alpha_0 + b_{i0}) + \alpha_1 X_{it} + \beta_0 A_{it} + \beta_1 A_{it} X_{it} + \epsilon_{it+1}
$$ {#eq-modelB}

GM-C is a special case of GM-G, where we set $\beta_1 = 0$, which implies the removal of the interaction term $\beta_1 A_{it} X_{it}$ (see @fig-GMC_path) and $\pi_{3i}$. Therefore, the within-person model from GM-G changes into:

$$Y_{it+1} = \pi_{0i} + \pi_{1i} X_{it} + \pi_{2i} A_{it} + \epsilon_{it+1}.$$

The composite model becomes:

$$
Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + b_{i0} + A_{it} (\beta_0 + b_{i2}) + \epsilon_{it+1}.
$$ {#eq-modelC}

## Data Analysis

Data generation and estimation were performed in `R`, version 4.4.2 [@rcoreteam2024], following @qian2020's methodology for consistency. After data generation, analytical MLMs with restricted maximum likelihood estimation were fit using the `lmer` function from the `lme4` package [@bates2015]. The MLM from @eq-modelG was fit for GM-G and A, the MLM from @eq-modelB for GM-B, and the MLM from @eq-modelC for GM-C[^2].

[^2]: Contrary to the data generating models, the analytical models do not model the covariate $X_{it}$ and the treatment $A_{it}$.

In the simulation study, we evaluated the bias of the analytical models across different settings by systematically varying the following 3 factors: GM with the levels G, A, B, and C; number of timepoints (T) with the levels 10 and 30; and sample size (N) with the levels 30, 100, and 200. By varying these factors, 24 unique settings were created, each replicated 10,000 times. Bias estimates were calculated as the difference between the mean of the estimated parameter values $\bar{{\beta}}_{0}$ (across replications) and the true treatment effect $\beta_{0} = 1$.

# Results

This section begins by constructing predictions about treatment effect bias for each GM, guided by the conditional independence assumption and the backdoor criterion. These predictions are then compared with the simulation study results, which present the estimated bias across the different GMs.

## Conditional Independence and Path Diagrams

The first criterion for evaluating the presence of bias in treatment effect estimates is the *conditional independence assumption*, introduced by @qian2020 and based on the work of @sitlani2012. According to @qian2020, this assumption should identify whether estimators of the treatment effect are consistent and unbiased under randomized treatment assignment. The assumption states that the covariate at time $t$ ($X_{it}$) should be independent of the individual’s random effects (intercept $b_{i0}$ and slope(s) $b_{i1}$) once we account for their history of covariates up to timepoint $t-1$ ($H_{it-1}$), previous treatment ($A_{it-1}$), and prior outcome ($Y_{it}$):

$$
X_{it} \perp (b_{i0}, b_{i1}) \mid H_{it-1}, A_{it-1}, Y_{it}.
$$

This assumption allows for $X_{it}$ to be influenced by earlier variables but not directly by random effects (i.e., unobserved individual characteristics). Evaluation of this assumption must be done based on domain knowledge [@qian2020]. To clarify the application of the conditional independence assumption, we pair the equations of the GMs with path diagrams [@duncan1966; @wright1934a] (see @fig-Pathdiagrams).

::: {#fig-Pathdiagrams layout-ncol="2"}
```{r}
#| label: fig-GMG_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM-G"
#| eval: true
#| out.width: "100%"

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, 2) {$X_{1i}$};
  \node (X1var) at (0, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, -2) {$A_{1i}$};
  \node[draw, rectangle] (A1X1) at (0.3, 0) {$A_{1i}\cdot X_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, 2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, -2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, 2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2X2) at (5.3, 0) {$A_{2i}\cdot X_{2i}$}; % Increased x-coordinate

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate
  \node[draw, circle, fill=gray!20] (b2) at (3, -4) {$b_{i2}$}; % Adjusted to align with middle row

  % Arrows with path coefficients
  \draw [->] (b0) -- (Y2);
  \draw [->] (b0) -- (Y3);
  \draw [->] (A2) edge node[above left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[above] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[above left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[above] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (A1X1) edge node[above] {$\beta_1$} (Y2);
  \draw [->] (A2X2) edge node[above left] {$\beta_1$} (Y3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);
  \draw [->] (b0) edge (X1);
  \draw [->] (b0) edge (X2);
  \draw [->] (b0) edge (X3);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);
  \draw [<->, out=315, in=225, looseness=2] (b2) to node[below] {$\sigma^2_{b2}$} (b2);

  % Additional paths for random effects
  \draw[->, dashed] (b2) -- (1.5,-1); 
  \draw[->, dashed] (b2) -- (5,-1.2); 

\end{tikzpicture}
```

```{r}
#| label: fig-GMA_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM-A"
#| eval: true
#| out.width: "100%"

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, 2) {$X_{1i}$};
  \node (X1var) at (0, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, -2) {$A_{1i}$};
  \node[draw, rectangle] (A1X1) at (0.3, 0) {$A_{1i}\cdot X_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, 2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, -2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, 2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2X2) at (5.3, 0) {$A_{2i}\cdot X_{2i}$}; % Increased x-coordinate

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate
  \node[draw, circle, fill=gray!20] (b2) at (3, -4) {$b_{i2}$}; % Adjusted to align with middle row

  % Arrows with path coefficients
  \draw [->] (b0) -- (Y2);
  \draw [->] (b0) -- (Y3);
  \draw [->] (A2) edge node[above left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[above] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[above left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[above] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (A1X1) edge node[above] {$\beta_1$} (Y2);
  \draw [->] (A2X2) edge node[above left] {$\beta_1$} (Y3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);
  \draw [<->, out=315, in=225, looseness=2] (b2) to node[below] {$\sigma^2_{b2}$} (b2);

  % Additional paths for random effects
  \draw[->, dashed] (b2) -- (1.5,-1); 
  \draw[->, dashed] (b2) -- (5,-1.2); 

\end{tikzpicture}
```

```{r}
#| label: fig-GMB_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM-B"
#| eval: true
#| out.width: "100%"

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, 2) {$X_{1i}$};
  \node (X1var) at (0, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, -2) {$A_{1i}$};
  \node[draw, rectangle] (A1X1) at (0.3, 0) {$A_{1i}\cdot X_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, 2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, -2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, 2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2X2) at (5.3, 0) {$A_{2i}\cdot X_{2i}$}; % Increased x-coordinate

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate

  % Arrows with path coefficients
  \draw [->] (b0) -- (Y2);
  \draw [->] (b0) -- (Y3);
  \draw [->] (A2) edge node[above left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[above] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[above left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[above] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (A1X1) edge node[above] {$\beta_1$} (Y2);
  \draw [->] (A2X2) edge node[above left] {$\beta_1$} (Y3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);
  \draw [->] (b0) edge (X1);
  \draw [->] (b0) edge (X2);
  \draw [->] (b0) edge (X3);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);

\end{tikzpicture}
```

```{r}
#| label: fig-GMC_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM-C"
#| eval: true
#| out.width: "100%"

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, 2) {$X_{1i}$};
  \node (X1var) at (0, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, -2) {$A_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, 2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, -2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, 2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, 3) {$\sigma^2_{X}$};

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate
  \node[draw, circle, fill=gray!20] (b2) at (3, -4) {$b_{i2}$}; % Adjusted to align with middle row

  % Arrows with path coefficients
  \draw [->] (b0) -- (Y2);
  \draw [->] (b0) -- (Y3);
  \draw [->] (A2) edge node[above left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[above] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[above left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[above] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);
  \draw [->] (b0) edge (X1);
  \draw [->] (b0) edge (X2);
  \draw [->] (b0) edge (X3);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);
  \draw [<->, out=315, in=225, looseness=2] (b2) to node[below] {$\sigma^2_{b2}$} (b2);

  % Additional paths for random effects
  \draw[->, dashed] (b2) -- (1.5,-1); 
  \draw[->, dashed] (b2) -- (5,-1.2); 

\end{tikzpicture}
```

Path Diagrams for Generative Models G, A, B and C (t = 1, 2, 3)
:::

*Note.* Random effects are represented by grey circles, observed variables by squares and relationships across variables by arrows, where dashed lines are reserved for random slopes.

\vspace{2em}

In GM-G, the covariate $X_{it}$ is directly influenced by the random intercept $b_{i0}$. Consequently, conditioning on prior variables, such as the outcome at the previous timepoint $Y_{it}$, does not fully block the influence of the random effects. As a result, $X_{it}$ remains dependent on the random effects, violating the assumption. This violation of the conditional independence assumption aligns with the biased estimates of the treatment effect observed in GM-G, as identified by @qian2020.

<!-- that $X_{it}$ should be independent of these unobserved factors once we account for prior variables -->

In contrast, GM-A, a special case of GM-G where no bias was found by @qian2020, removes the direct link between $X_{it}$ and the random effects $b_{i0}$. While there remains an indirect connection between $X_{it}$ and $b_{i0}$ through $Y_{it}$, conditioning on $Y_{it}$ effectively "breaks the link" between $X_{it}$ and the random effects, satisfying the conditional independence assumption.

For GM-B and GM-C, the direct link between the random effects and $X_{it}$ remains, as in GM-G. As a result, these models also violate the conditional independence assumption, suggesting the presence of bias in treatment effect estimates.

## Backdoor Criterion and DAGs

The second criterion for evaluating bias in treatment effect estimates is the *backdoor criterion* [@pearl1988; @pearl2009], which can help us decide which variables to control for. To detect backdoor paths, DAGs [@pearl1995; @pearl2009] are invaluable tools[^3]. Unlike conventional linear path diagrams [@wright1934a; @duncan1966] and structural equation models, DAGs are non-parametric. They encode causal assumptions about the data-generating process, where arrows indicate direct causal effects that may vary across individuals (effect heterogeneity) or depend on other variables (effect interaction or modification) [@elwert2014]. Hence, random slopes and interaction effects are not explicitly represented in DAGs, which precluded their use for evaluating the conditional independence assumption.

[^3]: An accessible introduction into DAGs and backdoor paths can be found in @rohrer2018.

The DAGs for each GM are formulated in @fig-DAGs, which confirm that random slopes and fixed interaction effects are absent. Indeed, this absence explains why the DAGs for GMs G, B, and C are equivalent.

::: {#fig-DAGs layout-ncol="2"}
```{r}
#| label: fig-GMG_DAG
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM-G, GM-B, GM-C"
#| eval: true

\begin{tikzpicture}[node distance=1cm]
  % Nodes for observed variables
  \node (X1) at (0, 2) {$X_{1}$};
  \node (A1) at (0, -2) {$A_{1}$};
  \node (Y2) at (3, 0) {$Y_{2}$};
  \node (X2) at (3, 2) {$X_{2}$};
  \node (A2) at (3, -2) {$A_{2}$};
  \node (Y3) at (6, 0) {$Y_{3}$};
  \node (X3) at (6, 2) {$X_{3}$};
  \node (U) at (4.5, 3) {$U$};

  % Directed edges
  \draw [->] (U) -- (Y2);
  \draw [->] (U) -- (Y3);
  \draw [->] (U) -- (X1);
  \draw [->] (U) -- (X2);
  \draw [->] (U) -- (X3);
  \draw [->] (A1) -- (Y2);
  \draw [->] (X1) -- (Y2);
  \draw [->] (Y2) -- (X2);
  \draw [->] (A2) -- (Y3);
  \draw [->] (X2) -- (Y3);
  \draw [->] (Y3) -- (X3);
\end{tikzpicture}
```

```{r}
#| label: fig-GMA_DAG
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM-A"
#| eval: true

\begin{tikzpicture}[node distance=1cm]
  % Nodes for observed variables
  \node (X1) at (0, 2) {$X_{1}$};
  \node (A1) at (0, -2) {$A_{1}$};
  \node (Y2) at (3, 0) {$Y_{2}$};
  \node (X2) at (3, 2) {$X_{2}$};
  \node (A2) at (3, -2) {$A_{2}$};
  \node (Y3) at (6, 0) {$Y_{3}$};
  \node (X3) at (6, 2) {$X_{3}$};
  \node (U) at (4.5, 3) {$U$};

  % Directed edges
  \draw [->] (U) -- (Y2);
  \draw [->] (U) -- (Y3);
  \draw [->] (A2) -- (Y3);
  \draw [->] (X2) -- (Y3);
  \draw [->] (Y2) -- (X2);
  \draw [->] (A1) -- (Y2);
  \draw [->] (X1) -- (Y2);
  \draw [->] (Y3) -- (X3);
\end{tikzpicture}
```

DAGs for Generative Models G, A, B and C (t = 1, 2, 3)
:::

*Note.* The node $U$ represents the random disturbance $b_{0i}$ in the generative models.

\vspace{2em}

Application of the backdoor criterion begins by identifying all paths that connect $A_t$ to $Y_{t+1}$, excluding the direct treatment effect $A_t \to Y_{t+1}$ itself. Next, we determine whether these paths can transmit association by applying the d-separation rules[^4]. A path that contains an arrow pointing to the treatment $A_t$ is considered a *backdoor path*.

[^4]: For an accessible introduction to d-separation, see [@hayduk2003].

Examining the DAGs presented in @fig-DAGs, we observe that none of the GMs contains any path connecting $A_t$ to $Y_{t+1}$ other than $A_t \to Y_{t+1}$ itself. This confirms that there are no backdoor paths in $A_t \to Y_{t+1}$, as $A_t$ lacks any parent nodes. Indeed, the random assignment of the treatment ensures, by design, the absence of backdoor paths. Consequently, $X_t$ does not need to be controlled for to obtain an unbiased estimate of the total effect. Nevertheless, the inclusion of $X_t$ should not introduce bias in the treatment effect[^5], as it does not lay on a pathway connecting $A_t$ to $Y_{t+1}$.

[^5]: Inclusion of the covariate may, however, increase the power to detect the treatment effect.

## Simulation Study

@fig-simulation-results present the bias estimates for each of the GMs[^6], where an absolute bias of $0.05$ implies a $5\%$ relative bias, since $\beta_{0} = 1$.

[^6]: The supplemental table with additional information can be found at the OSF repository: <https://osf.io/8xawt/?view_only=aad6a13b7a4a4d36aed76ed8aac584c4>

::: {#fig-simulation-results}
![](research-report_files/figure-pdf/fig-simulation-results-1.pdf)

Estimation bias for the fixed treatment effect $\beta_0$ of each generative model for different combinations of sample size $N$ and number of timepoints $T$ over 10,000 simulation replications
:::

*Note.* bias refers to the difference between the mean of the estimated parameter values $\bar{{\beta}}_{0}$ and the prespecified treatment effect $\beta_{0} = 1$.

\vspace{2em}

```{r}
#| label: fig-simulation-results
#| cache: true
#| eval: false
#| echo: false
#| fig-cap: "Estimation bias for the fixed treatment effect $\\beta_0$ of each generative model for different combinations of sample size $N$ and number of timepoints $T$ over 10,000 simulation replications"
#| warning: false
#| fig-height: 13
#| fig-width: 9
#| escape: false

runname <- "GM123ad-1000reps-researchreport-cleanscript_nrep10000" # set a runname

# set the number of simulations
nsim <- 10000

# simulation for Research Report
design <- expand.grid(sample_size = c(30, 100, 200), total_T = c(10, 30), dgm_type = c(1,2,3,"3a","3d"))

# make sure dgm_type is not a factor
design$dgm_type <- as.character(design$dgm_type)

# create dataframe to store results
design$mlm_beta_0_bias <- rep(list(numeric(1000)), nrow(design))

for (idesign in 1:nrow(design)) {
  
    results_list <- readRDS(here(paste0("simulation_results/", runname, "/", idesign, ".RDS")))
    
    dgm_type <- design$dgm_type[idesign]
    
    ### Restate the true values of the parameters ###
    
    beta_0_true <- 1 # was originallly 0.5 in the code but is 1 in the paper
    
    result_lmm <- results_list[grep("solution_lmm", names(results_list))]
    
    ### Extract the Results ###
    
    mlm_beta_0 <- sapply(result_lmm, function(l) l$coef["A", "Estimate"])
    design$mlm_beta_0_bias[[idesign]] <- mlm_beta_0 - beta_0_true
    
}

# remove dgm_type = 2
design <- design[design$dgm_type != 2, ]

# rename dgm_type = 3 to "G", dgm_type = 1 to "A", dgm_type = 3a to "B" and dgm_type = 3d to "C"
design$dgm_type <- ifelse(design$dgm_type == 3, "G", 
                          ifelse(design$dgm_type == 1, "A", 
                                 ifelse(design$dgm_type == "3a", "B", 
                                        ifelse(design$dgm_type == "3d", "C", NA))))

# Combine datasets into one
all_data <- design %>%
  mutate(total_T = as.factor(total_T), 
         sample_size = as.factor(sample_size)) %>%
  unnest(mlm_beta_0_bias) %>%
  mutate(dgm_type = factor(dgm_type, levels = c("G", "A", "B", "C")))

# Turn N and T into strings with an underscore
all_data <- all_data %>%
  mutate(total_T2 = as.character(total_T),
         sample_size2 = as.character(sample_size)) %>%
  mutate(total_T2 = str_replace_all(total_T2, "10", "T = 10"),
         total_T2 = str_replace_all(total_T2, "30", "T = 30"),
         sample_size2 = str_replace_all(sample_size2, "30", "N = 30"),
         sample_size2 = str_replace_all(sample_size2, "100", "N = 100"),
         sample_size2 = str_replace_all(sample_size2, "200", "N = 200")) %>%
  # Set factor levels to ensure correct order in the plot
  mutate(
    total_T2 = factor(total_T2, levels = c("T = 10", "T = 30")),
    sample_size2 = factor(sample_size2, levels = c("N = 30", "N = 100", "N = 200"))
  )

# Create the matrix-style plot
ggplot(all_data, aes(x = dgm_type, y = mlm_beta_0_bias, col = dgm_type)) +
  geom_boxplot() +
  geom_hline(yintercept = 0, linetype = "dashed") +  # Dashed horizontal line at 0
  ylim(-1.0, 0.7) +  # Set y-axis limits
  labs(x = "Generative Model", y = "Bias") +
  facet_grid(sample_size2 ~ total_T2) +  # Show T and N values in labels
  theme(
      axis.text.x = element_text(size = 14, vjust = -0.8),  # Increase font size of x-axis category labels
      axis.text.y = element_text(size = 14),
      axis.title.x = element_text(size = 16, vjust = -1.4),  # Increase font size of x-axis label
      axis.title.y = element_text(size = 16),  # Increase font size of y-axis label
      legend.position = "none",   # Remove legend
      panel.border = element_rect(colour = "black", fill = NA, size = 1),  # Add black border
      plot.margin = margin(5, 5, 12, 5),  # Add space around the plot
      panel.background = element_rect(fill = "white"),  # White background instead of light grey
      panel.grid.major = element_blank(),  # Remove major gridlines
      panel.grid.minor = element_blank(),  # Remove minor gridlines
      axis.line = element_line(colour = "black"),  # Black axis lines
      axis.ticks = element_line(size = 1, color = "black"), # Black ticks
      strip.text.x = element_text(face = "bold", color = "white", size = 15),
      strip.text.y = element_text(face = "bold", color = "white", size = 15),
      strip.background.x = element_rect(fill = "dodgerblue3", linetype = "solid",
                                          color = "black", linewidth = 1),
      strip.background.y = element_rect(fill = "firebrick2", linetype = "solid",
                                          color = "gray30", linewidth = 1)
  )
```

In this reproduction of @qian2020, the overall pattern was consistent with the original study: we observed bias for GM-G ($0.023-0.056$ or $2.3\%-5.6\%$), but not for GM-A ($\leq 0.002$ or $0.2\%$). These results align with expectations based on the conditional independence assumption, but contradict the backdoor criterion, which predicted the absence of bias for all the GMs. As shown in @fig-simulation-results, the bias in GM-G decreased as $T$ increased. In contrast, the bias remained stable across different levels of $N$, implying that the estimates are inconsistent (i.e. they do not converge to the true value as $N$ increases).

Similar to GM-A, we observed no discernible bias for GM-B ($\leq 0.005$ or $0.5\%$) and GM-C ($\leq 0.003$ or $0.3\%$). These findings align with the backdoor criterion’s prediction of no bias but contradict the expectations based on the conditional independence assumption, which suggested the presence of bias. Additionally, GM-B showed much smaller variability across simulation replications compared to the other GMs (see @fig-simulation-results).

In summary, these findings suggest that if the underlying GM did not include the direct dependency of the random intercept on the covariate (GM-A), the random slope $b_{i2}$ (GM-B), or the interaction term $\beta_1$ (GM-C), as in GM-G, the bias disappears. However, neither the backdoor criterion nor the conditional independence assumption provided consistent predictions of treatment effect bias across all models.

# Discussion

In this research report, we evaluated several GMs to investigate when endogenous time-varying covariates bias treatment effect estimates in MLMs under randomized treatment. We also assessed the ability of the conditional independence assumption and the backdoor criterion to predict when this bias will occur. We observed biased and inconsistent estimates in the most general model (GM-G), but not in the three special cases, that either did not contain a direct effect of the random intercept on the covariate (GM-A), a random slope for treatment (GM-B), or an interaction term between treatment and covariate (GM-C). The conditional independence assumption correctly predicted bias in GM-G and GM-A, but not in GM-B and GM-C. The backdoor criterion predicted no bias in any of the GMs, which was inconsistent with the bias found in GM-G. These findings suggest that, at least in this instance, the dependency between the covariate and treatment, the random slope for treatment, and the interaction effect are all essential for bias to occur.

This naturally raises important questions: why was no discernible bias observed in GM-B and GM-C, as predicted by the conditional independence assumption, and can this pattern generalize beyond the current GMs? One possibility is that heterogeneity in the treatment effect—both explained by the covariate and influenced by the random slope—is necessary for bias to occur. Both factors appear to be essential in driving the bias, rather than either factor alone. This aligns with @qian2020’s assertion that “applying linear mixed models is problematic because potential moderators of the treatment effect are frequently endogenous” (p. 375). If treatment effect moderation by a covariate is indeed a prerequisite for bias, it could explain the absence of bias in GM-C, where no interaction term was included. However this would not explain why the bias disappears in the model without the random slope (GM-B). More generally, note of caution is due here since the current study only considered a specific set of GMs. Further research is needed to determine whether these findings generalize or are specific to the evaluated GMs, thereby informing practical recommendations for using MLMs with endogenous time-varying covariates.

Regarding the backdoor criterion and DAGs [@pearl1988; @pearl2009], our results suggest that the classical non-parametric DAG may be insufficient to identify bias in GM-G. While DAG arrows can represent interaction effects[^7] and effect heterogeneity, these are not explicitly defined as such, precluding them from evaluating the conditional independence assumption. Similar concerns about using DAGs with the backdoor criterion in situations involving interaction effects have been raised [@weinberg2007; @attia2022]. Future research could explore how extensions of the DAG, which incorporate interaction effects, may allow the backdoor criterion to identify bias in treatment effect estimates.

[^7]: Note that the term "effect modification", while often used interchangeably with "interaction", has a distinct definition in the counterfactual framework [@vanderweele2009].

Another avenue for future investigation is the role of centering approaches (see Hamaker and Muthén [-@hamaker2020] for an overview). According to @antonakis2021, the assumption of uncorrelated random effects and level 1 covariates can be relaxed using Mundlak's contextual model[^8] [@mundlak1978], which adds cluster means of each covariate as predictors of the random intercept. This approach, which explicitly models the source of endogeneity as suggested by Bell and Jones [-@bell2015], could provide new insights into the treatment effect bias in GM-G and its absence in GM-A, GM-B and GM-C.

[^8]: This is referred to as the Correlated Random Effects (CRE) approach by @wooldridge2002.

Finally, @qian2020 only considered the independence between the random intercept and random slope, and perfect correlation between the random intercept of the outcome and the covariate. These assumptions may be violated in practice. Exploring cases where the random intercept and random slope are correlated or where the random intercepts differ across covariate and outcome could provide further insight.

# Conclusion

Dating all the way back to the work of Pepe and Anderson [-@pepe1994] it has been known that the endogeneity of time-varying covariates can result in biased parameter estimates. This research report is a first step towards understanding the work of @qian2020, who illustrated that this issue can even affect MLMs with randomized treatment. To recognize and understand completely when and why endogenous covariates may trouble an empirical investigation, further research is needed.

\newpage

# References

::: {#refs}
:::

<!-- --- -->

<!-- nocite: | -->

<!--   @* -->

<!-- --- -->
