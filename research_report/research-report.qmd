---
title: "Estimation of Effects of Endogenous Time-Varying Covariates: A Comparison Of Multilevel Linear Modeling and Generalized Estimating Equations"
subtitle: "Research Report" 
author: 
  - name: "Ward B. Eiling (9294163)"
    orcid: "0009-0007-8114-9497"
    affiliation: 
      - name: "Utrecht University"
      - department: "Methodology and Statistics"
date: "Dec 22, 2024"
date-format: long
format: # docx
  pdf:
    papersize: a4
    fig-pos: 'H'
    tbl-pos: 'H'
    keep-tex: true
    # toc: true
    number-sections: true
    colorlinks: true
    indent: true
    header-includes:
      - \usepackage{fancyhdr}
      - \usepackage{amsmath}
      - \usepackage{multirow}
      - \usepackage{booktabs}
      # - \usepackage{float}
      # - \usepackage{tikz}
      # - \usepackage{subcaption}
    geometry:
      - top=2.5cm
      - bottom=2.5cm
      - left=2.5cm
      - right=2.5cm
    mainfont: Latin Modern Roman
    sansfont: Latin Modern Roman
    linestretch: 2
    fontsize: 12pt
    template-partials:
      - "before-body.tex"
bibliography: references.bib
link-citations: true
csl: apa.csl
execute:
  message: false
  warning: false
fig-cap-location: top
---

```{r}
#| label: Packages
#| echo: false

# for knitting the document
library(papaja)
library(rlang)
library(rmarkdown)
library(knitr)
library(kableExtra)
library(here)
# for general data manipulation and plotting
library(tidyverse)
# for estimation
library(lme4)
library(geepack)
# for diagrams
library(dagitty)
library(ggdag)
library(DiagrammeR)
library(magick)
library(pdftools)
```

\newpage

# Introduction

<!-- -   Start with a paragraph describing a problem in the real-life world (so that a BoS member not familiar with statistics understands why you are pursuing research in this direction); -->

Across a wide range of disciplines, researchers analyze clustered longitudinal, observational data to investigate prospective causal relationships between variables. When analyzing such data, the psychological sciences most commonly resort to the multilevel linear model [MLM, @mcneish2017], which---in the context of longitudinal data analysis---separates observed variance into stable between-person differences and within-person fluctuations [@hamaker2020]. Conversely, other fields, such as biostatistics and econometrics often favour generalized estimating equations (GEE) for the analysis of longitudinal data [@mcneish2017]. Despite some cross-disciplinary efforts to compare these methods [@muth2016; @mcneish2017; @yan2013], their scarcity may leave researchers with limited guidance in choosing the most suitable approach for their application.

<!-- -   Then, add a paragraph describing what is known in the literature; -->

<!-- -   Also, describe what is not known and which gap you will address in your thesis; End with a clear research question and, if applicable, your hypothesis (for confirmatory research questions – it should be a testable hypothesis) or expectations (for exploratory research questions – can be much vaguer because of the explorative nature of the question). -->

A recent study by @qian2020 highlighted an issue present in both methods---except for GEE with working independence---where controlling for *time-varying endogenous covariates* may lead to biased causal estimates. A time-varying covariate is *endogenous* if it is directly or indirectly influenced by prior treatment or outcome, meaning its value may be determined by earlier stages of the process [@qian2020]. @fig-endogenous-example showcases a time-varying endogenous covariate $X_{it}$ in the context of a simple multilevel linear model with a random intercept $b_{0i}$. As a result of including these covariates in these models, ordinary interpretations of the coefficients are no longer valid [@qian2020, p. 3]. According to @diggle2002, this issue not only pertains GEE and MLM, but *all* longitudinal data analysis methods.

::: {#fig-endogenous-example}

```{r}
#| label: endogenous-example
#| engine: 'tikz'
#| echo: false
#| cache: true
#| eval: true

\begin{tikzpicture}
    % Nodes
    \node[draw] (X1) at (1.5, 1.5) {$X_{1i}$};
    \node[draw] (X2) at (6.0, 1.5) {$X_{2i}$};
    \node[draw] (Y2) at (6.0, 3.5) {$Y_{2i}$};
    \node[draw] (Y3) at (10.0, 3.5) {$Y_{3i}$};
    \node[draw, circle, fill=gray!20] (b0) at (8.0, 5.5) {$b_{0i}$};
    \node (eY2) at (7, 3.5) {$\sigma_{\epsilon}^2$};
    \node (eY3) at (11, 3.5) {$\sigma_{\epsilon}^2$};
    \node (eX1) at (1.5, 0.5) {$\sigma_{X_1}^2$};

    % Edges with labels
    \draw[->] (X1) -- (Y2);
    \draw[->] (X2) -- (Y3);
    \draw[->] (Y2) -- (X2);
    \draw[->] (b0) -- (Y2);
    
    \draw[->] (X1) -- (Y2);
    \draw[->] (X2) -- (Y3);

    %\draw[->, red] (Y2) -- (X2);
    %\draw[->, red] (b0) -- (Y2);

    \draw[->] (b0) -- (Y3);
    \draw[->] (eY2) -- (Y2);
    \draw[->] (eY3) -- (Y3);
    \draw[->] (eX1) -- (X1);

    \draw[<->, out=120, in=60, looseness=1.5] (b0) to node[above] {$\sigma_{b_0}^2$} (b0);
\end{tikzpicture}
```

*Note*. Adapted from Section 2.2 of @qian2020.

Multilevel Linear Model with Time-Varying Endogenous Covariate $X_{it}$.

:::

<!-- The issues described in @qian2020 can be categorized into two main problems: (1) model interpretation issues -->

However, due to a divide between the disciplines that employ these methods, such critiques of the MLM appear to have largely failed to reach the applied researcher in psychology. One specific reason might be that the technical jargon in other disciplines makes it difficult for researchers to recognize when and how these issues emerge[^1]. Therefore, this report aims to understand and explain the issue of including endogenous covariates in analyses involving GEE and MLM in a psychological context. To achieve this aim, the current investigation employs (a) graphical tools such as the directed acyclic graph (DAG) and path diagram to assess potentially relevant assumptions, as well as (b) data simulations with additional scenarios to pinpoint the issue. Accordingly, the following research questions will be addressed:

<!-- -   Also, describe what is not known and which gap you will address in your thesis; End with a clear research question and, if applicable, your hypothesis (for confirmatory research questions – it should be a testable hypothesis) or expectations (for exploratory research questions – can be much vaguer because of the explorative nature of the question). -->

<!-- Through a cross-fertilization of these literatures,  -->

<!-- sub-questions being specified to isolate the issue described in the @qian2020: -->

(1) When does the inclusion of endogenous variables in multilevel linear models result in biased estimates of the treatment effect?

    <!-- (a) How is the bias in the treatment effect affected by the removal of the interaction $\beta_1$ from generative model 3? -->

    <!-- (b) How is the bias in the treatment effect affected by the removal of the random slope $b_{i2}$ from generative model 3? -->

(2) When does the inclusion of of endogenous covariates in multilevel linear models result in a discrepancy between conditional and marginal interpretations of the treatment effect?

    <!-- (a) When removing the interaction $\beta_1$ from generative model 3, is there a difference between the marginal and conditional estimates of the treatment effect? -->

    <!-- (b) When removing the random slope $b_{i2}$ from generative model 3, is there a difference between the marginal and conditional estimates of the treatment effect? -->

<!-- Research questions 1 and 2 will be investigated by employing MLM and GEE estimation respectively. To further isolate the issue of the biased generative model proposed by @qian2020, additional generative models will be considered. -->

<!-- It would be interesting to compare the RI-CLPM to the CLPM, as the former is said to be superior in addressing certain confounders, but could it also be more susceptible to time-varying moderators? -->


<!-- [^1]: For instance, the term 'endogeneity' in econometrics, while related, has a distinct meaning from that of an endogenous variable, which can lead to confusion. -->

# Methods

To obtain a better understanding of the issue exposed by @qian2020, two methods were employed. First, graphical methods were used provide insight into the presence and extent of bias with potential violation of assumptions: (a) path diagrams were used to evaluate the conditional independence assumption and (b) directed acyclic graphs (DAGs) were used to evaluate the backdoor criterion (Pearl, 1988, 2009). Second, a simulation study was performed to reproduce the results for the generative models (GMs) from @qian2020 and to further isolate the issue using additional GMs. In this simulation, bias in the treatment effect (RQ 1) was assessed with analytical multilevel models. The discrepancy between conditional and marginal interpretations of the treatment effect (RQ 2) was assessed with GEE with working independence.

## Data Generation

In the simulation @qian2020 considered three generative models (GMs), all of which have an endogenous time-varying covariate. In GM1 and GM2, the endogenous covariate $X_{it}$ equals the previous outcome $Y_{it}$ plus some random noise, so the *conditional independence* assumption is valid. In GM3, the endogenous covariate depends directly on $b_{i0}$, violating the assumption. To isolate the issue in GM3, we consider two variations on this model: GM3A, where the random slope $b_{i2}$ for the treatment $A_{it}$ is removed; GM3B, where the interaction term $\beta_1 A_{it} X_{it}$ is removed. Note that the conditional independence assumption is violated in either of these variations. The details of the generative models are described below. We follow the notation of @qian2020 to allow for direct comparison, but rewrite the equations into within- and between-person models [see @raudenbush2002]. We accompany the equations of the GMs with graphical representations, where random effects are represented by grey circles, observed variables by squares and relationships across variables by arrows. The path diagrams of the three data generating models shows the discrepancies between the different generative models---especially concerning the interaction effects---more clearly than DAGs.

<!-- and GM3C, where the fixed effect $\alpha_1$ is removed -->

### Generative Model 1

In GM1, we considered a simple case with only a random intercept and a random slope for $X_{it}$. The outcome is generated according to the following repeated-observations or within-person model (level 1):

$$
Y_{it+1} = \pi_{0i} + \pi_{1i} X_{it} + \pi_{2i} A_{it} + \pi_{3i} A_{it} X_{it} + \epsilon_{it+1}
$$

with the person-level or between-person model (level 2):

$$
\pi_{0i} = \alpha_0 + b_{i0}, \quad b_{i0} \sim \mathcal{N}(0, \sigma_{b0}^2),
$$

$$
\pi_{1i} = \alpha_1,
$$

$$
\pi_{2i} = \beta_0 + b_{i2}, \quad b_{i2} \sim \mathcal{N}(0, \sigma_{b2}^2),
$$

$$
\pi_{3i} = \beta_1.
$$

By substitution, we get the single equation model:

$$
\begin{aligned}
Y_{it+1} &= \pi_{0i} + \pi_{1i} X_{it} + \pi_{2i} A_{it} + \pi_{3i} A_{it} X_{it} + \epsilon_{it+1} \\
&= (\alpha_0 + b_{i0}) + \alpha_1 X_{it} + (\beta_0 + b_{i2}) A_{it} + \beta_1 A_{it} X_{it} + \epsilon_{it+1} \\
&= \alpha_0 + \alpha_1 X_{it} + b_{i0} + A_{it} (\beta_0 + \beta_1 X_{it} + b_{i2}) + \epsilon_{it+1}.
\end{aligned}
$$

The random effects $b_{i0} \sim \mathcal{N}(0, \sigma_{b0}^2)$ and $b_{i2} \sim \mathcal{N}(0, \sigma_{b2}^2)$ are independent of each other. The covariate is generated as $X_{i1} \sim \mathcal{N}(0, 1)$, and for $t \geq 2$,

$$
X_{it} = Y_{it} + \mathcal{N}(0, 1).
$$

The randomization probability $p_t = P(A_{it} = 1 \mid H_{it})$ is constant at $1/2$. Thus, $A_{it} \sim \text{Bernoulli}(0.5)$ for $i = 1, \ldots, N$ and $t = 1, \ldots, T$. The exogenous noise is $\epsilon_{it+1} \sim \mathcal{N}(0, \sigma_\epsilon^2)$.

@fig-GM1_path shows the path diagram for GM1.

```{r}
#| label: fig-GM1_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-cap: "Path diagram for Generative Model 1 ($t = 1, 2, 3$)"
#| eval: true

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, 2) {$X_{1i}$};
  \node (X1var) at (0, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, -2) {$A_{1i}$};
  \node[draw, rectangle] (A1X1) at (0.3, 0) {$A_{1i}\cdot X_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, 2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, -2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, 2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2X2) at (5.3, 0) {$A_{2i}\cdot X_{2i}$}; % Increased x-coordinate

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate
  \node[draw, circle, fill=gray!20] (b2) at (3, -4) {$b_{i2}$}; % Adjusted to align with middle row

  % Arrows with path coefficients
  \draw [->] (b0) -- node[right] {$+$} (Y2);
  \draw [->] (b0) -- node[right] {$+$} (Y3);
  \draw [->] (A2) edge node[above left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[above] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[above left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[above] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (A1X1) edge node[above] {$\beta_1$} (Y2);
  \draw [->] (A2X2) edge node[above left] {$\beta_1$} (Y3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);
  \draw [<->, out=315, in=225, looseness=2] (b2) to node[below] {$\sigma^2_{b2}$} (b2);

  % Additional paths for random effects
  \draw[->, dashed] (b2) -- (1.5,-1); 
  \draw[->, dashed] (b2) -- (5,-1.2); 

\end{tikzpicture}
```

<!-- ### Generative Model 1A -->

<!-- GM1A is the same as GM1, except that the random slope $b_{i2}$ for the treatment $A_{it}$ is removed. The single equation model thus becomes: -->

<!-- $$ -->

<!-- Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + b_{i0} + A_{it} (\beta_0 + \beta_1 X_{it}) + \epsilon_{it+1}. -->

<!-- $$ -->

<!-- ### Generative Model 1B -->

<!-- GM1B is the same as GM1A, except that the interaction term $\beta_1 A_{it} X_{it}$ is removed. The single equation model thus becomes: -->

<!-- $$ -->

<!-- Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + b_{i0} + \beta_0 A_{it}  + \epsilon_{it+1}. -->

<!-- $$ -->

### Generative Model 2

In GM2, we considered the case with a random intercept and random slopes for (1) covariate $X_{it}$, (2) treatment $A_{it}$, and (3) the interaction between $A_{it}$ and $X_{it}$; and with a time-varying randomization probability for treatment. The outcome is generated according to the same repeated-observations model presented in GM1. However, the person-level model is different:

$$
\pi_{0i} = \alpha_0 + b_{i0}, \quad b_{i0} \sim \mathcal{N}(0, \sigma_{b0}^2),
$$

$$
\pi_{1i} = \alpha_1 + b_{i1}, \quad b_{i1} \sim \mathcal{N}(0, \sigma_{b1}^2),
$$

$$
\pi_{2i} = \beta_0 + b_{i2}, \quad b_{i2} \sim \mathcal{N}(0, \sigma_{b2}^2),
$$

$$
\pi_{3i} = \beta_1 + b_{i3}, \quad b_{i3} \sim \mathcal{N}(0, \sigma_{b3}^2).
$$

By substitution, we get the single equation model:

$$
\begin{aligned}
Y_{it+1} &= \pi_{0i} + \pi_{1i} X_{it} + \pi_{2i} A_{it} + \pi_{3i} A_{it} X_{it} + \epsilon_{it+1} \\ 
&= (\alpha_0 + b_{i0}) + (\alpha_1 + b_{i1}) X_{it} + (\beta_0 + b_{i2}) A_{it} + (\beta_1 + b_{i3}) A_{it} X_{it} + \epsilon_{it+1} \\ 
&= \alpha_0 + \alpha_1 X_{it} + b_{i0} + b_{i1} X_{it} + A_{it} \left( \beta_0 + \beta_1 X_{it} + b_{i2} + b_{i3} X_{it} \right) + \epsilon_{it+1}.
\end{aligned}
$$

The random effects $b_{ij} \sim \mathcal{N}(0, \sigma_{bj}^2)$, for $j = 0, 1, 2, 3$, are independent of each other. The covariate is generated as $X_{i1} \sim \mathcal{N}(0, 1)$, and for $t \geq 2$,

$$
X_{it} = Y_{it} + \mathcal{N}(0, 1).
$$

The randomization probability depends on $X_{it}$:

$$
p_t = P(A_{it} = 1 \mid H_{it}) = 
\begin{cases} 
0.7 & \text{if } X_{it} > -1.27, \\
0.3 & \text{if } X_{it} \leq -1.27,
\end{cases}
$$

where the cutoff $-1.27$ was chosen so that $p_t$ equals 0.7 or 0.3 for about half of the time. In other words, if the value of the covariate for any given person and time point is above the cutoff, the probability of receiving the treatment $p_t$ is 0.7; otherwise, it is 0.3. Accordingly, $A_{it} \sim \text{Bernoulli}(p_t)$ for $i = 1, \ldots, N$ and $t = 1, \ldots, T$. The exogenous noise is $\epsilon_{it+1} \sim \mathcal{N}(0, \sigma_\epsilon^2)$.

@fig-GM2_path shows the path diagram for GM2.

```{r}
#| label: fig-GM2_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-cap: "Path diagram for Generative Model 2 ($t = 1, 2, 3$)"
#| eval: true

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, 2) {$X_{1i}$};
  \node (X1var) at (0, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, -2) {$A_{1i}$};
  \node[draw, rectangle] (A1X1) at (0.3, 0) {$A_{1i}\cdot X_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, 2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, -2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, 2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2X2) at (5.3, 0) {$A_{2i}\cdot X_{2i}$}; % Increased x-coordinate

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; 
  \node[draw, circle, fill=gray!20] (b1) at (3, 4) {$b_{i1}$}; 
  \node[draw, circle, fill=gray!20] (b2) at (3, -4) {$b_{i2}$}; 
  \node[draw, circle, fill=gray!20] (b3) at (6, -4) {$b_{i3}$}; 

  % Arrows with path coefficients
  \draw [->] (b0) -- node[right] {$+$} (Y2);
  \draw [->] (b0) -- node[right] {$+$} (Y3);
  \draw [->] (A2) edge node[above left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[above] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[above left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[above] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (A1X1) edge node[above] {$\beta_1$} (Y2);
  \draw [->] (A2X2) edge node[above left] {$\beta_1$} (Y3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);

  \draw [->, bend right=40] (X1) to (A1); 
  \draw [->, bend right=40] (X2) to (A2);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);
  \draw [<->, out=45, in=135, looseness=2] (b1) to node[above] {$\sigma^2_{b1}$} (b1);
  \draw [<->, out=315, in=225, looseness=2] (b2) to node[below] {$\sigma^2_{b2}$} (b2);
  \draw [<->, out=315, in=225, looseness=2] (b3) to node[below] {$\sigma^2_{b3}$} (b3);

  % Additional paths for random effects
  \draw[->, dashed] (b2) -- (1.5,-1); 
  \draw[->, dashed] (b2) -- (5,-1.2); 

  \draw[->, dashed] (b1) -- (1.5,1); 
  \draw[->, dashed] (b1) -- (5,1.2); 

  \draw[->, dashed] (b3) -- (1.75, 0); 
  \draw[->, dashed] (b3) -- (6.75, 0); 

\end{tikzpicture}
```

<!-- ### Generative Model 2A -->

<!-- GM2A is the same as GM2, except that the random slopes $b_{i1}$, $b_{i2}$ and $b_{i3}$ are removed. The single equation model then becomes the same as GM1A, but with the time-varying randomization probabilities of GM2. -->

<!-- ### Generative Model 2B -->

<!-- GM2B is the same as GM2A, except that the interaction term $\beta_1 A_{it} X_{it}$ is removed. The single equation model then becomes the same as GM1B, but with the time-varying randomization probabilities of GM2. -->

### Generative Model 3

GM3 is the same as GM1, except that the covariate $X_{it}$ depends directly on $b_{i0}$:

$$
X_{i1} \sim \mathcal{N}(b_{i0}, 1), \quad X_{it} = Y_{it} + \mathcal{N}(b_{i0}, 1) \text{ for } t \geq 2.
$$

@fig-GM3_path shows the path diagram for GM3.

```{r}
#| label: fig-GM3_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-cap: "Path diagram for Generative Model 3 ($t = 1, 2, 3$)"
#| eval: true

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, 2) {$X_{1i}$};
  \node (X1var) at (0, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, -2) {$A_{1i}$};
  \node[draw, rectangle] (A1X1) at (0.3, 0) {$A_{1i}\cdot X_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, 2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, -2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, 2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2X2) at (5.3, 0) {$A_{2i}\cdot X_{2i}$}; % Increased x-coordinate

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate
  \node[draw, circle, fill=gray!20] (b2) at (3, -4) {$b_{i2}$}; % Adjusted to align with middle row

  % Arrows with path coefficients
  \draw [->] (b0) -- node[right] {$+$} (Y2);
  \draw [->] (b0) -- node[right] {$+$} (Y3);
  \draw [->] (A2) edge node[above left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[above] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[above left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[above] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (A1X1) edge node[above] {$\beta_1$} (Y2);
  \draw [->] (A2X2) edge node[above left] {$\beta_1$} (Y3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);
  \draw [->] (b0) edge (X1);
  \draw [->] (b0) edge (X2);
  \draw [->] (b0) edge (X3);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);
  \draw [<->, out=315, in=225, looseness=2] (b2) to node[below] {$\sigma^2_{b2}$} (b2);

  % Additional paths for random effects
  \draw[->, dashed] (b2) -- (1.5,-1); 
  \draw[->, dashed] (b2) -- (5,-1.2); 

\end{tikzpicture}
```

### Generative Model 3A

GM3A is the same as GM3, except that the random slope $b_{i2}$ for the treatment $A_{it}$ is removed. The single equation model then becomes:

$$
Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + b_{i0} + A_{it} (\beta_0 + \beta_1 X_{it}) + \epsilon_{it+1}.
$$

```{r}
#| label: fig-GM3A_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-cap: "Path diagram for Generative Model 3A ($t = 1, 2, 3$)"
#| eval: true

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, 2) {$X_{1i}$};
  \node (X1var) at (0, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, -2) {$A_{1i}$};
  \node[draw, rectangle] (A1X1) at (0.3, 0) {$A_{1i}\cdot X_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, 2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, -2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, 2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2X2) at (5.3, 0) {$A_{2i}\cdot X_{2i}$}; % Increased x-coordinate

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate

  % Arrows with path coefficients
  \draw [->] (b0) -- node[right] {$+$} (Y2);
  \draw [->] (b0) -- node[right] {$+$} (Y3);
  \draw [->] (A2) edge node[above left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[above] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[above left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[above] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (A1X1) edge node[above] {$\beta_1$} (Y2);
  \draw [->] (A2X2) edge node[above left] {$\beta_1$} (Y3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);
  \draw [->] (b0) edge (X1);
  \draw [->] (b0) edge (X2);
  \draw [->] (b0) edge (X3);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);

\end{tikzpicture}
```

### Generative Model 3B

GM3B is the same as GM3, except that the interaction term $\beta_1 A_{it} X_{it}$ is removed. The single equation model then becomes:

$$
Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + b_{i0} + A_{it} (\beta_0 + b_{i2}) + \epsilon_{it+1}.
$$

```{r}
#| label: fig-GM3B_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-cap: "Path diagram for Generative Model 3B ($t = 1, 2, 3$)"
#| eval: true

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, 2) {$X_{1i}$};
  \node (X1var) at (0, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, -2) {$A_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, 2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, -2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, 2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, 3) {$\sigma^2_{X}$};

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate
  \node[draw, circle, fill=gray!20] (b2) at (3, -4) {$b_{i2}$}; % Adjusted to align with middle row

  % Arrows with path coefficients
  \draw [->] (b0) -- node[right] {$+$} (Y2);
  \draw [->] (b0) -- node[right] {$+$} (Y3);
  \draw [->] (A2) edge node[above left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[above] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[above left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[above] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);
  \draw [->] (b0) edge (X1);
  \draw [->] (b0) edge (X2);
  \draw [->] (b0) edge (X3);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);
  \draw [<->, out=315, in=225, looseness=2] (b2) to node[below] {$\sigma^2_{b2}$} (b2);

  % Additional paths for random effects
  \draw[->, dashed] (b2) -- (1.5,-1); 
  \draw[->, dashed] (b2) -- (5,-1.2); 

\end{tikzpicture}
```

<!-- ### Generative Model 3C -->

<!-- GM3C is the same as GM3, except that the fixed slope $\alpha_1$ for the covariate $X_{it}$ is removed. The single equation model then becomes: -->

<!-- $$ -->

<!-- Y_{it+1} = \alpha_0 + b_{i0} + A_{it} (\beta_0 + \beta_1 X_{it} + b_{i2}) + \epsilon_{it+1}. -->

<!-- $$ -->

### Parameter Values

The following parameter values were adapted from @qian2020:

$$
\alpha_0 = -2, \quad \alpha_1 = -0.3, \quad \beta_0 = 1, \quad \beta_1 = 0.3,
$$

$$
\sigma_{b0}^2 = 4, \quad \sigma_{b1}^2 = \frac{1}{4}, \quad \sigma_{b2}^2 = 1, \quad \sigma_{b3}^2 = \frac{1}{4}, \quad \sigma_\epsilon^2 = 1.
$$

## Path Diagrams and Conditional Independence

@qian2020 proposes the use of the conditional independence assumption to identify whether bias may occur, which is given by:

$$ X_{it} \perp (b_{i0}, b_{i1}) \mid H_{it-1}, A_{it-1}, Y_{it}. $$

where $H_{it-1}$ refers to the history of the set of covariates, which in this case are all observations of covariate $X_{it}$ prior to the current timepoint $t$. This allows $X_{it}$ to be endogenous, but the endogenous covariate $X_{it}$ can only depend on the random effects through variables observed prior to $X_{it}$. If the only endogenous covariates are functions of prior treatments and prior outcomes, then the assumption automatically holds.

When inspecting @fig-GM1_path and @fig-GM2_path, we may notice that $X_{it}$ becomes independent of the random effects after conditioning on $Y_{it}$. On the other hand, we can see that this assumption is violated in GM3/3A/3B, as $X_{it}$ depends directly on $b_{i0}$ and can thus not be made independent of the random effects by conditioning on prior variables such as $Y_{it}$ (see @fig-GM3_path, @fig-GM3A_path and @fig-GM3B_path). Thus, we would expect biased estimates of the treatment effect for GM3/3A/3B.

## Backdoor Criterion and DAGs

DAGs are a useful tool for representing causal relationships between variables and to evaluate the assumptions needed for causal identification. According to the backdoor criterion [@pearl1988; @pearl2009], a requirement for causal identification, causal effects can be identified by blocking non-causal paths through conditioning on intermediate variables (e.g., controlling or matching). If any non-causal paths cannot be blocked due to omitted variables or measurement error, treatment and outcome remain linked via backdoor paths, leading to biased estimates of the treatment effect [@Kim2021a].

We formulated the DAGs in `dagitty`, where the random disturbance $b_{0i}$ was represented by the node U [e.g., @Kim2021a]. The DAGs for the first three observations of the three data generating models are presented in @fig-DAGs.

::: {#fig-DAGs layout-ncol="2"}
```{r}
#| label: fig-GM1_DAG
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM 1"
#| eval: true

\begin{tikzpicture}[node distance=1cm]
  % Nodes for observed variables
  \node (X1) at (0, 2) {$X_{1}$};
  \node (A1) at (0, -2) {$A_{1}$};
  \node (Y2) at (3, 0) {$Y_{2}$};
  \node (X2) at (3, 2) {$X_{2}$};
  \node (A2) at (3, -2) {$A_{2}$};
  \node (Y3) at (6, 0) {$Y_{3}$};
  \node (X3) at (6, 2) {$X_{3}$};
  \node (U) at (4.5, 3) {$U$};

  % Directed edges
  \draw [->] (U) -- (Y2);
  \draw [->] (U) -- (Y3);
  \draw [->] (A2) -- (Y3);
  \draw [->] (X2) -- (Y3);
  \draw [->] (Y2) -- (X2);
  \draw [->] (A1) -- (Y2);
  \draw [->] (X1) -- (Y2);
  \draw [->] (Y3) -- (X3);
\end{tikzpicture}
```

```{r}
#| label: fig-GM2_DAG
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM 2"
#| eval: true

\begin{tikzpicture}[node distance=1cm]
  % Nodes for observed variables
  \node (X1) at (0, 2) {$X_{1}$};
  \node (A1) at (0, -2) {$A_{1}$};
  \node (Y2) at (3, 0) {$Y_{2}$};
  \node (X2) at (3, 2) {$X_{2}$};
  \node (A2) at (3, -2) {$A_{2}$};
  \node (Y3) at (6, 0) {$Y_{3}$};
  \node (X3) at (6, 2) {$X_{3}$};
  \node (U) at (4.5, 3) {$U$};

  % Directed edges
  \draw [->] (U) -- (Y2);
  \draw [->] (U) -- (Y3);
  \draw [->, red] (A2) -- (Y3);
  \draw [->] (X2) -- (Y3);
  \draw [->] (Y2) -- (X2);
  \draw [->, red] (A1) -- (Y2);
  \draw [->] (X1) -- (Y2);
  \draw [->] (Y3) -- (X3);

  % Curved edges for X1 -> A1 and X2 -> A2
  \draw [->, bend left=40, red] (X1) to (A1);
  \draw [->, bend left=40, red] (X2) to (A2);
\end{tikzpicture}
```

```{r}
#| label: fig-GM3_DAG
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM 3, 3A, 3B"
#| eval: true

\begin{tikzpicture}[node distance=1cm]
  % Nodes for observed variables
  \node (X1) at (0, 2) {$X_{1}$};
  \node (A1) at (0, -2) {$A_{1}$};
  \node (Y2) at (3, 0) {$Y_{2}$};
  \node (X2) at (3, 2) {$X_{2}$};
  \node (A2) at (3, -2) {$A_{2}$};
  \node (Y3) at (6, 0) {$Y_{3}$};
  \node (X3) at (6, 2) {$X_{3}$};
  \node (U) at (4.5, 3) {$U$};

  % Directed edges
  \draw [->] (U) -- (Y2);
  \draw [->] (U) -- (Y3);
  \draw [->] (U) -- (X1);
  \draw [->] (U) -- (X2);
  \draw [->] (U) -- (X3);
  \draw [->] (A1) -- (Y2);
  \draw [->] (X1) -- (Y2);
  \draw [->] (Y2) -- (X2);
  \draw [->] (A2) -- (Y3);
  \draw [->] (X2) -- (Y3);
  \draw [->] (Y3) -- (X3);
\end{tikzpicture}
```

*Note.* The red arrows show the biased backdoor path(s) in the treatment efffect (before controlling for $X_{it}$).

DAGs for Generative Models 1, 2, 3, 3A, 3B (t = 1, 2, 3)
:::

When applying Pearl's backdoor criterion to GM1/3/3A/3B, it may be observed that there exists no backdoor path in the treatment effect $A_{it} \to Y_{it+1}$, as $A_{it}$ does not have any parents. While we need not control for covariate $X_{it}$ to obtain an unbiased total effect, doing so should not introduce bias.

<!-- In GM1, upon controlling for the covariate $X_{it}$, we block the mediation pathway in $Y_{it} \rightarrow Y_{it+1}$, which would otherwise lead to a biased treatment effect, through the addition of the random disturbance U.  -->

On the other hand, in GM2, there is a backdoor path in the treatment effect: $A_{it} \leftarrow X_{it} \rightarrow Y_{it+1}$ (see @fig-GM2_DAG). More specifically, $X_{it}$ is a confounder in the relationship between $A_{it}$ and $Y_{it+1}$. However, controlling for $X_{it}$ blocks this backdoor path, making the treatment effect unbiased. In other words, the history of covariate $X_{it}$ is a sufficient adjustment set for the treatment effect.

All things considered, according to the backdoor criterion, controlling for the covariate $X_{it}$ should not result in biased estimates of the treatment effect for any of the generative models.

<!-- Visually, we may notice from @fig-GM3_DAG that when we control for $X_{it}$ in GM3, we block the backdoor path from $X_{ti}$ to $Y_{it+1}$ through $X_{it}$, which is the path that would be biased if we did not control for $X_{it}$. -->

<!-- Alternatively, we can display the data generating models as a path diagram, where latent variables are represented by circles, observed variables by squares and relationships across variables by arrows. The path diagrams of the three data generating models is presented in @fig-pathdiagrams (GM1 in @fig-GM1_pd, GM2 in @fig-GM2_pd, and GM3 in @fig-GM3_pd), which shows the discrepancies between the different generative models more clearly than the DAGs. -->

<!-- SEE TEXT -->

<!-- We can make a couple observations from this path diagram: -->

<!-- -   Contrary to the DAG, this path diagram shows the moderation effect (1) of $X_{it}$ on the relationship between $X_{ti}$ and $Y_{it+1}$ and (2) of $u_{2i}$ on the relationship between $X_{it}$ and $Y_{it+1}$. -->

<!-- -   Similar to the example without treatment in section 2.2, the covariate $X_{it}$ is determined by the previous value of the outcome $Y_{ti}$---which makes it an endogenous time-varying covariate. -->

<!-- -   The path diagram does not display the difference in the randomized treatment assignment probabilities between GM1 and GM2. -->

## Data Analysis

We evaluated the performance of the models across a total of 30 different settings, each replicated 1,000 times, by systematically varying the following factors:

-   **Generative Models (GM):** 1, 2, 3, 3A, 3B

-   **Number of timepoints (T):** 10, 30

-   **Sample size (N):** 30, 100, 200

All data generation and estimation was performed in `R`, version 4.4.2 [@rcoreteam2024]. After the generation of data generation for any given setting, several models were fit. To fit the standard MLM, the `lmer` function from the R-package `lme4` [@bates2015] was employed with restricted maximum likelihood estimation. For the MLM, the analytical models were equivalent to each of the respective data-generating models. To fit the GEE with the "exchangeable", "independent" and "AR(1)" working correlation structures, the `geeglm` function from the R-package `geepack` [@halekoh2006] was employed with the identity link function. Since the random effects are not explicitly modelled in GEE, the analytical GEE models simply contain only the fixed effects of the generative model at hand.

<!-- the specification of the analytical GEE models is different from their MLM counterparts. More specifically, the analytical models simply contain only the fixed effects of the generative model hand.  -->

<!-- Since the fixed effects modeled in GM1, GM2, GM3, GM3a are the same (the only differences pertains to the modeling of random effects), the analytical *GEE model* is identical across these conditions: -->

<!-- $$ -->

<!-- Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + \beta_0 A_{it} + \beta_1 A_{it} X_{it} + \epsilon_{it+1}. -->

<!-- $$ -->

<!-- In GM3b, the fixed interaction effect is removed, so the analytical *GEE model* is given by: -->

<!-- $$ -->

<!-- Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + \beta_0 A_{it} + \epsilon_{it+1}. -->

<!-- $$ -->

<!-- As a reminder, the analytical *multilevel model* for GM1 and GM3 is given by: -->

<!-- $$ -->

<!-- Y_{it+1} = (\alpha_0 + b_{i0}) + \alpha_1 X_{it} + (\beta_0 + b_{i2}) A_{it} + \beta_1 A_{it} X_{it} + \epsilon_{it+1}. -->

<!-- $$ -->

<!-- which is fitted as `lmer(Y ~ X * A + (1 + A | id), data = data)` in R. -->

<!-- The analytical *multilevel model* for GM2 is given by: -->

<!-- $$ -->

<!-- Y_{it+1} = (\alpha_0 + b_{i0}) + (\alpha_1 + b_{i1}) X_{it} + (\beta_0 + b_{i2}) A_{it} + (\beta_1 + b_{i3}) A_{it} X_{it} + \epsilon_{it+1}. -->

<!-- $$ -->

<!-- which is fitted as `lmer(Y ~ X * A + (X * A | id), data = data)` in R. -->

<!-- The analytical *multilevel model* for GM1A, GM2A, and GM3A is given by: -->

<!-- $$ -->

<!-- Y_{it+1} = \alpha_0 + b_{i0} + \alpha_1 X_{it} + \beta_0 A_{it} + \beta_1 A_{it} X_{it} + \epsilon_{it+1}. -->

<!-- $$ -->

<!-- which is fitted as `lmer(Y ~ X * A + (1 | id), data = data)` in R. -->

<!-- The analytical *multilevel model* for GM1B, GM2B, and GM3B is given by: -->

<!-- $$ -->

<!-- Y_{it+1} = \alpha_0 + b_{i0} + \alpha_1 X_{it} + \beta_0 A_{it} + \epsilon_{it+1}. -->

<!-- $$ -->

<!-- which is fitted as `lmer(Y ~ X + A + (1 | id), data = data)` in R. -->

<!-- The specification of the GEE models related to each of the generative models is unsurprisingly different considering that GEE does not explicitly model random effects. For each of the generative models, we will fit three GEE models: one with an exchangeable correlation structure, one with an independent correlation structure, and one with an AR(1) correlation structure. The GEE models were fitted using the `geeglm` function in R. Since the fixed effects modeled in GM1, GM1a, GM2, GM2a, GM3, GM3a are the same (the only differences pertain to the modeling of random effects), the analytical *GEE model* is identical across these three conditions. The analytical *GEE model* is given by: -->

<!-- $$ -->

<!-- Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + \beta_0 A_{it} + \beta_1 A_{it} X_{it} + \epsilon_{it+1}. -->

<!-- $$ -->

<!-- The GEE models were fitted as `geeglm(Y ~ X * A, id = id, data = data, family = gaussian, corstr = "exchangeable")`, `geeglm(Y ~ X * A, id = id, data = data, family = gaussian, corstr = "independence")`, and `geeglm(Y ~ X * A, id = id, data = data, family = gaussian, corstr = "ar1")` in R. -->

<!-- In GM1b, GM2b, GM3b, the fixed interaction effect is removed, so the analytical *GEE model* is given by: -->

<!-- $$ -->

<!-- Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + \beta_0 A_{it} + \epsilon_{it+1}. -->

<!-- $$ -->

<!-- The GEE models were fitted as `geeglm(Y ~ X + A, id = id, data = data, family = gaussian, corstr = "exchangeable")`, `geeglm(Y ~ X + A, id = id, data = data, family = gaussian, corstr = "independence")`, and `geeglm(Y ~ X + A, id = id, data = data, family = gaussian, corstr = "ar1")` in R. -->

# Results

@tbl-simulation-results presents the simulation results for each of the generative and analytical models. The estimates for the analytical MLM may be interpreted in terms of bias. Here we find that there is little to no bias for GM1/2/3A/3B and substantial bias for GM3. Thus, once we remove either the dependency of the random intercept with the covariate (GM1), the random slope $b_{i2}$ (GM3A) or the interaction $\beta_1$ (GM3B) from GM3, the bias dissapears or becomes extremely small. The bias in GM3 decreases as the number of timepoints $T$ increases from 10 to 30. Note that the MLM model fitting success rates are particularly poor for GM2, where in the worst case, only 87 of the 1000 models were fitted.

For the GEE with independence, the values refer to the difference between the estimated marginal effect---which should be unbiased under endogenous covariates [see @pepe1994]---and the specified conditional effect. Here we find that there is a enormous difference between these effects for GM2, which increases along with an increase in $T$ and $N$, up to a difference of more than 6,000. This is followed by a difference of around .07-.09 for GM1, .02-.04 for GM3, $\leq 0.015$ for GM3B and close to zero for GM3A. The GEE models fitted succesfully for all settings.

::: {#tbl-simulation-results}

```{r}
#| label: simulation-results
#| echo: false
#| cache: true
#| escape: false

results <- readRDS(here("simulation_results/GM123ad-1000reps-researchreport/results_beta0_bias_sd_success.rds"))

# remove gee_ex and gee_ar1
results <- results[, !grepl("gee_ex|gee_ar1", colnames(results))]
# rename GM = 3a to "3A" and GM = 3d to "3B"
results$GM <- ifelse(results$GM == "3a", "3A", ifelse(results$GM == "3d", "3B", results$GM))
# reorder columns
results <- results[, c(1, 2, 3, 4, 5, 8, 6, 7, 9)]

# Create a kableExtra table with a second header
kableExtra::kbl(results, booktabs = T,
                digits = c(0, 0, 0, 3, 3, 3, 3, 3, 3),
                col.names = c("GM", "T", "N", "Bias", "SD", "SR", "Difference", "SD", "SR")) %>%
  add_header_above(c(" " = 3, "MLM" = 3, "GEE-IND" = 3)) %>%
  kable_styling(full_width = FALSE) %>%
  column_spec(1, width = "5em") %>%
  collapse_rows(columns = 1:2, valign = "middle") %>%
  kableExtra::kable_classic_2() 
```

*Note.* SR: model fitting success rate. Bias: $\hat{\beta}_{0,MLM} - \beta_{0,MLM}$. Difference: $\hat{\beta}_{0,GEE} - \beta_{0,MLM}$. SD: standard deviation of estimates across replications.

Simulation results for Generative Models 1, 2, 3, 3A and 3B over 1000 replications

:::

# Discussion

This report employed both graphical methods and data simulations to understand and explain the issue of endogenous covariates. Now we will discuss the findings relating to the two research questions, while excluding GM2 due to model fitting issues.

Using the conditional independence assumption of @qian2020, we would expect, based on the path diagrams, that the treatment effect would be biased for GM3, 3A and 3B. On the other hand, the backdoor criterion suggested the absence of bias for all generative models. While @qian2020 show that GM3 is the only model with bias in the treatment effect, the backdoor criterion failed to identify this bias, as there is no backdoor path in the treatment effect. This may be explained by the fact that the DAG does not impose restrictions based on (a) the random slopes and (b) interaction effects. Concerns regarding the use of Pearl's backdoor criterion in situations with interaction effects have been voiced by several people (see @weinberg2007; @attia2022).

<!-- (1) When does the inclusion of endogenous variables in multilevel linear models result in biased estimates of the treatment effect? -->

<!--     (a) How is the bias in the treatment effect affected by the removal of the interaction $\beta_1$ from generative model 3? -->

<!--     (b) How is the bias in the treatment effect affected by the removal of the random slope $b_{i2}$ from generative model 3? -->

The first research question—pertaining to the extent of treatment effect bias in MLM estimates of generative model that were nested in GM3---was investigated using the analytical multilevel model. First, we reproduced the findings by @qian2020 that the estimators are consistent for GM1 and GM2, but inconsistent for GM3. Using additional generative models, we found that bias became indiscernable when removing from GM3 either the dependency between the random intercept and covariate (GM1), the random slope for treatment (GM3A) or the interaction effect (GM3B). This finding is in sharp contrast to the suggestion of the conditional independence assumption that the treatment effect would be biased for GM3, 3A and 3B.

<!-- (2) When does the inclusion of of endogenous covariates in multilevel linear models result in a discrepancy between conditional and marginal interpretations of the treatment effect? -->

<!--     (a) When removing the interaction $\beta_1$ from generative model 3, is there a difference between the marginal and conditional estimates of the treatment effect? -->

<!--     (b) When removing the random slope $b_{i2}$ from generative model 3, is there a difference between the marginal and conditional estimates of the treatment effect? -->

The second research question—related to the discrepancy between marginal and conditional interpretations of the treatment effect—was assessed with analytical GEE with working independence. Here we found extreme differences between the estimated marginal and specified conditional effect for GM2, suggesting that the marginal interpretation breaks down the most for this generative model[^2]. Hence, for this GM, a false interpretation of the MLM parameters as marginal, would potentially have great inferential consequences. For GM1 and GM3, there smaller but still noticeable differences between the marginal and conditional effect. This suggests that the marginal interpretation of the treatment effect may be recovered for GM3, 3A and 3B, but not for GM2. Especially for GM3A, this difference was practically indescernable, suggesting that the marginal interpretation of the treatment effect may be recovered. Conversely, @qian2020 notes that if the random effect in the model does not interact with the treatment variable, the interaction recovers its marginal interpretation but the treatment effect does not (p. 382). This difference in conclusions may be explained through the difference in approach: while @qian2020 provides an analytical answer, the current study provides approximations through simulations.

[^2]: Note, however, that this generative model may not be plausible given the extreme spread across the covariate and treatment variables.

<!-- Issues with GM2 -->

For the GM2 setting of @qian2020, we found several issues, which were most pronounced for $T = 30$. First, we noticed extreme model fitting issues for the MLM, due to, among other things, a lack of convergence and singularity. It should be noted that unlike the script used here, @qian2020 deals only with errors of the `lmer()` function, but not with warnings (e.g., pertaining to non-convergence) in their script. This discrepancy may explain the slightly different estimates of MLM bias for GM2. Second, we found extremely large GEE estimates of the treatment effect. This may be explained by the fact that the values of the covariate and outcome were also extremely great, often exceeding a million. All things considered, this suggests that GM2 may be a poorly specified model.



<!-- This may imply the presence of a non-stationary process (e.g., unit root). -->

<!-- # Other ideas -->

<!-- -   Initially, it seemed that the issue of endogenous covariates meant that, for unbiased estimation of the treatment effect, we should rely on GEE with independence. However, it is important not to conflate the issues mentioned by @qian2020. The first issue pertains to model interpretation: should we expect covariates to be endogenous and be primarily interested in marginal interpretations of the parameters rather than the person-specific (conditional-on-the-random-effect) interpretation, we should indeed employ GEE with working independence (OR STRUCTURAL MARGINAL MODELS??? CHECK THIS OUT IN ZOTERO). The second issue pertains to model fitting: once we conclude that person-specific interpretation aligns with our interest but we fear the presence of endogenous covariates, we have to assess the conditional independence assumption. -->

<!-- -   Across all generative model, we generally found that the estimated fixed treatment effect differed more from the specified MLM effect for the analytical GEE models than for the analytical MLM model. This is rather unsurprising, considering that the MLM is analyzing the exact same model as was specified, thereby putting it at an advantage over the GEE. -->

<!-- -   While GEE with independence may indeed yield unbiased estimators of the marginal effect as mentioned by @qian2020, they do not reflect the value of the model parameter. And since the endogeneity of a covariate implies that it is determined by the random effect, thereby making the marginal relationship between any given $X_{it}$ and $Y_{it+1}$ different as shown by @qian2020 (section 2.2), it is unclear what utility the combined marginal effect may serve in this context. -->

<!-- -   They set the same seed for every setting, potentially making the first dataset the same for every setting. Nevertheless, properly randomizing does not seem to drastically impact the results. The settings with GM2, $T = 30$ and $N \geq 100$ of @qian2020 results in very implausible values for the covariate $X_{it}$ and outcome $Y_{it+1}$, often exceeding a million. This may imply the presence of a non-stationary process (e.g., unit root). In the case of GEE analytical models, these settings result in very large estimates. -->

<!-- -   wasn't working exchangeability in GEE equivalent to MLM? then use this to argue why using this GEE variation. Because for the difference of GEE with independence from the actual specified conditional parameter estimates should indicate a difference between the marginal and conditional effect, but this is not bias. Can we speak of bias with exchangeability? -->

<!--     -   no it depends on the random effects, and it was compound symmetry. Leave this for next time. -->

\newpage

# References

::: {#refs}
:::

<!-- --- -->

<!-- nocite: | -->

<!--   @* -->

<!-- --- -->

<!-- # Appendix -->

<!-- ## Original Section from Qian et al. (2020): "4. Simulation" -->

<!-- In the simulation, we considered three generative models (GMs), all of which have an endogenous covariate. In the first two GMs, the endogenous covariate $X_{it}$ equals the previous outcome $Y_{it}$ plus some random noise, so the conditional independence assumption (10) is valid. In GM 3, the endogenous covariate depends directly on $b_i$, violating assumption (10). The details of the generative models are described below. -->

<!-- In GM1, we considered a simple case with only a random intercept and a random slope for $A_{it}$, so that $Z_{i(t_0)} = Z_{i(t_2)} = 1$ in model (7). The outcome is generated as: -->

<!-- $$ -->
<!-- Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + b_{i0} + A_{it} (\beta_0 + \beta_1 X_{it} + b_{i2}) + \epsilon_{it+1}. -->
<!-- $$ -->

<!-- The random effects $b_{i0} \sim N(0, \sigma_{b0}^2)$ and $b_{i2} \sim N(0, \sigma_{b2}^2)$ are independent of each other. The covariate is generated as $X_{i1} \sim N(0, 1)$, and for $t \geq 2$, -->

<!-- $$ -->
<!-- X_{it} = Y_{it} + N(0, 1). -->
<!-- $$ -->

<!-- The randomization probability $p_t$ is constant at $1/2$. The exogenous noise is $\epsilon_{it+1} \sim N(0, \sigma_\epsilon^2)$. -->

<!-- In GM2, we considered the case where $Z_{i(t_0)} = Z_{i(t_2)} = 1$, with time-varying randomization probability. The outcome is generated as: -->

<!-- $$ -->
<!-- Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + b_{i0} + b_{i1} X_{it} + A_{it} (\beta_0 + \beta_1 X_{it} + b_{i2} + b_{i3} X_{it}) + \epsilon_{it+1}. -->
<!-- $$ -->

<!-- The random effects $b_{ij} \sim N(0, \sigma_{b_j}^2)$, for $0 \leq j \leq 3$, are independent of each other. The covariate is generated as $X_{i1} \sim N(0, 1)$, and for $t \geq 2$, -->

<!-- $$ -->
<!-- X_{it} = Y_{it} + N(0, 1). -->
<!-- $$ -->

<!-- The randomization probability depends on $X_{it}$: -->

<!-- $$ -->
<!-- p_t = 0.7 \cdot 1(X_{it} > -1.27) + 0.3 \cdot 1(X_{it} \leq -1.27), -->
<!-- $$ -->

<!-- where $1(\cdot)$ represents the indicator function, and the cutoff $-1.27$ was chosen so that $p_t$ equals 0.7 or 0.3 for about half of the time. The exogenous noise is $\epsilon_{it+1} \sim N(0, \sigma_\epsilon^2)$. -->

<!-- GM3 is the same as GM 1, except that the covariate $X_{it}$ depends directly on $b_i$: -->

<!-- $$ -->
<!-- X_{i1} \sim N(b_{i0}, 1), \quad X_{it} = Y_{it} + N(b_{i0}, 1) \text{ for } t \geq 2. -->
<!-- $$ -->

<!-- We chose the following parameter values: -->

<!-- $$ -->
<!-- \alpha_0 = -2, \quad \alpha_1 = -0.3, \quad \beta_0 = 1, \quad \beta_1 = 0.3, -->
<!-- $$ -->

<!-- $$ -->
<!-- \sigma_{b0}^2 = 4, \quad \sigma_{b1}^2 = \frac{1}{4}, \quad \sigma_{b2}^2 = 1, \quad \sigma_{b3}^2 = \frac{1}{4}, \quad \sigma_\epsilon^2 = 1. -->
<!-- $$ -->

<!-- \newpage -->

<!-- ## Overview of Variations on Generative Model 3 -->

<!-- |  |  |  |  |  | -->
<!-- |---------------|---------------|---------------|---------------|---------------| -->
<!-- | Generative Model | random slope treatment $b_{i2}$ | interactie $\beta_1$ | fixed slope covariate $\alpha_1$ | bias | -->
<!-- | 3 | yes | yes | yes | yes, negative | -->
<!-- | 3a | no | yes | yes | no | -->
<!-- | 3d | yes | no | yes | no | -->
<!-- | 3h | yes | yes | no | yes, positive | -->

<!-- : Models with 1 Parameter Less -->

<!-- |  |  |  |  |  | -->
<!-- |---------------|---------------|---------------|---------------|---------------| -->
<!-- | Generative Model | random slope treatment $b_{i2}$ | interactie $\beta_1$ | fixed slope covariate $\alpha_1$ | bias | -->
<!-- | 3 | yes | yes | yes | yes, negative | -->
<!-- | 3b | no | no | yes | no | -->
<!-- | 3i | no | yes | no |  | -->
<!-- | 3j | yes | no | no |  | -->

<!-- ## Simulation Plan Proposal -->

<!-- To uncover the undesirable effects of endogenous covariates and investigate robustness against these effects, we will carry out simulations in which data will be generated according to several increasingly complex scenarios. These scenarios will be visually represented using directed acyclic graphs and analyzed using GEE, MLM and DSEM. We will start out with a scenario of the basic MLM---where a time-varying outcome $Y$ is regressed on a single time-varying predictor $X$ and in the presence of stable between person differences in the intercept---and increase the complexity until we reach the scenario that includes a time-varying endogenous covariate. The primary interest of this simulation study is the comparative performance of different specifications of the MLM and GEE in terms of bias in the estimation of the effect of $X$ to $Y$. The secondary interest is the efficiency in mean squared error (MSE). We consider settings with timepoints $T = 10,30$ and sample size $N = 30, 100, 200$. -->

<!-- Statistical analyses pertaining to the GEE and basic MLM will be performed in `R`, version 4.4.2 [@rcoreteam2024]. To fit the GEE, the R-package `geepack` [@halekoh2006] will evaluate several different working correlation structures, including independent, exchangeable, AR(1) and unstructured. To fit the basic MLM, the R-package `lme4` [@bates2015] will be employed, where we will use restricted maximum likelihood estimation. -->

<!-- ## Trash -->

<!-- ```{=tex} -->
<!-- \begin{table} -->
<!-- \centering -->
<!-- %\renewcommand{\arraystretch}{0.65} % Decrease row height -->
<!-- \begin{tabular}{@{}lcccccccc@{}} -->
<!-- \toprule -->
<!-- GM & T & N & \multicolumn{2}{c}{MLM} & \multicolumn{2}{c}{GEE-Ind} & MLM\_success \\ -->
<!-- \cmidrule(lr){4-5} \cmidrule(lr){6-7} -->
<!--    &   &   & Bias & SD & Bias & SD &   \\ \midrule -->
<!-- \multirow{6}{*}{1} & 10 & 30 & 0.000 & 0.238 & 0.071 & 0.296 & 0.998 \\  -->
<!--    & 10 & 100 & -0.012 & 0.129 & 0.074 & 0.169 & 1.000 \\  -->
<!--    & 10 & 200 & 0.003 & 0.093 & 0.085 & 0.116 & 0.999 \\  -->
<!--    & 30 & 30 & -0.001 & 0.203 & 0.085 & 0.224 & 0.998 \\  -->
<!--    & 30 & 100 & -0.007 & 0.107 & 0.083 & 0.123 & 0.996 \\  -->
<!--    & 30 & 200 & 0.001 & 0.079 & 0.094 & 0.088 & 0.996 \\  -->
<!-- \midrule -->
<!-- \multirow{6}{*}{2} & 10 & 30 & 0.011 & 0.282 & 0.306 & 1.630 & 0.925 \\  -->
<!--    & 10 & 100 & 0.005 & 0.147 & 0.565 & 1.836 & 0.881 \\  -->
<!--    & 10 & 200 & 0.008 & 0.103 & 0.935 & 1.887 & 0.844 \\  -->
<!--    & 30 & 30 & 0.000 & 0.220 & 182.565 & 4751.387 & 0.603 \\  -->
<!--    & 30 & 100 & -0.014 & 0.114 & -356.412 & 39799.388 & 0.247 \\  -->
<!--    & 30 & 200 & -0.013 & 0.087 & 6319.792 & 136201.790 & 0.087 \\  -->
<!-- \midrule -->
<!-- \multirow{6}{*}{3} & 10 & 30 & -0.052 & 0.245 & 0.020 & 0.249 & 0.999 \\  -->
<!--    & 10 & 100 & -0.064 & 0.134 & 0.024 & 0.141 & 1.000 \\  -->
<!--    & 10 & 200 & -0.051 & 0.096 & 0.035 & 0.097 & 1.000 \\  -->
<!--    & 30 & 30 & -0.024 & 0.206 & 0.030 & 0.208 & 0.997 \\  -->
<!--    & 30 & 100 & -0.030 & 0.108 & 0.027 & 0.112 & 0.996 \\  -->
<!--    & 30 & 200 & -0.023 & 0.080 & 0.037 & 0.081 & 0.997 \\  -->
<!-- \midrule -->
<!-- \multirow{6}{*}{3A} & 10 & 30 & 0.000 & 0.126 & -0.004 & 0.157 & 1.000 \\  -->
<!--    & 10 & 100 & 0.004 & 0.073 & 0.001 & 0.090 & 1.000 \\  -->
<!--    & 10 & 200 & 0.002 & 0.048 & -0.001 & 0.062 & 1.000 \\  -->
<!--    & 30 & 30 & -0.001 & 0.071 & -0.003 & 0.090 & 1.000 \\  -->
<!--    & 30 & 100 & 0.000 & 0.040 & -0.001 & 0.051 & 1.000 \\  -->
<!--    & 30 & 200 & -0.000 & 0.028 & -0.000 & 0.036 & 1.000 \\  -->
<!-- \midrule -->
<!-- \multirow{6}{*}{3B} & 10 & 30 & 0.001 & 0.217 & -0.013 & 0.241 & 0.999 \\  -->
<!--    & 10 & 100 & -0.008 & 0.121 & -0.008 & 0.138 & 1.000 \\  -->
<!--    & 10 & 200 & 0.005 & 0.087 & 0.003 & 0.097 & 1.000 \\  -->
<!--    & 30 & 30 & 0.000 & 0.193 & -0.004 & 0.200 & 1.000 \\  -->
<!--    & 30 & 100 & -0.008 & 0.103 & -0.007 & 0.108 & 0.997 \\  -->
<!--    & 30 & 200 & 0.001 & 0.075 & 0.001 & 0.080 & 0.999 \\  -->
<!-- \bottomrule -->
<!-- \end{tabular} -->
<!-- \end{table} -->
<!-- ``` -->

<!-- ::: {#tbl-simulation-summary} -->

<!-- ```{=tex} -->
<!-- \begin{table} -->
<!--     \centering -->
<!--     \begin{tabular}{@{}p{1.5cm} p{5cm} cc cc@{}} -->
<!--         \toprule -->
<!--         GM & Characteristics & \multicolumn{2}{c}{$\hat{\beta}_{0,MLM}-\beta_{0,MLM}$} & \multicolumn{2}{c}{$\hat{\beta}_{0,GEE-ind}-\beta_{0,MLM}$} \\  -->
<!--         \cmidrule(lr){3-4} \cmidrule(lr){5-6} -->
<!--            &                 & \( T = 10 \) & \( T = 30 \) & \( T = 10 \) & \( T = 30 \) \\ \midrule -->
<!--         1  & Includes random intercept and random slope for treatment & 0.003 & 0.001 & 0.086 & 0.090 \\ -->
<!--         3  & Model 1 with dependency random intercept and covariate       & -0.051 & -0.023 & 0.033 & 0.032 \\ -->
<!--         3A & Model 3 without random slope $b_{2i}$           & 0.002 & -0.000 & -0.001 & -0.000 \\ -->
<!--         3B & Model 3 without interaction effect $\beta_1$ (between treatment and covariate) & 0.005 & 0.001 & 0.003 & 0.001 \\ -->
<!--         \bottomrule -->
<!--     \end{tabular} -->
<!-- \end{table} -->
<!-- ``` -->

<!-- Simulation results for $N=200$ and $\beta_{0,MLM} = 1$ over $1000$ replications, with $T=10$ and $T=30$. -->

<!-- ::: -->

<!-- ```{r} -->
<!-- #| label: fig-DAGs -->
<!-- #| fig-cap: "DAG for Generative Models" -->
<!-- #| layout-ncol: 2 -->
<!-- #| fig-subcap: -->
<!-- #|  - "Generative Model 1" -->
<!-- #|  - "Generative Model 2" -->
<!-- #|  - "Generative Model 3, 3A and 3B" -->
<!-- #| echo: false -->
<!-- #| eval: false -->
<!-- #| cache: true -->

<!-- GM1_DAG <- dagitty('dag { -->
<!-- bb="0,0,1,1" -->
<!-- U [latent,pos="0.500,0.200"] -->
<!-- A_1 [exposure,pos="0.200,0.600"] -->
<!-- A_2 [exposure,pos="0.400,0.600"] -->
<!-- Y_2 [outcome,pos="0.450,0.450"] -->
<!-- Y_3 [outcome,pos="0.650,0.450"] -->
<!-- X_1 [adjusted,pos="0.200,0.300"] -->
<!-- X_2 [adjusted,pos="0.400,0.300"] -->
<!-- X_3 [pos="0.600,0.300"] -->
<!-- U -> Y_2 -->
<!-- U -> Y_3 -->
<!-- A_1 -> Y_2 -->
<!-- A_2 -> Y_3 -->
<!-- Y_2 -> X_2 -->
<!-- Y_3 -> X_3 -->
<!-- X_1 -> Y_2 -->
<!-- X_2 -> Y_3 -->
<!-- }') -->

<!-- GM2_DAG <- dagitty('dag { -->
<!-- bb="0,0,1,1" -->
<!-- U [latent,pos="0.500,0.200"] -->
<!-- A_1 [exposure,pos="0.200,0.600"] -->
<!-- A_2 [exposure,pos="0.400,0.600"] -->
<!-- Y_2 [outcome,pos="0.450,0.450"] -->
<!-- Y_3 [outcome,pos="0.650,0.450"] -->
<!-- X_1 [adjusted,pos="0.200,0.300"] -->
<!-- X_2 [adjusted,pos="0.400,0.300"] -->
<!-- X_3 [pos="0.600,0.300"] -->
<!-- U -> Y_2 -->
<!-- U -> Y_3 -->
<!-- A_1 -> Y_2 -->
<!-- A_2 -> Y_3 -->
<!-- Y_2 -> X_2 -->
<!-- Y_3 -> X_3 -->
<!-- X_1 -> A_1 -->
<!-- X_1 -> Y_2 -->
<!-- X_2 -> A_2 -->
<!-- X_2 -> Y_3 -->
<!-- } -->
<!-- ') -->

<!-- GM3ab_DAG <- dagitty('dag { -->
<!-- bb="0,0,1,1" -->
<!-- U [latent,pos="0.500,0.200"] -->
<!-- A_1 [exposure,pos="0.200,0.600"] -->
<!-- A_2 [exposure,pos="0.400,0.600"] -->
<!-- Y_2 [outcome,pos="0.450,0.450"] -->
<!-- Y_3 [outcome,pos="0.650,0.450"] -->
<!-- X_1 [adjusted,pos="0.200,0.300"] -->
<!-- X_2 [adjusted,pos="0.400,0.300"] -->
<!-- X_3 [pos="0.600,0.300"] -->
<!-- U -> Y_2 -->
<!-- U -> Y_3 -->
<!-- U -> X_1 -->
<!-- U -> X_2 -->
<!-- U -> X_3 -->
<!-- A_1 -> Y_2 -->
<!-- A_2 -> Y_3 -->
<!-- Y_2 -> X_2 -->
<!-- Y_3 -> X_3 -->
<!-- X_1 -> Y_2 -->
<!-- X_2 -> Y_3 -->
<!-- }') -->

<!-- ggdag::ggdag_status(GM1_DAG) + ggdag::theme_dag() -->
<!-- ggdag::ggdag_status(GM2_DAG) + ggdag::theme_dag() -->
<!-- ggdag::ggdag_status(GM3ab_DAG) + ggdag::theme_dag() -->
<!-- ``` -->

<!-- ```{=tex} -->

<!-- % latex table generated in R 4.4.2 by xtable 1.8-4 package -->

<!-- % Mon Dec  9 11:20:44 2024 -->

<!-- \begin{table}[H] -->

<!-- \centering -->

<!-- \caption{Results for beta0 bias with Standard deviation and success rate, 1000 replications}  -->

<!-- \renewcommand{\arraystretch}{0.7} % Decrease row height -->

<!-- \resizebox{\textwidth}{!}{ -->

<!-- \begin{tabular}{lrrrrrrr} -->

<!-- \toprule -->

<!-- GM & T & N & MLM\_bias & MLM\_sd & GEE-Ind\_bias & GEE-Ind\_sd & MLM\_success \\  -->

<!-- \midrule -->

<!-- \multirow{6}{*}{1} & 10 & 30 & 0.000 & 0.238 & 0.071 & 0.296 & 0.998 \\  -->

<!--    & 10 & 100 & -0.012 & 0.129 & 0.074 & 0.169 & 1.000 \\  -->

<!--    & 10 & 200 & 0.003 & 0.093 & 0.085 & 0.116 & 0.999 \\  -->

<!--    & 30 & 30 & -0.001 & 0.203 & 0.085 & 0.224 & 0.998 \\  -->

<!--    & 30 & 100 & -0.007 & 0.107 & 0.083 & 0.123 & 0.996 \\  -->

<!--    & 30 & 200 & 0.001 & 0.079 & 0.094 & 0.088 & 0.996 \\  -->

<!-- \midrule -->

<!-- \multirow{6}{*}{2} & 10 & 30 & 0.011 & 0.282 & 0.306 & 1.630 & 0.925 \\  -->

<!--    & 10 & 100 & 0.005 & 0.147 & 0.565 & 1.836 & 0.881 \\  -->

<!--    & 10 & 200 & 0.008 & 0.103 & 0.935 & 1.887 & 0.844 \\  -->

<!--    & 30 & 30 & 0.000 & 0.220 & 182.565 & 4751.387 & 0.603 \\  -->

<!--    & 30 & 100 & -0.014 & 0.114 & -356.412 & 39799.388 & 0.247 \\  -->

<!--    & 30 & 200 & -0.013 & 0.087 & 6319.792 & 136201.790 & 0.087 \\  -->

<!-- \midrule -->

<!-- \multirow{6}{*}{3} & 10 & 30 & -0.052 & 0.245 & 0.020 & 0.249 & 0.999 \\  -->

<!--    & 10 & 100 & -0.064 & 0.134 & 0.024 & 0.141 & 1.000 \\  -->

<!--    & 10 & 200 & -0.051 & 0.096 & 0.035 & 0.097 & 1.000 \\  -->

<!--    & 30 & 30 & -0.024 & 0.206 & 0.030 & 0.208 & 0.997 \\  -->

<!--    & 30 & 100 & -0.030 & 0.108 & 0.027 & 0.112 & 0.996 \\  -->

<!--    & 30 & 200 & -0.023 & 0.080 & 0.037 & 0.081 & 0.997 \\  -->

<!-- \midrule -->

<!-- \multirow{6}{*}{3A} & 10 & 30 & 0.000 & 0.126 & -0.004 & 0.157 & 1.000 \\  -->

<!--    & 10 & 100 & 0.004 & 0.073 & 0.001 & 0.090 & 1.000 \\  -->

<!--    & 10 & 200 & 0.002 & 0.048 & -0.001 & 0.062 & 1.000 \\  -->

<!--    & 30 & 30 & -0.001 & 0.071 & -0.003 & 0.090 & 1.000 \\  -->

<!--    & 30 & 100 & 0.000 & 0.040 & -0.001 & 0.051 & 1.000 \\  -->

<!--    & 30 & 200 & -0.000 & 0.028 & -0.000 & 0.036 & 1.000 \\  -->

<!-- \midrule -->

<!-- \multirow{6}{*}{3B} & 10 & 30 & 0.001 & 0.217 & -0.013 & 0.241 & 0.999 \\  -->

<!--    & 10 & 100 & -0.008 & 0.121 & -0.008 & 0.138 & 1.000 \\  -->

<!--    & 10 & 200 & 0.005 & 0.087 & 0.003 & 0.097 & 1.000 \\  -->

<!--    & 30 & 30 & 0.000 & 0.193 & -0.004 & 0.200 & 1.000 \\  -->

<!--    & 30 & 100 & -0.008 & 0.103 & -0.007 & 0.108 & 0.997 \\  -->

<!--    & 30 & 200 & 0.001 & 0.075 & 0.001 & 0.080 & 0.999 \\  -->

<!-- \bottomrule -->

<!-- \end{tabular} -->

<!-- } -->

<!-- \label{tab:beta0_bias_sd_success} -->

<!-- \end{table} -->

<!-- ``` -->

<!-- ::: {layout-ncol=2} -->

<!-- ```{r} -->

<!-- #| label: fig-GM1_DAG -->

<!-- #| engine: 'tikz' -->

<!-- #| echo: false -->

<!-- #| cache: true -->

<!-- #| fig-cap: "GM 1" -->

<!-- #| eval: true -->

<!-- \begin{tikzpicture} -->

<!--   % Nodes for observed variables (squares) -->

<!--   \node[draw, rectangle] (X1) at (0, -2) {$X_{1}$}; -->

<!--   \node[draw, rectangle] (A1) at (0, 2) {$A_{1}$}; -->

<!--   \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2}$}; -->

<!--   \node[draw, rectangle] (X2) at (3, -2) {$X_{2}$}; -->

<!--   \node[draw, rectangle] (A2) at (3, 2) {$A_{2}$}; -->

<!--   \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3}$}; -->

<!--   \node[draw, rectangle] (X3) at (8, -2) {$X_{3}$}; -->

<!--   % Node for latent variable (circle) -->

<!--   \node[draw, circle, fill=gray!20] (U) at (6, 4) {$U$}; -->

<!--   % Directed edges -->

<!--   \draw [->] (U) -- (Y2); -->

<!--   \draw [->] (U) -- (Y3); -->

<!--   \draw [->] (A2) -- (Y3); -->

<!--   \draw [->] (X2) -- (Y3); -->

<!--   \draw [->] (Y2) -- (X2); -->

<!--   \draw [->] (A1) -- (Y2); -->

<!--   \draw [->] (X1) -- (Y2); -->

<!--   \draw [->] (Y3) -- (X3); -->

<!-- \end{tikzpicture} -->

<!-- ``` -->

<!-- ```{r} -->

<!-- #| label: fig-GM2_DAG -->

<!-- #| engine: 'tikz' -->

<!-- #| echo: false -->

<!-- #| cache: true -->

<!-- #| fig-cap: "GM 2 ($t = 1, 2, 3$)" -->

<!-- #| eval: true -->

<!-- \begin{tikzpicture} -->

<!--   % Nodes for observed variables (squares) -->

<!--   \node[draw, rectangle] (X1) at (0, -2) {$X_{1}$}; -->

<!--   \node[draw, rectangle] (A1) at (0, 2) {$A_{1}$}; -->

<!--   \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2}$}; -->

<!--   \node[draw, rectangle] (X2) at (3, -2) {$X_{2}$}; -->

<!--   \node[draw, rectangle] (A2) at (3, 2) {$A_{2}$}; -->

<!--   \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3}$}; -->

<!--   \node[draw, rectangle] (X3) at (8, -2) {$X_{3}$}; -->

<!--   % Node for latent variable (circle) -->

<!--   \node[draw, circle, fill=gray!20] (U) at (6, 4) {$U$}; -->

<!--   % Directed edges -->

<!--   \draw [->] (U) -- (Y2); -->

<!--   \draw [->] (U) -- (Y3); -->

<!--   \draw [->] (A2) -- (Y3); -->

<!--   \draw [->] (X2) -- (Y3); -->

<!--   \draw [->] (Y2) -- (X2); -->

<!--   \draw [->] (A1) -- (Y2); -->

<!--   \draw [->] (X1) -- (Y2); -->

<!--   \draw [->] (Y3) -- (X3); -->

<!--   % Curved edges for X1 -> A1 and X2 -> A2 -->

<!--   \draw [->, bend left=40] (X1) to (A1); -->

<!--   \draw [->, bend left=40] (X2) to (A2); -->

<!-- \end{tikzpicture} -->

<!-- ``` -->

<!-- ```{r} -->

<!-- #| label: fig-GM3_DAG -->

<!-- #| engine: 'tikz' -->

<!-- #| echo: false -->

<!-- #| cache: true -->

<!-- #| fig-cap: "GM 3, 3A, 3B" -->

<!-- #| eval: true -->

<!-- \begin{tikzpicture} -->

<!--   % Nodes for observed variables -->

<!--   \node[draw, rectangle] (X1) at (0, -2) {$X_1$}; -->

<!--   \node[draw, rectangle] (A1) at (0, 2) {$A_1$}; -->

<!--   \node[draw, rectangle] (Y2) at (3, 0) {$Y_2$}; -->

<!--   \node[draw, rectangle] (X2) at (3, -2) {$X_2$}; -->

<!--   \node[draw, rectangle] (A2) at (3, 2) {$A_2$}; -->

<!--   \node[draw, rectangle] (Y3) at (8, 0) {$Y_3$}; -->

<!--   \node[draw, rectangle] (X3) at (8, -2) {$X_3$}; -->

<!--   % Latent variable -->

<!--   \node[draw, circle, fill=gray!20] (U) at (6, 4) {$U$}; -->

<!--   % Edges -->

<!--   \draw [->] (U) -- (Y2); -->

<!--   \draw [->] (U) -- (Y3); -->

<!--   \draw [->] (U) -- (X1); -->

<!--   \draw [->] (U) -- (X2); -->

<!--   \draw [->] (U) -- (X3); -->

<!--   \draw [->] (A1) -- (Y2); -->

<!--   \draw [->] (X1) -- (Y2); -->

<!--   \draw [->] (Y2) -- (X2); -->

<!--   \draw [->] (A2) -- (Y3); -->

<!--   \draw [->] (X2) -- (Y3); -->

<!--   \draw [->] (Y3) -- (X3); -->

<!-- \end{tikzpicture} -->

<!-- ``` -->

<!-- DAGs for Generative Models 1, 2, and 3.  -->

<!-- The red arrows indicate the biased paths after controlling for the covariate $X_{it}$. -->

<!-- ::: -->

<!-- ```{r} -->

<!-- #| label: fig-GM_DAG -->

<!-- #| engine: 'tikz' -->

<!-- #| echo: false -->

<!-- #| cache: true -->

<!-- #| fig-cap: "DAGs for Generative Models 1, 2, and 3.  ($t = 1, 2, 3$)" -->

<!-- #| ncols: 2 -->

<!-- #| fig-subcap:  -->

<!-- #|  - "GM1" -->

<!-- #|  - "GM2" -->

<!-- #|  - "GM3" -->

<!-- #| eval: true -->

<!-- \begin{tikzpicture} -->

<!--   % Nodes for observed variables (squares) -->

<!--   \node[draw, rectangle] (X1) at (0, -2) {$X_{1}$}; -->

<!--   \node[draw, rectangle] (A1) at (0, 2) {$A_{1}$}; -->

<!--   \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2}$}; -->

<!--   \node[draw, rectangle] (X2) at (3, -2) {$X_{2}$}; -->

<!--   \node[draw, rectangle] (A2) at (3, 2) {$A_{2}$}; -->

<!--   \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3}$}; -->

<!--   \node[draw, rectangle] (X3) at (8, -2) {$X_{3}$}; -->

<!--   % Node for latent variable (circle) -->

<!--   \node[draw, circle, fill=gray!20] (U) at (6, 4) {$U$}; -->

<!--   % Directed edges -->

<!--   \draw [->] (U) -- (Y2); -->

<!--   \draw [->] (U) -- (Y3); -->

<!--   \draw [->] (A2) -- (Y3); -->

<!--   \draw [->] (X2) -- (Y3); -->

<!--   \draw [->] (Y2) -- (X2); -->

<!--   \draw [->] (A1) -- (Y2); -->

<!--   \draw [->] (X1) -- (Y2); -->

<!--   \draw [->] (Y3) -- (X3); -->

<!-- \end{tikzpicture} -->

<!-- \begin{tikzpicture} -->

<!--   % Nodes for observed variables (squares) -->

<!--   \node[draw, rectangle] (X1) at (0, -2) {$X_{1}$}; -->

<!--   \node[draw, rectangle] (A1) at (0, 2) {$A_{1}$}; -->

<!--   \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2}$}; -->

<!--   \node[draw, rectangle] (X2) at (3, -2) {$X_{2}$}; -->

<!--   \node[draw, rectangle] (A2) at (3, 2) {$A_{2}$}; -->

<!--   \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3}$}; -->

<!--   \node[draw, rectangle] (X3) at (8, -2) {$X_{3}$}; -->

<!--   % Node for latent variable (circle) -->

<!--   \node[draw, circle, fill=gray!20] (U) at (6, 4) {$U$}; -->

<!--   % Directed edges -->

<!--   \draw [->] (U) -- (Y2); -->

<!--   \draw [->] (U) -- (Y3); -->

<!--   \draw [->] (A2) -- (Y3); -->

<!--   \draw [->] (X2) -- (Y3); -->

<!--   \draw [->] (Y2) -- (X2); -->

<!--   \draw [->] (A1) -- (Y2); -->

<!--   \draw [->] (X1) -- (Y2); -->

<!--   \draw [->] (Y3) -- (X3); -->

<!--   % Curved edges for X1 -> A1 and X2 -> A2 -->

<!--   \draw [->, bend left=40] (X1) to (A1); -->

<!--   \draw [->, bend left=40] (X2) to (A2); -->

<!-- \end{tikzpicture} -->

<!-- \begin{tikzpicture} -->

<!--   % Nodes for observed variables -->

<!--   \node[draw, rectangle] (X1) at (0, -2) {$X_1$}; -->

<!--   \node[draw, rectangle] (A1) at (0, 2) {$A_1$}; -->

<!--   \node[draw, rectangle] (Y2) at (3, 0) {$Y_2$}; -->

<!--   \node[draw, rectangle] (X2) at (3, -2) {$X_2$}; -->

<!--   \node[draw, rectangle] (A2) at (3, 2) {$A_2$}; -->

<!--   \node[draw, rectangle] (Y3) at (8, 0) {$Y_3$}; -->

<!--   \node[draw, rectangle] (X3) at (8, -2) {$X_3$}; -->

<!--   % Latent variable -->

<!--   \node[draw, circle, fill=gray!20] (U) at (6, 4) {$U$}; -->

<!--   % Edges -->

<!--   \draw [->] (U) -- (Y2); -->

<!--   \draw [->] (U) -- (Y3); -->

<!--   \draw [->] (U) -- (X1); -->

<!--   \draw [->] (U) -- (X2); -->

<!--   \draw [->] (U) -- (X3); -->

<!--   \draw [->] (A1) -- (Y2); -->

<!--   \draw [->] (X1) -- (Y2); -->

<!--   \draw [->] (Y2) -- (X2); -->

<!--   \draw [->] (A2) -- (Y3); -->

<!--   \draw [->] (X2) -- (Y3); -->

<!--   \draw [->] (Y3) -- (X3); -->

<!-- \end{tikzpicture} -->

<!-- ``` -->

<!-- a -->

<!-- ```{=tex} -->

<!-- \begin{figure}[htbp] -->

<!--     % Subfigure 1 -->

<!--     \begin{subfigure}[t]{0.48\textwidth} -->

<!--         \centering -->

<!--         \begin{tikzpicture} -->

<!--           % Nodes for observed variables (squares) -->

<!--           \node[draw, rectangle] (X1) at (0, -2) {$X_{1}$}; -->

<!--           \node[draw, rectangle] (A1) at (0, 2) {$A_{1}$}; -->

<!--           \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2}$}; -->

<!--           \node[draw, rectangle] (X2) at (3, -2) {$X_{2}$}; -->

<!--           \node[draw, rectangle] (A2) at (3, 2) {$A_{2}$}; -->

<!--           \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3}$}; -->

<!--           \node[draw, rectangle] (X3) at (8, -2) {$X_{3}$}; -->

<!--           % Node for latent variable (circle) -->

<!--           \node[draw, circle, fill=gray!20] (U) at (6, 4) {$U$}; -->

<!--           % Directed edges -->

<!--           \draw [->] (U) -- (Y2); -->

<!--           \draw [->] (U) -- (Y3); -->

<!--           \draw [->] (A2) -- (Y3); -->

<!--           \draw [->] (X2) -- (Y3); -->

<!--           \draw [->] (Y2) -- (X2); -->

<!--           \draw [->] (A1) -- (Y2); -->

<!--           \draw [->] (X1) -- (Y2); -->

<!--           \draw [->] (Y3) -- (X3); -->

<!--         \end{tikzpicture} -->

<!--         \caption{Generative Model 1} -->

<!--         \label{fig:GM1_DAG} -->

<!--     \end{subfigure} -->

<!--     \hfill -->

<!--     % Subfigure 2 -->

<!--     \begin{subfigure}[t]{0.48\textwidth} -->

<!--         \centering -->

<!--         \begin{tikzpicture} -->

<!--           % Nodes for observed variables (squares) -->

<!--           \node[draw, rectangle] (X1) at (0, -2) {$X_{1}$}; -->

<!--           \node[draw, rectangle] (A1) at (0, 2) {$A_{1}$}; -->

<!--           \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2}$}; -->

<!--           \node[draw, rectangle] (X2) at (3, -2) {$X_{2}$}; -->

<!--           \node[draw, rectangle] (A2) at (3, 2) {$A_{2}$}; -->

<!--           \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3}$}; -->

<!--           \node[draw, rectangle] (X3) at (8, -2) {$X_{3}$}; -->

<!--           % Node for latent variable (circle) -->

<!--           \node[draw, circle, fill=gray!20] (U) at (6, 4) {$U$}; -->

<!--           % Directed edges -->

<!--           \draw [->] (U) -- (Y2); -->

<!--           \draw [->] (U) -- (Y3); -->

<!--           \draw [->] (A2) -- (Y3); -->

<!--           \draw [->] (X2) -- (Y3); -->

<!--           \draw [->] (Y2) -- (X2); -->

<!--           \draw [->] (A1) -- (Y2); -->

<!--           \draw [->] (X1) -- (Y2); -->

<!--           \draw [->] (Y3) -- (X3); -->

<!--           % Curved edges for X1 -> A1 and X2 -> A2 -->

<!--           \draw [->, bend left=40] (X1) to (A1); -->

<!--           \draw [->, bend right=40] (X2) to (A2); -->

<!--         \end{tikzpicture} -->

<!--         \caption{Generative Model 2} -->

<!--         \label{fig:GM2_DAG} -->

<!--     \end{subfigure} -->

<!--     \vspace{1em} -->

<!--     % Subfigure 3 -->

<!--     \begin{subfigure}[t]{0.48\textwidth} -->

<!--         \centering -->

<!--         \begin{tikzpicture} -->

<!--           % Nodes for observed variables -->

<!--           \node[draw, rectangle] (X1) at (0, -2) {$X_1$}; -->

<!--           \node[draw, rectangle] (A1) at (0, 2) {$A_1$}; -->

<!--           \node[draw, rectangle] (Y2) at (3, 0) {$Y_2$}; -->

<!--           \node[draw, rectangle] (X2) at (3, -2) {$X_2$}; -->

<!--           \node[draw, rectangle] (A2) at (3, 2) {$A_2$}; -->

<!--           \node[draw, rectangle] (Y3) at (8, 0) {$Y_3$}; -->

<!--           \node[draw, rectangle] (X3) at (8, -2) {$X_3$}; -->

<!--           % Latent variable -->

<!--           \node[draw, circle, fill=gray!20] (U) at (6, 4) {$U$}; -->

<!--           % Edges -->

<!--           \draw [->] (U) -- (Y2); -->

<!--           \draw [->] (U) -- (Y3); -->

<!--           \draw [->] (U) -- (X1); -->

<!--           \draw [->] (U) -- (X2); -->

<!--           \draw [->] (U) -- (X3); -->

<!--           \draw [->] (A1) -- (Y2); -->

<!--           \draw [->] (X1) -- (Y2); -->

<!--           \draw [->] (Y2) -- (X2); -->

<!--           \draw [->] (A2) -- (Y3); -->

<!--           \draw [->] (X2) -- (Y3); -->

<!--           \draw [->] (Y3) -- (X3); -->

<!--         \end{tikzpicture} -->

<!--         \caption{Generative Model 3} -->

<!--         \label{fig:GM3_DAG} -->

<!--     \end{subfigure} -->

<!--     \caption{DAGs for Generative Models 1, 2, and 3. Each subfigure corresponds to a distinct model configuration.} -->

<!--     \label{fig:combined_DAGs} -->

<!-- \end{figure} -->

<!-- ``` -->
