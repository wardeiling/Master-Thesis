---
title: "Treatment Effect Bias in Multilevel Linear Models under Time-Varying Endogenous Covariates: A New Look at Qian et al. (2020)"
# The Impact of Endogenous Time-Varying Covariates on Treatment Effect Estimates in Multilevel Linear Models: Revisiting Qian et al. (2020)
# "Untangling Bias in Multilevel Linear Models: The Role of Endogenous Time-Varying Covariates"
# "The Dangers of Including Time-Varying Endogenous Covariates in the Multilevel Linear Model"
# Estimation of Effects of Endogenous Time-Varying Covariates: A Comparison Of Multilevel Linear Modeling and Generalized Estimating Equations"
subtitle: "Research Report" 
author: 
  - name: "Ward B. Eiling (9294163)"
    orcid: "0009-0007-8114-9497"
    affiliation: 
      - name: "Utrecht University"
      - department: "Methodology and Statistics"
date: last-modified # deadline: "Dec 22, 2024"
date-format: long
format: #docx
  pdf:
    papersize: a4
    fig-pos: 'H'
    tbl-pos: 'H'
    # keep-tex: true
    # toc: true
    number-sections: true
    colorlinks: true
    indent: true
    header-includes:
      - \usepackage{fancyhdr}
      - \usepackage{amsmath}
      - \usepackage{multirow}
      - \usepackage{booktabs}
      - \usepackage{pifont}
      # - \usepackage{float}
      # - \usepackage{tikz}
      # - \usepackage{subcaption}
    geometry:
      - top=2.5cm
      - bottom=2.5cm
      - left=2.5cm
      - right=2.5cm
    mainfont: Latin Modern Roman
    sansfont: Latin Modern Roman
    linestretch: 2
    fontsize: 11pt
    template-partials:
      - "before-body.tex"
bibliography: references.bib
link-citations: true
csl: apa7.csl
execute:
  message: false
  warning: false
fig-cap-location: top
---

```{r}
#| label: Packages
#| echo: false

# for knitting the document
library(papaja)
library(rlang)
library(rmarkdown)
library(knitr)
library(kableExtra)
library(here)
# for general data manipulation and plotting
library(tidyverse)
# for estimation
# library(lme4)
# library(geepack)
# for diagrams
library(dagitty)
library(ggdag)
library(DiagrammeR)
library(magick)
library(pdftools)
```

# Introduction

<!-- -   Start with a paragraph describing a problem in the real-life world (so that a BoS member not familiar with statistics understands why you are pursuing research in this direction); -->

Across a wide range of disciplines, researchers analyze clustered longitudinal, observational data to investigate prospective causal relationships between variables. When analyzing such data, psychological researchers most commonly use the multilevel linear model[^1] [MLM, @bauer2011], which—in the context of longitudinal data analysis—partitions observed variance into stable between-person differences and within-person fluctuations [@hamaker2020]. Research questions explored with the MLM often result in the availability of time invariant and/or time-varying covariates, the latter measured repeatedly over time. The inclusion of covariates is a common strategy to improve parameter precision [@boruvka2018] and address bias introduced by confounders [@Robins2000]. Nevertheless, this approach is not universally beneficial, as conditioning on endogenous covariates—those influenced by (prior) treatment/exposure or outcome—can create challenges for standard methods like MLMs, which implicitly assume the exogeneity of covariates [@Erler2019].

[^1]: The MLM is known by various names in different substantive fields, including: linear mixed model, hierarchical linear model, random-effect model and mixed-effects model.

<!-- -   Then, add a paragraph describing what is known in the literature; -->

<!-- -   Also, describe what is not known and which gap you will address in your thesis; End with a clear research question and, if applicable, your hypothesis (for confirmatory research questions – it should be a testable hypothesis) or expectations (for exploratory research questions – can be much vaguer because of the explorative nature of the question). -->

Dating back to the work of Pepe and Anderson [-@pepe1994] in the biostatistics literature, this assumption has been shown to be non-trivial when endogenous covariates are time-varying. In fact, their inclusion in longitudinal studies can lead to biased treatment effect estimates, an issue that, despite its significance, has received limited attention in psychological research. Building on this foundation, a recent paper by @qian2020 examined the suitability of MLM for estimating the causal effect of a time-varying treatment. Specifically, they focused on settings where the exposure is randomly assigned at each occasion within individuals. Such randomized treatments may include, for example, prompts delivered through push notifications to remind participants of mindfulness-based strategies [@nahum-shani2021; @walton2018]. While random assignment with a constant probability might seem sufficient to identify (the presence and absence of) causal effects, @qian2020 showed that model fitting issues and parameter bias can arise when a *time-varying endogenous covariate* is present.

<!-- The issues described in @qian2020 can be categorized into two main problems: (1) model interpretation and (2) model fitting. In this investigation, we focus on the latter issue of model fitting of the MLM  -->

However, due to a divide between the disciplines that employ the MLM, such critiques appear to have largely failed to reach the applied researcher in psychology. One specific reason might be that the technical jargon in other disciplines makes it difficult for researchers to recognize when and how these issues emerge. This report aims to explore why @qian2020 observed biased estimates of the treatment effect in certain data-generating mechanisms containing endogenous covariates, while not for others. Additionally, it seeks to explain this issue to an audience of psychologists. The study will first employ graphical diagrams to assess two criteria across various models involving an endogenous time-varying covariate and randomized treatment: (a) path diagrams to evaluate the conditional independence assumption introduced by @qian2020 and (b) directed acyclic graphs (DAGs) to assess the backdoor criterion [@pearl1988]. Subsequently, data simulations based on @qian2020's original scenarios, along with additional ones, will be performed to reproduce and isolate the underlying issue and evaluate whether these criteria can predict the presence of bias in the treatment effect. The following research question will be addressed: *When does the inclusion of endogenous variables in multilevel linear models result in biased estimates of the treatment effect?*

<!-- -   Also, describe what is not known and which gap you will address in your thesis; End with a clear research question and, if applicable, your hypothesis (for confirmatory research questions – it should be a testable hypothesis) or expectations (for exploratory research questions – can be much vaguer because of the explorative nature of the question). -->

# Methods

In this section, we describe four generative models (GMs) that incorporate a time-varying endogenous covariate and randomized treatment, followed by the methodology used to evaluate treatment effect bias across settings.

## Data Generation

We consider two GMs from @qian2020, one (GM-A) being a special case of the general model (GM-G) where bias was detected. To further isolate the source of bias, we introduce two additional special cases, GM-B and GM-C. We first describe the GM-G in detail, and then proceed to its three special cases.

### General Model

Following the original notation of @qian2020, the outcome of GM-G was generated as:

$$
Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + b_{i0} + A_{it} (\beta_0 + \beta_1 X_{it} + b_{i2}) + \epsilon_{it+1}
$$ {#eq-modelG}

where $Y_{it+1}$ is the outcome, $X_{it}$ the covariate, $A_{it}$ the treatment, $b_{i0}$ the random intercept, $b_{i2}$ the random slope for treatment, and $\epsilon_{it+1}$ the error term. The observed variables vary across individuals $i$ and timepoints $t$. Alternatively, the model can be rewritten in the multilevel notation of Raudenbush and Bryk [-@raudenbush2002], with at the within-person level (level 1):

$$ 
\begin{aligned} Y_{it+1} &= \alpha_0 + \alpha_1 X_{it} + b_{i0} + A_{it} (\beta_0 + \beta_1 X_{it} + b_{i2}) + \epsilon_{it+1} \\ &= (\alpha_0 + b_{i0}) + \alpha_1 X_{it} + (\beta_0 + b_{i2}) A_{it} + \beta_1 A_{it} X_{it} + \epsilon_{it+1} \\ &= \pi_{0i} + \pi_{1i} X_{it} + \pi_{2i} A_{it} + \pi_{3i} A_{it} X_{it} + \epsilon_{it+1}. \end{aligned}
$$

and at the between-person level (level 2):

$$
\begin{aligned}
\pi_{0i} &= \alpha_0 + b_{i0}, & \text{where} \quad b_{i0} &\sim \mathcal{N}(0, \sigma_{b0}^2), \\
\pi_{1i} &= \alpha_1, \\
\pi_{2i} &= \beta_0 + b_{i2}, & \text{where} \quad b_{i2} &\sim \mathcal{N}(0, \sigma_{b2}^2), \\
\pi_{3i} &= \beta_1.
\end{aligned}
$$

The parameters $\alpha_0 = -2$, $\alpha_1 = -0.3$, $\beta_0 = 1$, and $\beta_1 = 0.3$ are fixed effects that are constant across individuals. Conversely, $b_{i0}$ and $b_{i2}$ are independent random effects that capture individual-specific deviations from population parameters $\alpha_0$ and $\beta_0$ respectively. The presence of the interaction term $\beta_1$ implies treatment heterogeneity: the effect of $A_{it}$ on $Y_{it+1}$ depends on the value of $X_{it}$. The random intercept $b_{i0}$, random slope $b_{i2}$ and exogenous noise $\epsilon_{it+1}$ are assumed to be normally distributed with mean zero and variance $\sigma_{b0}^2 = 4$, $\sigma_{b2}^2 = 1$ and $\sigma_\epsilon^2 = 1$, respectively.

The covariate is generated as:

$$
X_{it} = 
\begin{cases} 
b_{i0} + \epsilon_{X_{it}} & \text{if } t = 1, \\
b_{i0} + Y_{it} + \epsilon_{X_{it}} & \text{if } t \geq 2,
\end{cases}
\quad \text{where} \quad \epsilon_{X_{it}} \sim \mathcal{N}(0, 1)
$$

The treatment randomization probability is constant at $p_t = 0.5$, so $A_{it} \sim \text{Bernoulli}(0.5)$, implying that for every $i$ and $t$, the probability that treatment is assigned is equivalent to a fair coinflip. Relationships between variables are illustrated in @fig-GMG_path.

### Special Cases

We consider three special cases of GM-G, namely GM-A, GM-B and GM-C. The relation of each special case to GM-G is summarized in @tbl-gm-differences.

| Generative Model | Name in @qian2020 | dependency $b_{i0}$ and $X_{it}$ | random slope treatment $b_{i2}$ | interaction $\beta_1$ |
|---------------|---------------|---------------|---------------|---------------|
| G(eneral)        | 3                 | $\checkmark$                     | $\checkmark$                    | $\checkmark$          |
| A                | 1                 | $\times$                         | $\checkmark$                    | $\checkmark$          |
| B                | NA                | $\checkmark$                     | $\times$                        | $\checkmark$          |
| C                | NA                | $\checkmark$                     | $\checkmark$                    | $\times$              |

: Summary of Differences Between Generative Models {#tbl-gm-differences}

GM-A is a special case of GM-G, where the effect of $b_{i0}$ on $X_{it}$ is set to zero, which implies that $X_{it}$ is not directly determined by $b_{i0}$ (see @fig-GMA_path). Instead, $X_{it}$ is given by:

$$
X_{it} = 
\begin{cases} 
\epsilon_{X_{it}} & \text{if } t = 1, \\
Y_{it} + \epsilon_{X_{it}} & \text{if } t \geq 2,
\end{cases}
\quad \text{where} \quad \epsilon_{X_{it}} \sim \mathcal{N}(0, 1)
$$

GM-B is a special case of GM-G in which $b_{i2}$ was removed (see @fig-GMB_path) by setting $\sigma_{b2}^2 = 0$. While the within-person model is the same as GM-G, there is a slight alteration at the between-person level:

$$ \pi_{2i} = \beta_0. $$

The composite model then becomes:

$$
Y_{it+1} = (\alpha_0 + b_{i0}) + \alpha_1 X_{it} + \beta_0 A_{it} + \beta_1 A_{it} X_{it} + \epsilon_{it+1}
$$ {#eq-modelB}

GM-C is a special case of GM-G, where we set $\beta_1 = 0$, which implies the removal of $\beta_1 A_{it} X_{it}$ (see @fig-GMC_path) and $\pi_{3i}$. Therefore, the within-person model from GM-G changes into:

$$Y_{it+1} = \pi_{0i} + \pi_{1i} X_{it} + \pi_{2i} A_{it} + \epsilon_{it+1}.$$

The composite model becomes:

$$
Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + b_{i0} + A_{it} (\beta_0 + b_{i2}) + \epsilon_{it+1}.
$$ {#eq-modelC}

## Data Analysis

Data generation and estimation were performed in `R`, version 4.4.2 [@rcoreteam2024], following @qian2020's methodology for consistency. After data generation, analytical MLMs with restricted maximum likelihood estimation were fit using the `lme4` package [@bates2015]. The MLM from @eq-modelG was fit for GM-G and A, the MLM from @eq-modelB for GM-B, and the MLM from @eq-modelC for GM-C[^2].

[^2]: Contrary to the data generating models, the analytical models do not model the covariate $X_{it}$ and the treatment $A_{it}$.

In the simulation study, we evaluated the bias of the analytical models across different settings by systematically varying 3 factors: Generative Model ($\text{GM} = \text{G, A, B, C}$); number of timepoints ($T = 10, 30$); and sample size ($N = 30, 100, 200$). By varying these factors, 24 unique settings were created, each replicated 10,000 times. Bias estimates were calculated as the difference between the mean of the estimated parameter values $\bar{{\beta}}_{0}$ (across replications) and the true treatment effect $\beta_{0} = 1$.

# Results

This section begins by formulating predictions about treatment effect bias for each GM based on the conditional independence assumption and the backdoor criterion. These predictions are compared to simulation results, which present the estimated bias across the GMs.

## Conditional Independence and Path Diagrams

The *conditional independence assumption*, introduced by @qian2020 and based on @sitlani2012 evaluates whether treatment effect estimators are consistent and unbiased under randomized treatment assignment. This assumption states that the covariate at time $t$ ($X_{it}$) should be independent of the individual’s random effects (intercept $b_{i0}$ and slope(s) $b_{i1}$) once we account for their history of covariates up to timepoint $t-1$ ($H_{it-1}$), previous treatment ($A_{it-1}$), and prior outcome ($Y_{it}$):

$$
X_{it} \perp (b_{i0}, b_{i1}) \mid H_{it-1}, A_{it-1}, Y_{it}.
$$

This implies that $X_{it}$ can be influenced by prior observed variables but not directly by random effects. Verification of this assumption relies on domain knowledge [@qian2020]. To illustrate its application, we constructed path diagrams [@wright1934a] (see @fig-Pathdiagrams).

::: {#fig-Pathdiagrams layout-ncol="2"}
```{r}
#| label: fig-GMG_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM-G"
#| eval: true
#| out.width: "100%"

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, 2) {$X_{1i}$};
  \node (X1var) at (0, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, -2) {$A_{1i}$};
  \node[draw, rectangle] (A1X1) at (0.3, 0) {$A_{1i}\cdot X_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, 2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, -2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, 2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2X2) at (5.3, 0) {$A_{2i}\cdot X_{2i}$}; % Increased x-coordinate

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate
  \node[draw, circle, fill=gray!20] (b2) at (3, -4) {$b_{i2}$}; % Adjusted to align with middle row

  % Arrows with path coefficients
  \draw [->] (b0) -- (Y2);
  \draw [->] (b0) -- (Y3);
  \draw [->] (A2) edge node[above left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[above] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[above left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[above] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (A1X1) edge node[above] {$\beta_1$} (Y2);
  \draw [->] (A2X2) edge node[above left] {$\beta_1$} (Y3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);
  \draw [->] (b0) edge (X1);
  \draw [->] (b0) edge (X2);
  \draw [->] (b0) edge (X3);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);
  \draw [<->, out=315, in=225, looseness=2] (b2) to node[below] {$\sigma^2_{b2}$} (b2);

  % Additional paths for random effects
  \draw[->, dashed] (b2) -- (1.5,-1); 
  \draw[->, dashed] (b2) -- (5,-1.2); 

\end{tikzpicture}
```

```{r}
#| label: fig-GMA_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM-A"
#| eval: true
#| out.width: "100%"

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, 2) {$X_{1i}$};
  \node (X1var) at (0, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, -2) {$A_{1i}$};
  \node[draw, rectangle] (A1X1) at (0.3, 0) {$A_{1i}\cdot X_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, 2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, -2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, 2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2X2) at (5.3, 0) {$A_{2i}\cdot X_{2i}$}; % Increased x-coordinate

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate
  \node[draw, circle, fill=gray!20] (b2) at (3, -4) {$b_{i2}$}; % Adjusted to align with middle row

  % Arrows with path coefficients
  \draw [->] (b0) -- (Y2);
  \draw [->] (b0) -- (Y3);
  \draw [->] (A2) edge node[above left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[above] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[above left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[above] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (A1X1) edge node[above] {$\beta_1$} (Y2);
  \draw [->] (A2X2) edge node[above left] {$\beta_1$} (Y3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);
  \draw [<->, out=315, in=225, looseness=2] (b2) to node[below] {$\sigma^2_{b2}$} (b2);

  % Additional paths for random effects
  \draw[->, dashed] (b2) -- (1.5,-1); 
  \draw[->, dashed] (b2) -- (5,-1.2); 

\end{tikzpicture}
```

```{r}
#| label: fig-GMB_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM-B"
#| eval: true
#| out.width: "100%"

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, 2) {$X_{1i}$};
  \node (X1var) at (0, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, -2) {$A_{1i}$};
  \node[draw, rectangle] (A1X1) at (0.3, 0) {$A_{1i}\cdot X_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, 2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, -2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, 2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2X2) at (5.3, 0) {$A_{2i}\cdot X_{2i}$}; % Increased x-coordinate

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate

  % Arrows with path coefficients
  \draw [->] (b0) -- (Y2);
  \draw [->] (b0) -- (Y3);
  \draw [->] (A2) edge node[above left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[above] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[above left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[above] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (A1X1) edge node[above] {$\beta_1$} (Y2);
  \draw [->] (A2X2) edge node[above left] {$\beta_1$} (Y3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);
  \draw [->] (b0) edge (X1);
  \draw [->] (b0) edge (X2);
  \draw [->] (b0) edge (X3);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);

\end{tikzpicture}
```

```{r}
#| label: fig-GMC_path
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM-C"
#| eval: true
#| out.width: "100%"

\begin{tikzpicture}
  % Nodes for observed variables (squares)
  \node[draw, rectangle] (X1) at (0, 2) {$X_{1i}$};
  \node (X1var) at (0, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A1) at (0, -2) {$A_{1i}$};
  \node[draw, rectangle] (Y2) at (3, 0) {$Y_{2i}$}; % Increased x-coordinate
  \node (Y2var) at (4, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X2) at (3, 2) {$X_{2i}$}; % Increased x-coordinate
  \node (X2var) at (3, 3) {$\sigma^2_{X}$};
  \node[draw, rectangle] (A2) at (3, -2) {$A_{2i}$}; % Increased x-coordinate
  \node[draw, rectangle] (Y3) at (8, 0) {$Y_{3i}$}; % Increased x-coordinate
  \node (Y3var) at (9, 0) {$\sigma^2_{\epsilon}$};
  \node[draw, rectangle] (X3) at (8, 2) {$X_{3i}$}; % Increased x-coordinate
  \node (X3var) at (8, 3) {$\sigma^2_{X}$};

  % Nodes for latent variables (circles)
  \node[draw, circle, fill=gray!20] (b0) at (6, 4) {$b_{i0}$}; % Increased x-coordinate
  \node[draw, circle, fill=gray!20] (b2) at (3, -4) {$b_{i2}$}; % Adjusted to align with middle row

  % Arrows with path coefficients
  \draw [->] (b0) -- (Y2);
  \draw [->] (b0) -- (Y3);
  \draw [->] (A2) edge node[above left] {$\beta_0$} (Y3);
  \draw [->] (X2) edge node[above] {$\alpha_1$} (Y3);
  \draw [->] (Y2) edge node[right] {$1$} (X2);
  \draw [->] (A1) edge node[above left] {$\beta_0$} (Y2);
  \draw [->] (X1) edge node[above] {$\alpha_1$} (Y2);
  \draw [->] (Y3) edge node[right] {$1$} (X3);
  \draw [->] (X1var) edge (X1);
  \draw [->] (X2var) edge (X2);
  \draw [->] (X3var) edge (X3);
  \draw [->] (Y2var) edge (Y2);
  \draw [->] (Y3var) edge (Y3);
  \draw [->] (b0) edge (X1);
  \draw [->] (b0) edge (X2);
  \draw [->] (b0) edge (X3);

  % Curved arrows for variances
  \draw [<->, out=45, in=135, looseness=2] (b0) to node[above] {$\sigma^2_{b0}$} (b0);
  \draw [<->, out=315, in=225, looseness=2] (b2) to node[below] {$\sigma^2_{b2}$} (b2);

  % Additional paths for random effects
  \draw[->, dashed] (b2) -- (1.5,-1); 
  \draw[->, dashed] (b2) -- (5,-1.2); 

\end{tikzpicture}
```

Path Diagrams for Generative Models G, A, B and C (t = 1, 2, 3)
:::

*Note.* Random effects are represented by grey circles, observed variables by squares and relationships across variables by arrows, where dashed lines are reserved for random slopes.

\vspace{2em}

In GM-G, the $X_{it}$ is directly influenced by $b_{i0}$. Consequently, conditioning on prior variables (e.g., $Y_{it}$), does not fully block the influence of the random effects, violating the assumption. This aligns with the bias identified by @qian2020. In GM-A, the direct link between $X_{it}$ and $b_{i0}$ is removed. Although an indirect link remains between $X_{it}$ and $b_{i0}$ through $Y_{it}$, conditioning on $Y_{it}$ "breaks the link" between $X_{it}$ and $b_{i0}$, satisfying the assumption. This aligns with the absence of bias found by @qian2020. For GM-B and GM-C, the direct link between $X_{it}$ and $b_{i0}$ remains, as in GM-G, violating the assumption and suggesting the presence of bias.

## Backdoor Criterion and DAGs

The *backdoor criterion* [@pearl1988] can help determine which variables to control for to avoid bias in treatment effect estimates. To detect backdoor paths, DAGs [@pearl1995] are invaluable tools[^3]. Unlike linear path diagrams [@wright1934a] and structural equation models, DAGs are non-parametric. They encode causal assumptions about the data-generating process, with arrows representing direct causal effects that may exhibit heterogeneity across individuals or depend on other variables (effect interaction/modification) [@elwert2014]. Consequently, DAGs do not explicitly represent random slopes and interaction effects, which precluded their use for evaluating the conditional independence assumption.

[^3]: An accessible introduction into DAGs and backdoor paths can be found in @rohrer2018.

The DAGs for each GM (shown in @fig-DAGs) confirm the absence of random slopes and fixed interaction effects, explaining why GM-G, GM-B, and GM-C are equivalent.

::: {#fig-DAGs layout-ncol="2"}
```{r}
#| label: fig-GMG_DAG
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM-G, GM-B, GM-C"
#| eval: true

\begin{tikzpicture}[node distance=1cm]
  % Nodes for observed variables
  \node (X1) at (0, 2) {$X_{1}$};
  \node (A1) at (0, -2) {$A_{1}$};
  \node (Y2) at (3, 0) {$Y_{2}$};
  \node (X2) at (3, 2) {$X_{2}$};
  \node (A2) at (3, -2) {$A_{2}$};
  \node (Y3) at (6, 0) {$Y_{3}$};
  \node (X3) at (6, 2) {$X_{3}$};
  \node (U) at (4.5, 3) {$U$};

  % Directed edges
  \draw [->] (U) -- (Y2);
  \draw [->] (U) -- (Y3);
  \draw [->] (U) -- (X1);
  \draw [->] (U) -- (X2);
  \draw [->] (U) -- (X3);
  \draw [->] (A1) -- (Y2);
  \draw [->] (X1) -- (Y2);
  \draw [->] (Y2) -- (X2);
  \draw [->] (A2) -- (Y3);
  \draw [->] (X2) -- (Y3);
  \draw [->] (Y3) -- (X3);
\end{tikzpicture}
```

```{r}
#| label: fig-GMA_DAG
#| engine: 'tikz'
#| echo: false
#| cache: true
#| fig-subcap: "GM-A"
#| eval: true

\begin{tikzpicture}[node distance=1cm]
  % Nodes for observed variables
  \node (X1) at (0, 2) {$X_{1}$};
  \node (A1) at (0, -2) {$A_{1}$};
  \node (Y2) at (3, 0) {$Y_{2}$};
  \node (X2) at (3, 2) {$X_{2}$};
  \node (A2) at (3, -2) {$A_{2}$};
  \node (Y3) at (6, 0) {$Y_{3}$};
  \node (X3) at (6, 2) {$X_{3}$};
  \node (U) at (4.5, 3) {$U$};

  % Directed edges
  \draw [->] (U) -- (Y2);
  \draw [->] (U) -- (Y3);
  \draw [->] (A2) -- (Y3);
  \draw [->] (X2) -- (Y3);
  \draw [->] (Y2) -- (X2);
  \draw [->] (A1) -- (Y2);
  \draw [->] (X1) -- (Y2);
  \draw [->] (Y3) -- (X3);
\end{tikzpicture}
```

DAGs for Generative Models G, A, B and C (t = 1, 2, 3)
:::

*Note.* The node $U$ represents the random disturbance $b_{0i}$ in the generative models.

\vspace{2em}

Application of the backdoor criterion begins by identifying all paths that connect $A_t$ to $Y_{t+1}$, excluding the direct treatment effect $A_t \to Y_{t+1}$ itself. Next, we determine whether these paths can transmit association by applying the d-separation rules[^4]. A path that contains an arrow pointing to the treatment $A_t$ is considered a *backdoor path*.

[^4]: For an accessible introduction to d-separation, see [@hayduk2003].

In the DAGs presented in @fig-DAGs, no paths connect $A_t$ to $Y_{t+1}$ except the treatment effect $A_t \to Y_{t+1}$ itself. Thus, $A_t$ has no parent nodes, which implies that no backdoor paths exist[^5]. Therefore, $X_t$ need not be controlled for to obtain an unbiased causal effect. However, including $X_t$ does not introduce bias as it lies outside the pathways connecting $A_t$ and $Y_{t+1}$[^6].

[^5]: Indeed, the random assignment of the treatment ensures, by design, the absence of backdoor paths.

[^6]: Inclusion of the covariate may, however, increase the power to detect the treatment effect.

## Simulation Study

@fig-simulation-results present the bias estimates in $\beta_0$ for each of the GMs[^7]. In this reproduction of @qian2020, we observed results consistent with the original study: GM-G exhibited bias ($0.023-0.056$ or $2.3\%-5.6\%$), while GM-A showed none ($\leq 0.002$ or $0.2\%$). These findings align with the conditional independence assumption but contradict the backdoor criterion, which predicted no bias for any GM. As shown in @fig-simulation-results, GM-G bias decreased with increasing $T$ but remained stable across varying $N$, indicating inconsistency (i.e., estimates do not converge to the true value as $N$ increases).

Similarl to GM-A, no discernible bias was observed for GM-B ($\leq 0.005$ or $0.5\%$) and GM-C ($\leq 0.003$ or $0.3\%$). These results align with the backdoor criterion, but contradict the conditional independence assumption, which predicted bias for both GMs. GM-B also exhibited much lower variability across simulations compared to the other GMs.

[^7]: The supplemental table with additional information can be found at the OSF repository: <https://osf.io/8xawt/?view_only=aad6a13b7a4a4d36aed76ed8aac584c4>

::: {#fig-simulation-results}
![](research-report_files/figure-pdf/fig-simulation-results-1.pdf)

Estimation bias for the fixed treatment effect $\beta_0$ of each generative model for different combinations of sample size $N$ and number of timepoints $T$ over 10,000 simulation replications
:::

*Note.* bias refers to the difference between the mean of the estimated parameter values $\bar{{\beta}}_{0}$ and the prespecified treatment effect $\beta_{0} = 1$.

\vspace{2em}

```{r}
#| label: fig-simulation-results
#| cache: true
#| eval: false
#| echo: false
#| fig-cap: "Estimation bias for the fixed treatment effect $\\beta_0$ of each generative model for different combinations of sample size $N$ and number of timepoints $T$ over 10,000 simulation replications"
#| warning: false
#| fig-height: 13
#| fig-width: 9
#| escape: false

runname <- "GM123ad-1000reps-researchreport-cleanscript_nrep10000" # set a runname

# set the number of simulations
nsim <- 10000

# simulation for Research Report
design <- expand.grid(sample_size = c(30, 100, 200), total_T = c(10, 30), dgm_type = c(1,2,3,"3a","3d"))

# make sure dgm_type is not a factor
design$dgm_type <- as.character(design$dgm_type)

# create dataframe to store results
design$mlm_beta_0_bias <- rep(list(numeric(1000)), nrow(design))

for (idesign in 1:nrow(design)) {
  
    results_list <- readRDS(here(paste0("simulation_results/", runname, "/", idesign, ".RDS")))
    
    dgm_type <- design$dgm_type[idesign]
    
    ### Restate the true values of the parameters ###
    
    beta_0_true <- 1 # was originallly 0.5 in the code but is 1 in the paper
    
    result_lmm <- results_list[grep("solution_lmm", names(results_list))]
    
    ### Extract the Results ###
    
    mlm_beta_0 <- sapply(result_lmm, function(l) l$coef["A", "Estimate"])
    design$mlm_beta_0_bias[[idesign]] <- mlm_beta_0 - beta_0_true
    
}

# remove dgm_type = 2
design <- design[design$dgm_type != 2, ]

# rename dgm_type = 3 to "G", dgm_type = 1 to "A", dgm_type = 3a to "B" and dgm_type = 3d to "C"
design$dgm_type <- ifelse(design$dgm_type == 3, "G", 
                          ifelse(design$dgm_type == 1, "A", 
                                 ifelse(design$dgm_type == "3a", "B", 
                                        ifelse(design$dgm_type == "3d", "C", NA))))

# Combine datasets into one
all_data <- design %>%
  mutate(total_T = as.factor(total_T), 
         sample_size = as.factor(sample_size)) %>%
  unnest(mlm_beta_0_bias) %>%
  mutate(dgm_type = factor(dgm_type, levels = c("G", "A", "B", "C")))

# Turn N and T into strings with an underscore
all_data <- all_data %>%
  mutate(total_T2 = as.character(total_T),
         sample_size2 = as.character(sample_size)) %>%
  mutate(total_T2 = str_replace_all(total_T2, "10", "T = 10"),
         total_T2 = str_replace_all(total_T2, "30", "T = 30"),
         sample_size2 = str_replace_all(sample_size2, "30", "N = 30"),
         sample_size2 = str_replace_all(sample_size2, "100", "N = 100"),
         sample_size2 = str_replace_all(sample_size2, "200", "N = 200")) %>%
  # Set factor levels to ensure correct order in the plot
  mutate(
    total_T2 = factor(total_T2, levels = c("T = 10", "T = 30")),
    sample_size2 = factor(sample_size2, levels = c("N = 30", "N = 100", "N = 200"))
  )

# Create the matrix-style plot
ggplot(all_data, aes(x = dgm_type, y = mlm_beta_0_bias, col = dgm_type)) +
  geom_boxplot() +
  geom_hline(yintercept = 0, linetype = "dashed") +  # Dashed horizontal line at 0
  ylim(-1.0, 0.7) +  # Set y-axis limits
  labs(x = "Generative Model", y = "Bias") +
  facet_grid(sample_size2 ~ total_T2) +  # Show T and N values in labels
  theme(
      axis.text.x = element_text(size = 14, vjust = -0.8),  # Increase font size of x-axis category labels
      axis.text.y = element_text(size = 14),
      axis.title.x = element_text(size = 16, vjust = -1.4),  # Increase font size of x-axis label
      axis.title.y = element_text(size = 16),  # Increase font size of y-axis label
      legend.position = "none",   # Remove legend
      panel.border = element_rect(colour = "black", fill = NA, size = 1),  # Add black border
      plot.margin = margin(5, 5, 12, 5),  # Add space around the plot
      panel.background = element_rect(fill = "white"),  # White background instead of light grey
      panel.grid.major = element_blank(),  # Remove major gridlines
      panel.grid.minor = element_blank(),  # Remove minor gridlines
      axis.line = element_line(colour = "black"),  # Black axis lines
      axis.ticks = element_line(size = 1, color = "black"), # Black ticks
      strip.text.x = element_text(face = "bold", color = "white", size = 15),
      strip.text.y = element_text(face = "bold", color = "white", size = 15),
      strip.background.x = element_rect(fill = "dodgerblue3", linetype = "solid",
                                          color = "black", linewidth = 1),
      strip.background.y = element_rect(fill = "firebrick2", linetype = "solid",
                                          color = "gray30", linewidth = 1)
  )
```

In summary, if the GM either does not include the direct dependency of the random intercept on the covariate (GM-A), the random slope $b_{i2}$ (GM-B), or the interaction term $\beta_1$ (GM-C), as in GM-G, bias disappears. However, neither the backdoor criterion nor the conditional independence assumption consistently predicted treatment effect bias across models.

# Discussion

In this report, we evaluated several GMs to investigate when endogenous time-varying covariates bias treatment effect estimates in MLMs under randomized treatment; and assessed the ability of the conditional independence assumption and backdoor criterion to predict when this bias will occur. We found biased and inconsistent estimates in the most general model (GM-G) but not in three special cases: GM-A (no direct effect of the random intercept on the covariate), GM-B (no random slope for treatment), and GM-C (no interaction between treatment and covariate). The conditional independence assumption correctly predicted bias in GM-G and GM-A, but not in GM-B and GM-C. The backdoor criterion predicted no bias in any of the GMs, contradicting the bias observed in GM-G. These findings suggest that the dependency between the covariate and treatment, the random slope for treatment, and the interaction effect are all essential for bias to occur.

This raises two questions: why was no discernible bias observed in GM-B and GM-C, as predicted by the conditional independence assumption, and can this pattern generalize beyond the current GMs? One possibility is that the interaction and random slope, both of which drive heterogeneity in the treatment effect, may need to coexist for bias to manifest. This aligns with @qian2020’s assertion that “applying linear mixed models is problematic because potential moderators of the treatment effect are frequently endogenous” (p. 375). If treatment effect moderation by a covariate is indeed a prerequisite for bias, it could explain the lack of bias in GM-C (no interaction). However, this does not account for the absence of bias in GM-B (no random slope). As the current study only tested a specific set of GMs, further research is needed to assess whether these findings generalize, providing practical recommendations for using MLMs with endogenous time-varying covariates.

Regarding the backdoor criterion and DAGs [@pearl1988], our results suggest standard non-parametric DAGs may be insufficient to identify bias in GM-G, since they do not explicitly represent random slopes and interaction effects[^8]. Similar concerns about using DAGs in situations involving interaction effects have been raised [@weinberg2007; @attia2022]. Future research could explore how extended DAG frameworks, that explicitly incorporate interaction effects, may allow the backdoor criterion to identify bias.

[^8]: Note that the term "effect modification", while often used interchangeably with "interaction", has a distinct definition in the counterfactual framework [@vanderweele2009].

Another avenue for future investigation is the role of centering approaches[^9]. According to @antonakis2021, the assumption of uncorrelated between random effects and level 1 covariates can be relaxed using Mundlak's contextual model[^10] [@mundlak1978], which adds cluster means of each covariate as predictors of the random intercept. This approach, which explicitly models the source of endogeneity as suggested by Bell and Jones [-@bell2015], could shed light on the treatment effect bias present in GM-G but absent in the other GMs.

[^9]: An overview of centering approaches and their effects on interpretation in the MLM can be found in Hamaker and Muthén [-@hamaker2020].

[^10]: This is referred to as the Correlated Random Effects (CRE) approach by @wooldridge2002.

Finally, @qian2020 assumed independence between the random intercept and slope and perfect correlation between the random intercepts of the outcome and covariate. Since these assumptions may not hold in practice, investigating scenarios with different assumptions on the random effects could yield further insights.

<!-- # Conclusion -->

<!-- Dating all the way back to the work of Pepe and Anderson [-@pepe1994] it has been known that the endogeneity of time-varying covariates can result in biased parameter estimates. This research report is a first step towards understanding the work of @qian2020, who illustrated that this issue can even affect MLMs with randomized treatment. To recognize and understand completely when and why endogenous covariates may trouble an empirical investigation, further research is needed. -->

\newpage

# References

::: {#refs}
:::

<!-- --- -->

<!-- nocite: | -->

<!--   @* -->

<!-- --- -->
