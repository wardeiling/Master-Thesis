---
title: "DGMs of Qian et al. (2020) - Part 1: Without Treatment"
author: "Ward B. Eiling"
date: "2024-10-24"
date-modified: last-modified
format: 
  html:
    toc: true
    toc-location: left
    toc-depth: 3
    toc-expand: 3
    number-sections: true
    # embed-resources: false
    # default-image-extension: svg
  # pdf:
  #   fig-pos: 'h'
  #   toc: true
  #   toc-depth: 5
  #   number-sections: true
  #   # embed-resources: true
  #   # default-image-extension: svg
execute:
  message: false
  warning: false
fig-cap-location: top
---

```{r}
#| label: Packages
#| echo: false

# for general data manipulation and plotting
library(tidyverse)
# for estimation
library(lme4)
library(gee)
library(geex)
library(geepack)
library(nlme)
# for presentation of results
library(jtools)
# for diagrams
library(dagitty)
library(ggdag)
library(DiagrammeR)
library(magick)
library(pdftools)
```

\newpage

## Introduction

In dit document zal ik de data-genererende modellen gepresenteerd door Qian et al. (2020) opnieuw creëren, vergezeld door visuele weergaven van deze modellen. Laten we eerst enkele hoofdpunten van GEE samenvatten.

Parameters in marginale modellen (bijv. GEE) worden geïnterpreteerd als populatieniveau-effecten (en dus niet als individuele effecten!)

<!-- > That is, $\beta_j$ is the impact of variate j on average, across the whole population (assuming other variates fixed: Winter 437) -->

-   Dat wil zeggen, $\beta_j$ is het effect van variant j gemiddeld over de hele populatie (ervan uitgaande dat andere varianten vastliggen: Winter 437).

-   Voorbeeld: Hoe beïnvloedt veroudering de neiging om dementie te ontwikkelen in de gehele populatie?

Merk op dat de lineaire structuur van de GEE impliciet aanneemt dat $Y_{it} \perp X_{ik} \mid X_{it}$, voor alle $t \neq k$

-   Dit bekent: De uitkomst op tijdstip $t$ is conditioneel onafhankelijk van verklarende factoren op alle andere tijdstippen $k$, gegeven de verklarende factoren op het huidige tijdstip $t$

-   Dit wordt de "full covariate conditional mean (FCCM)" aanname genoemd door Diggle et al. (2002) en Qian et al. (2020). Maar zij herschrijven dit net iets anders (equation 5):

    $$
    E[Y_{it+1} \mid X_{it}] = E[Y_{it+1} \mid X_{i1}, \ldots, X_{iT}]
    $$

Deze aanname geldt bij (a) tijdsinvariante covariaten en (b) deterministische/exogene tijdsafhankelijke covariaten, maar niet bij stochastische/endogene tijdsafhankelijke covariaten!

> Een uitzondering op deze regel is wanneer we een "diagonale weight matrix" gebruiken in de GEE (i.e., working independence).

Laten we zien hoe deze aanname geschonden kan worden:

![Example from STAT 437](figure/STAT437-example.png){#fig-STAT437-example}

Zoals geïllustreerd door de rode pijl, is $Y1$ niet onafhankelijk van $X2$, gegeven (dat we de waarde weten van) $X1$. Als we dus geinteresserd zijn in het marginale effect van $X2$ op $Y2$, dan is in dit geval de uitkomst biased door een schending van de FCCM aanname:

$$
E[Y_{i2} \mid X_{i2}] \neq E[Y_{i2} \mid X_{i1},X_{i2}]
$$

<!-- probleem: het wordt moeilijk te bepalen hoeveel van het waargenomen effect een verborgen effect is van de uitkomst die de toekomstige covariaat beïnvloedt. -->

Vanuit een causaal perspectief kunnen we ook zeggen dat—in de bovenstaande DAG—$Y1$ een confounder is in de relatie tussen $X2$ en $Y2$. Als we hiervoor niet controleren is het effect van $X2$ op $Y2$ een ander effect (door deze invloed van $Y1$) dan het effect van $X1$ op $Y1$, wat wel aan de FCCM aanname voldoet.

## Situations Without Treatment

### Example in Section 2.2 of Qian et al. (2020)

Beschouw als concreet voorbeeld het geval waarin elk individu op 2 tijdspunten wordt geobserveerd ($T_i = 2$), en de covariaat op het tweede tijdspunt de lag-1 uitkomst is: $X_{i2} = Y_{i2}$.

> Door de uitkomst te laggen, hebben we in wezen drie tijdspunten: $X_{i1}$, $X_{i2} = Y_{i2}$, en $Y_{i3}$.

Stel dat de variabelen worden gegenereerd volgens het volgende multilevel lineaire model (MLM) met een random intercept:

$$
b_i \sim N(0, \sigma_u^2),
$$

$$
X_{i1} \sim N(0, \sigma_{X_1}^2) \text{ independently of } b_i,
$$

$$
Y_{i2} \mid X_{i1}, b_i \sim N(\beta_0 + \beta_1 X_{i1} + b_i, \sigma_\epsilon^2),
$$

$$
X_{i2} = Y_{i2},
$$

$$
Y_{i3} \mid X_{i1}, Y_{i2}, X_{i2}, b_i \sim N(\beta_0 + \beta_1 X_{i2} + b_i, \sigma_\epsilon^2).
$$

#### Translating the notation

In de tabel hieronder geven we de vertaling van de originele notatie in Qian et al. (2020) naar notatie die gebruikelijker is in psychologisch onderzoek.

| Parameter                       | Original            | New           |
|---------------------------------|---------------------|---------------|
| Fixed intercept                 | $\alpha_0$          | $\gamma_{00}$ |
| Fixed slope for $X_{it}$        | $\beta_1$           | $\beta_1$     |
| Random intercept                | $b_{i0}$            | $u_{0i}$      |
| Residual variance of Error term | $\sigma_\epsilon^2$ | $\sigma_e^2$  |
| Covariate                       | $X_{it}$            | $Z_{it}$      |

Laten we nu het model in deze notatie herschrijven:

$$
u_{0i} \sim N(0, \sigma_u^2),
$$

$$
Z_{i1} \sim N(0, \sigma_{Z_1}^2) \text{ independently of } u_{0i},
$$

$$
Y_{i2} \mid Z_{i1}, u_{0i} \sim N(\gamma_{00} + \beta_1 Z_{i1} + u_{0i}, \sigma_e^2),
$$

$$
Z_{i2} = Y_{i2},
$$

$$
Y_{i3} \mid Z_{i1}, Y_{i2}, Z_{i2}, u_{0i} \sim N(\gamma_{00} + \beta_1 Z_{i2} + u_{0i}, \sigma_e^2).
$$

#### Visualizing the Model

We zijn geinteresseerd in het effect van $Z_{it}$ ($= Y_{it}$) op $Y_{i(t+1)}$.

Laten we eerst de DAG tekenen (see @fig-DAG). In dit specifieke geval wordt de endogene tijdsvariërende covariaat veroorzaakt door de vorige uitkomst en beïnvloedt het de toekomstige uitkomst.

```{r}
#| label: fig-DAG
#| cache: true
#| fig-cap: "DAG of Model Without Treatment"
#| echo: false

# alternatively, we can include the random intercept
section2.2_DAG_randomeff <- dagitty(
  'dag {
bb="0,0,1,1"
Z_1 [exposure,pos="0.182,0.160"]
Z_2 [exposure,pos="0.454,0.160"]
Y_2 [outcome,pos="0.458,0.282"]
Y_3 [outcome,pos="0.732,0.283"]
u_0 [latent,pos="0.621,0.399"]
Z_1 -> Y_2
Z_2 -> Y_3
Y_2 -> Z_2
u_0 -> Y_2
u_0 -> Y_3
}
')

ggdag::ggdag_status(section2.2_DAG_randomeff) + ggdag::theme_dag()

# ggdag::ggdag_adjustment_set(section2.2_DAG_randomeff, text_size = 5) +
#   ggdag::theme_dag() +
#   labs(title = 'Adjustment sets', col='Adjusted', shape='Adjusted') +
#   theme(legend.position='bottom')
```

Merk op dat er een open biasing path bestaat van $Y_2$ naar $Z_2$ in de DAG: de predictor/covariaat $Z_2$ wordt *veroorzaakt door* (in dit geval equivalent aan) de eerdere uitkomst $Y_2$—en is dus een tijdsvariërende, *endogene* covariaat. Daarom gaat de aanname van de "full covariate conditional mean (FCCM)" niet op:

$$
E[Y_{it+1} \mid Z_{it}] = E[Y_{it+1} \mid Z_{i1}, \ldots, Z_{iT}]
$$

aangezien

$$
E[Y_{i3} \mid Z_{i2}] \neq E[Y_{i3} \mid Z_{i1}, \ldots, Z_{i2}]
$$

Oftewel, de verwachte waarde van $Y_i3$ gegeven $Z_{i2}$ is niet gelijk aan de verwachte waarde van $Y_{i3}$ gegeven $Z_{i1}$ en $Z_{i2}$. Anders gezegd, de uitkomst $Y_3$ is niet conditioneel onafhankelijk van de uitleggende factoren/covariaten op alle andere tijdspunten (i.e., $Z_1$), gegeven dat we de waarden kennen van de covariaat $Z_2$ op het huidige tijdstip $t=2$.

Het in rekening brengen van afhankelijkheden tussen observaties in de uitkomst (i.e., het gebruik van "non-diagonal covariance weighting", zie Diggle et al. (2002))---door middel van (a) working correlation matrices in GEE (behalve onafhankelijkheid) of (b) door de random intercept in het multilevel lineaire model---leid tot conditionele schattingen op basis van de random effecten, en niet tot schattingen op populatieniveau. Daarentegen schatten de GEE met working independence en GLM het model gewoon door onafhankelijkheid aan te nemen, waardoor de schattingen op populatieniveau wel valide zijn.
<!-- Daarnaast is $u_0$ een confounder in de relatie tussen $Y_2$ en $Y_3$, en daarmee ook in de relatie tussen $Z_2$ en $Y_3$, aangezien $Y_2 = Z_2$.  -->

Wat gebeurt hier precies?

1.    In de schatting d.m.v. de MLM is $u_0$ een confounder in de relatie tussen $Y_2$ en $Y_3$, en daarmee ook in de relatie tussen $Z_2$ en $Y_3$ (aangezien $Y_2 = Z_2$). Dus de random intercept beinvloedt de covariate $Z_2$ en daarmee ook de uitkomst $Y_3$.

> "$Z_{it}$ being endogenous means it may depend on previous outcomes, which in turn implies dependence on the random effect $u_{0i}$. Thus, $E(u_{0i} \mid Z_{it})$ is usually nonzero and the conditional model may no longer imply the marginal model" (Qian et al., 2020)

<!-- In andere woorden, de random intercept wordt geschat op basis van alle beschikbare tijdspunten (geschiedenis en toekomst). Door het open biasing/backdoor pad van $Y_2$ naar $Z_2$ word, wanneer we het fixed effect $\beta_1$ schatten in de relatie tussen $Z_2$ en $Y_3$, de marginale schatting vertekend doordat informatie van de relatie tussen $Z_1$ en $Y_2$ wordt meegenomen. -->

2.    In de GEE met working independence en GLM wordt de relatie tussen $Z_2$ en $Y_3$ geschat op basis van de aanname van onafhankelijkheid, en is er dus geen confounder $u_0$ in de relatie tussen $Z_2$ en $Y_3$.

> Daardoor is $E(u_{0i} \mid Z_{it}) = 0$ en de marginale schatting van $\beta_1$ in de relatie tussen $Z_2$ en $Y_3$ is onvertekend.

<!-- ```{r, engine = 'tikz'} -->
<!-- #| label: fig-pathdiagram.section2.2 -->
<!-- #| cache: true -->
<!-- #| echo: false -->
<!-- #| fig-cap: "Path Diagram of Model Without Treatment" -->

<!-- \begin{tikzpicture} -->
<!--   % Nodes -->
<!--   \node[draw] (Z1) at (1.5, 1.5) {$Z_1$}; -->
<!--   \node[draw] (Z2) at (6.0, 1.5) {$Z_2$}; -->
<!--   \node[draw] (Y2) at (6.0, 3.5) {$Y_2$}; -->
<!--   \node[draw] (Y3) at (10.0, 3.5) {$Y_3$}; -->
<!--   \node[draw, circle, fill=gray!20] (u0) at (8.0, 5.5) {$u_{0i}$}; -->
<!--   \node[draw, circle, fill=gray!20] (eY2) at (7.5, 3.5) {$e_{i2}$}; -->
<!--   \node[draw, circle, fill=gray!20] (eY3) at (11.5, 3.5) {$e_{i3}$}; -->

<!--   % Edges with labels -->
<!--   \draw[->] (Z1) -- (Y2) node[midway, below right] {$\beta_{1}$}; -->
<!--   \draw[->] (Z2) -- (Y3) node[midway, below right] {$\beta_{1}$}; -->
<!--   \draw[->] (Y2) -- (Z2) node[midway, left] {$=$}; -->
<!--   \draw[->] (u0) -- (Y2) node[midway, left] {$+$}; -->
<!--   \draw[->] (u0) -- (Y3) node[midway, left] {$+$}; -->
<!--   \draw[->] (eY2) -- (Y2) ; -->
<!--   \draw[->] (eY3) -- (Y3) ; -->

<!--   % Double-headed arrows for residual variances -->
<!--   \draw[<->, out=120, in=60, looseness=1.5] (eY2) to node[above] {$\sigma_e^2$} (eY2); -->
<!--   \draw[<->, out=120, in=60, looseness=1.5] (eY3) to node[above] {$\sigma_e^2$} (eY3); -->
<!--   \draw[<->, out=120, in=60, looseness=1.5] (Z1) to node[above] {$\sigma_{Z_1}^2$} (Z1); -->

<!-- \end{tikzpicture} -->
<!-- ``` -->

![Path Diagram of Model Without Treatment](figure/fig-pathdiagram.section2.2-1.png){#fig-pathdiagram.section2.2}

In @fig-pathdiagram.section2.2 wordt duidelijk dat de marginale relatie tussen $Z_1$ en $Y_2$ eenvoudigweg is:

$$
E[Y_{i2} \mid Z_{i1}] = \gamma_{00} + \beta_1 Z_{i1}
$$

De relatie tussen $Z_2$ en $Y_3$ is complexer door de invloed van de random intercept $u_{0i}$

$$
E[Y_{i3} \mid Z_{i2}] = (1- \rho \zeta - \rho ) \gamma_{00} + [(1-\rho \zeta)  \beta_1 + \rho] Z_{i2}
$$

Specifieker gezegd, de interpretatie van de slope $\beta_1$ als effect op populatieniveau is alleen valide voor het effect van $Z_1$ op $Y_2$. Daarom kunnen we wel zeggen dat

> $\beta_1$ het effect is van variabele $Z_1$ gemiddeld op $Y_2$, over de gehele populatie (onder voorwaarde van constante waarden van andere variabelen)

maar ditzelfde kunnen we niet zeggen voor het effect van $Z_2$ op $Y_3$.

De relatie tussen $Z_2$ en $Y_3$ wordt verstoord door het niet-geobserveerde random effect $u_{0i}$, dat ook geassocieerd is met $Y_2$.

#### Data Generation and Estimation

Laten we nu de data genereren volgens dit model en het model schatten met een multilevel lineair model (MLM) en een generalized estimating equation (GEE)-model. Hier zijn we geïnteresseerd in het schatten van het effect van $Z_{it}$ op $Y_{i(t+1)}$. Voor deze analyse in long-format werd daarom een lagged variabele gebruikt voor $Z_{it}$, zodat $Y_{i2}$ wordt regressed op $Z_{i1}$ en $Y_{i3}$ op $Z_{i2}$.

```{r}
#| label: Scenario2.2_DataGeneration
#| cache: true
#| eval: false
#| echo: false

# Create a function to generate the data
generate_data <- function(n_i = 5000, sigma_u = 1, sigma_Z1 = 1, sigma_e = 1, beta_1 = 0.8, gamma_00 = 2) {
  data_list <- list()
  
  # Simulate data for each individual
  for (i in 1:n_i) {
    u_0i <- rnorm(1, 0, sigma_u)
    Z_i2_lag <- Z_i1 <- rnorm(1, 0, sigma_Z1)
    Y_i2 <- rnorm(1, gamma_00 + beta_1 * Z_i1 + u_0i, sigma_e)
    Z_i3_lag <- Z_i2 <- Y_i2
    Y_i3 <- rnorm(1, gamma_00 + beta_1 * Z_i2 + u_0i, sigma_e)
    
    # Store the data in a list
    subject_data <- data.frame(
      id = i,
      time = 1:2,
      Y = c(Y_i2, Y_i3),
      Z_lag1 = c(Z_i2_lag, Z_i3_lag)
    )
    
    data_list[[i]] <- subject_data
  }
  
  # Combine all subjects' data into a single data frame
  data_long <- do.call(rbind, data_list)
  return(data_long)
}

# Create a function that runs the simulations
run_simulations <- function(n_sim, n_i, sigma_u, sigma_Z1, sigma_e, beta_1, gamma_00) {
  
  # Initialize a list to store results
  all_estimates <- list()
  
  # Simulation loop
  for (sim in 1:n_sim) {
    # Generate the data
    data_sim <- generate_data(n_i, sigma_u, sigma_Z1, sigma_e, beta_1, gamma_00)
    
    # Fit the models
    mlm_mle <- lmer(Y ~ Z_lag1 + (1 | id), data = data_sim, REML = FALSE)
    gee_exch <- gee(Y ~ Z_lag1, id = id, data = data_sim, family = gaussian, corstr = "exchangeable")
    gee_ind <- gee(Y ~ Z_lag1, id = id, data = data_sim, family = gaussian, corstr = "independence")
    gee_ar1 <- gee(Y ~ Z_lag1, id = id, data = data_sim, family = gaussian, corstr = "AR-M", Mv = 1)
    glm <- glm(Y ~ Z_lag1, data = data_sim, family = gaussian)
    # gls_symm <- nlme::gls(Y ~ Z_lag1, data = data_sim, correlation = corSymm(form = ~ 1 | id), method = "ML")
    
    # Extract the fixed effect estimates for each model
    estimates <- data.frame(
      sim = sim,
      MLM_mle_intercept = fixef(mlm_mle)[1],
      MLM_mle_slope = fixef(mlm_mle)[2],
      GEE_exch_intercept = coef(gee_exch)[1],
      GEE_exch_slope = coef(gee_exch)[2],
      GEE_ind_intercept = coef(gee_ind)[1],
      GEE_ind_slope = coef(gee_ind)[2],
      GEE_ar1_intercept = coef(gee_ar1)[1],
      GEE_ar1_slope = coef(gee_ar1)[2],
      GLM_intercept = coef(glm)[1],
      GLM_slope = coef(glm)[2] #,
      #GLS_symm_intercept = coef(gls_symm)[1],
      #GLS_symm_slope = coef(gls_symm)[2]
    )
    
    # Store the estimates in the list
    all_estimates[[sim]] <- estimates
  }
  
  # Combine results into a single data frame
  results <- do.call(rbind, all_estimates)
  return(results)
}

# Set the parameters for the simulation
n_sim <- 1  # Number of simulations
n_i <- 100000   # Number of individuals per simulation
sigma_u <- 1  # Variance of random intercept
sigma_Z1 <- 1 # Variance of Z1
sigma_e <- 1 # Residual variance
beta_1 <- 0.8  # Slope
gamma_00 <- 2  # Intercept

# Run the simulation and store the results
simulation_results <- run_simulations(n_sim, n_i, sigma_u, sigma_Z1, sigma_e, beta_1, gamma_00)

# Calculate mean and standard deviation of estimates
section2.2_summary_stats <- data.frame(
  row.names = c("Intercept", "Z_lag1"),
  MLM_mle = c(mean(simulation_results$MLM_mle_intercept), mean(simulation_results$MLM_mle_slope)),
  GEE_exch = c(mean(simulation_results$GEE_exch_intercept), mean(simulation_results$GEE_exch_slope)),
  GEE_ind = c(mean(simulation_results$GEE_ind_intercept), mean(simulation_results$GEE_ind_slope)),
  GEE_ar1 = c(mean(simulation_results$GEE_ar1_intercept), mean(simulation_results$GEE_ar1_slope)),
  GLM = c(mean(simulation_results$GLM_intercept), mean(simulation_results$GLM_slope)) #,
  #GLS_symm = c(mean(simulation_results$GLS_symm_intercept), mean(simulation_results$GLS_symm_slope))
)

saveRDS(section2.2_simulation_results, "output/section2.2_simulation_results_1_100000.rds")
saveRDS(section2.2_summary_stats, "output/section2.2_summary_stats_1_100000.rds")
# Test: If we run the code with n = 100.000 and u0 = 0, all models return the specified fixed effects.
```

We gebruiken de volgende specificatie

```{r}
#| label: section2.2-spec
#| echo: true
#| eval: false

# Set the parameters for the simulation
n_sim <- 1  # Number of simulations
n_i <- 100000   # Number of individuals per simulation
sigma_u <- 1  # Variance of random intercept
sigma_Z1 <- 1 # Variance of Z1
sigma_e <- 1 # Residual variance
beta_1 <- 0.8  # Slope
gamma_00 <- 2  # Intercept

# Simulate data for each individual
for (i in 1:n_i) {
  u_0i <- rnorm(1, 0, sigma_u)
  Z_i2_lag <- Z_i1 <- rnorm(1, 0, sigma_Z1)
  Y_i2 <- rnorm(1, gamma_00 + beta_1 * Z_i1 + u_0i, sigma_e)
  Z_i3_lag <- Z_i2 <- Y_i2
  Y_i3 <- rnorm(1, gamma_00 + beta_1 * Z_i2 + u_0i, sigma_e)
  
  # Store the data in a list
  subject_data <- data.frame(
    id = i,
    time = 1:2,
    Y = c(Y_i2, Y_i3),
    Z_lag1 = c(Z_i2_lag, Z_i3_lag)
  )
  
  data_list[[i]] <- subject_data
}

# Fit the models
mlm_mle <- lmer(Y ~ Z_lag1 + (1 | id), data = data_sim, REML = FALSE)
gee_exch <- gee(Y ~ Z_lag1, id = id, data = data_sim, family = gaussian, corstr = "exchangeable")
```


```{r}
#| label: tbl-section2.2output
#| tbl-cap: "Fixed effects (N = 1.000.000, nsim = 1)"
#| echo: false

section2.2_summary_stats <- readRDS("output/section2.2_summary_stats_1_100000.rds")
knitr::kable(section2.2_summary_stats, digits = 2)
```

We can clearly see that the MLM and GEE models provide exactly the same estimates for the fixed intercept and fixed regression coefficient, with the exception of the GEE with independence working correlation structure.

> According to Pepe and Anderson (1994), this is the only structure that can avoid bias in the estimation of the fixed effects (i.e., that has a valid marginal interpretation).

As a reminder, the fixed effects were specified as $\gamma_{00} = 2$ and $\beta_1 = 0.8$. Thus, we can see that all models except the GEE with independence working correlation structure returns estimates that are very close to the true values---which represented the conditional mean of $Y$ given $Z$ and $u_{0i}$ (rather than the marginal mean of $Y$ given $Z$).

To see why this makes sense, it is important to realize that the parameter estimates represent the parsimonious conditional relationship

$$
E[Y_{it+1} \mid Z_{it}, u_{0i}] = \gamma_{00} + \beta_1 Z_{it} + u_{0i}
$$

And not the marginal relationship, which according to Qian et al. (2020) is given by:

$$
E[Y_{i2} \mid Z_{i1}] = \gamma_{00} + \beta_1 Z_{i1}
$$

$$
E[Y_{i3} \mid Z_{i2}] = (1- \rho \zeta - \rho ) \gamma_{00} + [(1-\rho \zeta)  \beta_1 + \rho] Z_{i2}
$$

Let's confirm this by calculating the true marginal effect

```{r}
#| label: Marginal Effects

# Restate the parameters
sigma_u <- 1  # Variance of random intercept
sigma_Z1 <- 1 # Variance of Z1
sigma_e <- 1 # Residual variance
beta_1 <- 0.8  # Slope
gamma_00 <- 2  # Intercept

# prepare variances (instead of SD)
sigma2_u0 = sigma_u^2
sigma2_e = sigma_e^2
sigma2_Z1 = sigma_Z1^2

# Compute rho and zeta
rho = sigma2_u0 / (sigma2_u0 + sigma2_e)
zeta = (beta_1 * sigma2_Z1) / (beta_1 * sigma2_Z1 + sigma2_u0 + sigma2_e)

# Now let's compute the marginal effects
marginal_intercept_Z1_Y2 <- gamma_00
marginal_slope_Z1_Y2 <- beta_1
marginal_intercept_Z2_Y3 <- (1 - rho * zeta - rho) * gamma_00
marginal_slope_Z2_Y3 <- ((1 - rho * zeta) * beta_1 + rho)

(marginal_intercept_Z1_Y2 + marginal_intercept_Z2_Y3) / 2
(marginal_slope_Z1_Y2 + marginal_slope_Z2_Y3) / 2
```

Als we deze waarde voor de marginale slope vergelijken met estimatie van de GLM vinden we erg vergelijkbare waardes. Hetzelfde is niet waar voor de marginale intercept.

> Wat doen we fout? Hoe moet dit wel?

<!-- ### Intermezzo: What are marginal effects/models? -->

<!-- Marginal models are a class of models that are used to estimate the population average effect of a covariate on an outcome. This may be useful, for instance, when prediction or indeed complete modelling of the data are not the main goal of an analysis (Pepe and Anderson, 1994). -->

<!-- > "Consider, for example, the future practice of screening for risk of respiratory disease, where one might simply ascertain Vitamin A deficiency, weight, height and other covariates at a single time point and make a determination of the child's risk based on these measurements." (Pepe and Anderson, 1994) -->

<!-- Here, the cross-sectional model is of primary interest for use in future screening practices and an in-depth model of longitudinal data is of secondary interest (Pepe and Anderson, 1994). -->

<!-- This contrasts with psychological research, where the cross-sectional model is often deemed problematic in the context of longitudinal data analysis, because it conflates (rather than separates) within-subject and between-subject effects. Instead, we tend to be much more interested in (1) the model that best describes the data (i.e., has the best model fit) and (2) the "why" question: complete model of the effects (including within- and between-person effects). What differs here is the aim of the study. -->

<!-- In what situations may psychological researchers be primarily interested in marginal effects over finding the best description of the data? -->

<!-- 1.  Clinical Psychology - Making a diagnosis based on measurements of covariates at a single time point, so that patients can be identified and helped who are at risk of developing a mental disorder. -->

<!-- 2.  Educational Psychology - Making predictions of course performance based on covariates (e.g., hours studying on average) at a single time point, so that students can be identified and helped who are at risk of failing a course. -->

<!-- Whether marginal or conditional models are preferred depends simply upon the research question and aim of the study: -->

<!-- > "We do not suggest that marginal models are preferable in general to conditional models. In Section 1 we provided one example where the marginal model is, in fact, preferable but in many cases it will not be. Indeed, which model should be used depends entirely on the questions to be addressed with the data. If a good description of the process generating the data is required then fully conditional or random effects models might be pursued." (Pepe and Anderson, 1994) -->

<!-- ### Intermezzo: What is the difference between REML and MLE? -->

<!-- When fitting a multilevel linear model (MLM) we can choose between restricted maximum likelihood (REML) and maximum likelihood estimation (MLE). In REML, $\sigma^2$ and $\rho$ (intra class correlation) are essentially considered nuisance parameters, which makes sure that small sample bias is reduced. However, since we do not obtain the complete log likelihood, we cannot compare models using the likelihood ratio test. When sample sizes are sufficiently large, the two methods are asymptotically equivalent. -->

<!-- the variance components $\sigma^2_u$ are estimated by maximizing the likelihood of the residuals, conditional on the fixed effects. In MLE, the residual variance $\sigma^2$ and the variance components $\sigma^2_u$ are estimated by maximizing the likelihood of the residuals, conditional on the fixed effects and the random effects. -->

<!-- The difference between the two methods lies in the way they estimate the variance components of the model. -->

<!-- source: [STAT 437: 007. Linear Marginal Models: Likelihood, Inference, and Asymptotics (Theory)](https://www.youtube.com/watch?v=ZjQo0H_FFwI&list=PL0Y1Z_F8RtX7jIbexsd3lizDXMET_WGX0&index=7) -->

### Example in Diggle et al. (2002)

In "Longitudinal Data Analysis" by Diggle et al. (2002), there is another example, where there is a time-varying endogenous covariate $X_t$ which is not caused by previous outcomes $Y_{t-1}$, but instead is caused by previous values of covariate $X_{t-1}$, which simultaneously affects future outcomes $Y_{t+1}$.

The following data generating mechanism was used:

$$Y_{it} = \gamma_0 + \gamma_1 X_{it} + \gamma_2 X_{it-1} + b_i + e_{it} \quad \text{where} \quad b_i \sim N(0,1), \quad e_{it} \sim N(0,1),$$

$$
X_{it} = \rho X_{it-1} + \epsilon_{it} \quad \text{where} \quad \epsilon_{it} \sim N(0,1).
$$

where $b_i$, $e_{it}$, and $\epsilon_{it}$ are mutually independent.

This data generating mechanism has the following DAG

```{r}
#| label: fig-Diggle2002_DAG
#| echo: false
#| cache: true
#| fig-cap: "DAG for Diggle (2002)"

Diggle2002_DAG <- dagitty('dag {
bb="0,0,1,1"
"X_t+1" [exposure,pos="0.633,0.344"]
"X_t-1" [exposure,pos="0.290,0.343"]
"Y_t+1" [outcome,pos="0.627,0.241"]
X_t [exposure,pos="0.439,0.344"]
Y_t [outcome,pos="0.437,0.246"]
b_i [latent,pos="0.537,0.156"]
"X_t+1" -> "Y_t+1"
"X_t-1" -> X_t
"X_t-1" -> Y_t
X_t -> "X_t+1"
X_t -> "Y_t+1"
X_t -> Y_t
b_i -> "Y_t+1"
b_i -> Y_t
}')

ggdag::ggdag_status(Diggle2002_DAG) + theme_dag()

# ggdag::ggdag_adjustment_set(Diggle2002_DAG, text_size = 5) +
#   ggdag::theme_dag() +
#   labs(title = 'Adjustment sets', col='Adjusted', shape='Adjusted') +
#   theme(legend.position='bottom')
```

In this DAG, we can see that $X_{t-1}$ is a confounder of the relationship between $Y_{t}$ and $X_{t}$. However, of course, this variable will be controlled for by including a lagged version of $X$ in the model.

Alternatively, we can also use a path diagram to illustrate more clearly the types of dependencies and relationships present in the data generating model.

<!-- ```{r, engine = 'tikz'} -->
<!-- #| label: fig-Diggle2002_PathDiagram -->
<!-- #| echo: false -->
<!-- #| cache: true -->
<!-- #| fig-cap: "Path diagram for Diggle (2002)" -->

<!-- \begin{tikzpicture} -->

<!--   % Nodes -->
<!--   \node[draw] (Xt_1) at (0, 1.5) {$X_{t-1}$}; -->
<!--   \node[draw] (Xt) at (3, 1.5) {$X_t$}; -->
<!--   \node[draw] (Xt1) at (7, 1.5) {$X_{t+1}$}; -->
<!--   \node[draw] (Yt) at (3, 3.5) {$Y_t$}; -->
<!--   \node[draw] (Yt1) at (7, 3.5) {$Y_{t+1}$}; -->
<!--   \node[draw, circle, fill=gray!20] (b_i) at (5, 5.5) {$b_i$}; -->

<!--   % Residuals for Y and X -->
<!--   \node[draw, circle, fill=gray!20] (eYt) at (5, 3.5) {$e_{it}$}; -->
<!--   \node[draw, circle, fill=gray!20] (eYt1) at (9.5, 3.5) {$e_{it}$}; -->
<!--   \node[draw, circle, fill=gray!20] (eXt) at (3, 0) {$\epsilon_{it}$}; -->
<!--   \node[draw, circle, fill=gray!20] (eXt1) at (7, 0) {$\epsilon_{it}$}; -->
<!--   \node[draw, circle, fill=gray!20] (eXt1_1) at (0, 0) {$\epsilon_{it}$}; -->

<!--   % Edges with labels -->
<!--   \draw[->] (Xt_1) -- (Xt) node[midway, above] {$\rho$}; -->
<!--   \draw[->] (Xt) -- (Xt1) node[midway, above] {$\rho$}; -->
<!--   \draw[->] (Xt_1) -- (Yt) node[midway, above left] {$\gamma_2$}; -->
<!--   \draw[->] (Xt) -- (Yt) node[midway, left] {$\gamma_1$}; -->
<!--   \draw[->] (Xt) -- (Yt1) node[midway, above left] {$\gamma_2$}; -->
<!--   \draw[->] (Xt1) -- (Yt1) node[midway, left] {$\gamma_1$}; -->
<!--   \draw[->] (b_i) -- (Yt) node[midway, right] {$+$}; -->
<!--   \draw[->] (b_i) -- (Yt1) node[midway, right] {$+$}; -->

<!--   % Residual arrows for Y -->
<!--   \draw[<->, out=120, in=60, looseness=1.5] (eYt) to node[above] {$\sigma_e^2$} (eYt); -->
<!--   \draw[<->, out=120, in=60, looseness=1.5] (eYt1) to node[above] {$\sigma_e^2$} (eYt1); -->
<!--   \draw[<->, out=300, in=240, looseness=1.5] (eXt1) to node[below] {$\sigma_\epsilon^2$} (eXt1); -->
<!--   \draw[<->, out=300, in=240, looseness=1.5] (eXt) to node[below] {$\sigma_\epsilon^2$} (eXt); -->
<!--   \draw[<->, out=300, in=240, looseness=1.5] (eXt1_1) to node[below] {$\sigma_\epsilon^2$} (eXt1_1); -->

<!--   % Residual arrows for X -->
<!--   \draw[->] (eYt) -- (Yt); -->
<!--   \draw[->] (eYt1) -- (Yt1); -->
<!--   \draw[->] (eXt) -- (Xt); -->
<!--   \draw[->] (eXt1) -- (Xt1); -->
<!--   \draw[->] (eXt1_1) -- (Xt_1); -->

<!-- \end{tikzpicture} -->
<!-- ``` -->

![Path diagram for Diggle et al. (2002)](figure/fig-Diggle2002_PathDiagram-1.png){#fig-Diggle2002_PathDiagram}

For a range of correlations ($\rho = 0.9–0.1$) they simulated 100 data sets each of which contained data on $m = 200$ subjects with up to 10 observations per subject. The number of observations for each subject, $n_i$, was generated as a uniform random variable between 2 and 10.

> Because missing data did not help estimation, I changed this to 10 observations for each subject.

In this case, the true marginal relationship of the fixed slope between $X_{it}$ and $Y_{it}$ is given by

$$\beta_1 = \gamma_1 + \rho \cdot \gamma_2$$

We can do estimation in two ways:

1.  Using a *cross-sectional mean model* (i.e., marginal model): estimating the linear model $Y_{it} = \beta_0 + \beta_1 X_{it}$
2.  Using a *full conditional model*: estimating the linear model $Y_{it} = \gamma_0 + \gamma_1 X_{it} + \gamma_2 X_{it-1}$

We can estimate the marginal model and its parameter $\beta_1$ using GEE and MLM. In this case, however, the marginal model is biased under all conditions except the GEE with working independence correlation structure.

```{r}
#| label: Diggle2002_CSM
#| echo: true
#| cache: true
#| eval: false

# Load necessary libraries
library(geepack)

# Simulation parameters
set.seed(123)
n_subjects <- 100000 # Number of subjects
max_time <- 10    # Maximum number of observations per subject
n_sim <- 1     # Number of simulations
gamma_0 <- 0
gamma_1 <- 1
gamma_2 <- 1
rho_values <- seq(0.9, 0.1, by = -0.2) # Range of autocorrelations

# Compute true marginal means for the fixed slope
true_marginal_b1 <- data.frame(
  rho = rho_values,
  beta1 = gamma_1 + rho_values * gamma_2
)

# Function to generate data for a single simulation
generate_data <- function(rho) {
  data_list <- list()
  
  for (i in 1:n_subjects) {
    n_i <- 10 # or fixed number of observations
    X_it <- numeric(n_i)
    Y_it <- numeric(n_i)
    b_i <- rnorm(1, 0, 1)        # Random intercept
    e_it <- rnorm(n_i, 0, 1)
    
    # Generate covariate X_it with autoregressive structure
    X_it[1] <- rnorm(1, 0, 1)    # Initial value for X_it
    for (t in 2:n_i) {
      X_it[t] <- rho * X_it[t-1] + rnorm(1, 0, sqrt(1 - rho^2))
    }
    
    # Generate outcome Y_it based on current and lagged values of X_it
    Y_it[1] <- gamma_0 + gamma_1 * X_it[1] + b_i + e_it[1]
    if (n_i > 1) {
      for (t in 2:n_i) {
        Y_it[t] <- gamma_0 + gamma_1 * X_it[t] + gamma_2 * X_it[t-1] + b_i + e_it[t]
      }
    }
    
    # Store the data in a list
    subject_data <- data.frame(
      subject = i,
      time = 1:n_i,
      Y = Y_it,
      X = X_it
    )
    data_list[[i]] <- subject_data
  }
  
  # Combine all subjects' data into a single data frame
  data <- do.call(rbind, data_list)
  return(data)
}

# Simulation loop
results <- data.frame()
for (rho in rho_values) {
  beta1_estimates <- matrix(NA, n_sim, 4) # Store beta1 estimates for each correlation structure
  
  for (sim in 1:n_sim) {
    # Generate data for this simulation
    sim_data <- generate_data(rho)
    
    # Fit GEE models with different working correlation structures
    gee_ind <- geeglm(Y ~ X, id = subject, data = sim_data, corstr = "independence")
    gee_exch <- geeglm(Y ~ X, id = subject, data = sim_data, corstr = "exchangeable")
    gee_ar1 <- geeglm(Y ~ X, id = subject, data = sim_data, corstr = "ar1")
    mlm_mle <- lmer(Y ~ X + (1 | subject), data = sim_data, REML = FALSE)
    
    # Store the estimated beta1 for each model
    beta1_estimates[sim, 1] <- coef(gee_ind)["X"]
    beta1_estimates[sim, 2] <- coef(gee_exch)["X"]
    beta1_estimates[sim, 3] <- coef(gee_ar1)["X"]
    beta1_estimates[sim, 4] <- fixef(mlm_mle)["X"]
  }
  
  # Calculate average estimates of beta1 for each correlation structure
  mean_estimates <- colMeans(beta1_estimates, na.rm = TRUE)
  results <- rbind(results, data.frame(
    rho = rho,
    beta1_ind = mean_estimates[1],
    beta1_exch = mean_estimates[2],
    beta1_ar1 = mean_estimates[3],
    beta1_mlm = mean_estimates[4]
  ))
}

# format and save results
results_complete <- cbind(results, beta1_true = true_marginal_b1$beta1) %>%
  select(rho, beta1_true, beta1_ind, beta1_exch, beta1_ar1, beta1_mlm)

saveRDS(results_complete, file = "output/Diggle2002_results_CSM_1_100000.rds")
```

Alternatively, we can include X_lag1 in the analysis/estimation and estimate the full conditional model:

```{r}
#| label: Diggle2002_FCM
#| echo: true
#| cache: true
#| eval: false

# Load necessary libraries
library(geepack)

# Simulation parameters
set.seed(123)
n_subjects <- 100000 # Number of subjects
max_time <- 1    # Maximum number of observations per subject
n_sim <- 1     # Number of simulations
gamma_0 <- 0
gamma_1 <- 1
gamma_2 <- 1
rho_values <- seq(0.9, 0.1, by = -0.2) # Range of autocorrelations

# State true marginal conditional means for the fixed slope
true_cond_gamma1 <- rep(gamma_1, length(rho_values))

# Function to generate data for a single simulation
generate_data <- function(rho) {
  data_list <- list()
  
  for (i in 1:n_subjects) {
    # n_i <- sample(2:max_time, 1) # Random number of observations (2 to 10)
    n_i <- 10 # or fixed number of observations
    X_it <- numeric(n_i)
    Y_it <- numeric(n_i)
    b_i <- rnorm(1, 0, 1)        # Random intercept
    e_it <- rnorm(n_i, 0, 1)
    
    # Generate covariate X_it with autoregressive structure
    X_it[1] <- rnorm(1, 0, 1)    # Initial value for X_it
    for (t in 2:n_i) {
      X_it[t] <- rho * X_it[t-1] + rnorm(1, 0, sqrt(1 - rho^2))
    }
    
    # Generate outcome Y_it based on current and lagged values of X_it
    Y_it[1] <- gamma_0 + gamma_1 * X_it[1] + b_i + e_it[1]
    if (n_i > 1) {
      for (t in 2:n_i) {
        Y_it[t] <- gamma_0 + gamma_1 * X_it[t] + gamma_2 * X_it[t-1] + b_i + e_it[t]
      }
    }
    
    # Store the data in a list
    subject_data <- data.frame(
      subject = i,
      time = 1:n_i,
      Y = Y_it,
      X = X_it,
      X_lag1 = c(NA, X_it[-n_i]) # Lagged value of X, with NA for the first observation
    )
    data_list[[i]] <- subject_data
  }
  
  # Combine all subjects' data into a single data frame
  data <- do.call(rbind, data_list)
  return(data)
}

# Simulation loop
results <- data.frame()
for (rho in rho_values) {
  gamma1_estimates <- matrix(NA, n_sim, 4) # Store beta1 estimates for each correlation structure
  
  for (sim in 1:n_sim) {
    # Generate data for this simulation
    sim_data <- generate_data(rho)
    
    # Remove rows with missing X_lag1 values
    sim_data <- na.omit(sim_data)
    
    # Fit GEE models with different working correlation structures
    gee_ind <- geeglm(Y ~ X + X_lag1, id = subject, data = sim_data, corstr = "independence")
    gee_exch <- geeglm(Y ~ X + X_lag1, id = subject, data = sim_data, corstr = "exchangeable")
    gee_ar1 <- geeglm(Y ~ X + X_lag1, id = subject, data = sim_data, corstr = "ar1")
    mlm_mle <- lmer(Y ~ X + X_lag1 + (1 | subject), data = sim_data, REML = FALSE)
    
    # Store the estimated beta1 for each model
    gamma1_estimates[sim, 1] <- coef(gee_ind)["X"]
    gamma1_estimates[sim, 2] <- coef(gee_exch)["X"]
    gamma1_estimates[sim, 3] <- coef(gee_ar1)["X"]
    gamma1_estimates[sim, 4] <- fixef(mlm_mle)["X"]
  }
  
  # Calculate average estimates of beta1 for each correlation structure
  mean_estimates <- colMeans(beta1_estimates, na.rm = TRUE)
  results <- rbind(results, data.frame(
    rho = rho,
    gamma1_ind = mean_estimates[1],
    gamma1_exch = mean_estimates[2],
    gamma1_ar1 = mean_estimates[3],
    gamma1_mlm = mean_estimates[4]
  ))
}

# format and save results
results_complete <- cbind(results, gamma1_true = true_cond_gamma1) %>%
  select(rho, gamma1_true, gamma1_ind, gamma1_exch, gamma1_ar1, gamma1_mlm)

saveRDS(results_complete, file = "output/Diggle2002_results_FCM_1_100000.rds")
```

Here are the results of the simulation. Note that compared to the cross-sectional model, in the full conditional model we add the lagged variable `X_lag1` in the model, whereby we do not obtain the marginal estimate of the fixed slope. Instead, we obtain the conditional estimate of the fixed slope, which is the effect of `X` on `Y` after accounting for the effect of `X_lag1`. In other words, we get the coefficients for $\gamma_1$ and $\gamma_2$ in the model $Y_{it} = \gamma_0 + \gamma_1 X_{it} + \gamma_2 X_{it-1} + b_i + e_{it}$ rather than the coefficient $\beta_1$ in the marginal model $Y_{it} = \beta_0 + \beta_1 X_{it} + b_i + e_{it}$.

```{r}
#| label: tbl-Diggle2002
#| tbl-cap: "Average Estimates for Different Correlation Structures"
#| echo: false

# Load the results
results_CSM <- readRDS("output/Diggle2002_results_CSM_1_100000.rds")
results_FCM <- readRDS("output/Diggle2002_results_FCM_1_100000.rds")

# Print the results
knitr::kable(results_CSM, digits = 3, caption = "Cross-Sectional MeanModel. Average Estimates of Beta1 for Different Correlation Structures")
knitr::kable(results_FCM, digits = 3, caption = "Full Conditional Model. Average Estimates of gamma1 for Different Correlation Structures")
```

In @fig-Diggle2002, we plot the average estimates of $\beta_1$ for different correlation structures in the cross-sectional model.

```{r}
#| label: fig-Diggle2002
#| fig-cap: "Cross-Sectional Mean Model. Average Estimates of Beta1 for Different Correlation Structures"
#| echo: false
#| cache: false

# Plot the results
results_long <- reshape2::melt(results_CSM, id.vars = "rho", variable.name = "correlation_structure", value.name = "beta1_estimate")
ggplot(results_long, aes(x = rho, y = beta1_estimate, color = correlation_structure)) +
  geom_line() +
  labs(x = "Autocorrelation (rho)",
       y = "Estimated Beta1") +
  theme_minimal()
```

These findings are somewhat similar to the original simulation results found by Diggle et al. (2002).

![Simulation Results of Diggle et al. (2002)](figure/Diggle2002_Table12.3.png){#tbl-Diggle2002}

The discrepancy may be due to a slight difference/mistake in its implementation. Nevertheless, like Diggle et al. (2002), we can see that the GEE with working independence is the least biased in terms of marginal effects, followed by the exchangeable and AR(1) working correlation structures.
