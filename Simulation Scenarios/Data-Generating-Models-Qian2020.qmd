---
title: "Data Generating Models of Qian et al. (2020)"
author: "Ward B. Eiling"
date: "2024-10-17"
format: 
  html:
    toc: true
    toc-location: left
    toc-expand: 5
    number-sections: true
    # embed-resources: false
    default-image-extension: svg
execute:
  message: false
  warning: false
comments:
  hypothesis: 
    theme: clean
fig-cap-location: top
---

## Introduction

In this document, I will recreate the data generating models presented by Qian et al. (2020), accompanied by visual representations of these models.

```{r}
#| label: Packages
#| cache: true

# Load packages
library(dagitty)
library(tidyverse)
library(ggdag)
library(lme4)
library(jtools)
library(gee)
library(geex)
library(nlme)
```

## Simple Concrete Example: Without Treatment

### Example in section 2.2 of Qian et al. (2020)

As a concrete example, consider the case where each individual is observed for 2 time points ($T_i = 2$), and the covariate at the second time point is the lag-1 outcome: $X_{i2} = Y_{i2}$.

> By lagging the outcome, we essentially have three time points: $X_{i1}$, $X_{i2} = Y_{i2}$, and $Y_{i3}$.

Suppose the variables are generated from the following multilevel linear model (MLM) with a random intercept:

$$
b_i \sim N(0, \sigma_u^2),
$$

$$
X_{i1} \sim N(0, \sigma_{X_1}^2) \text{ independently of } b_i,
$$

$$
Y_{i2} \mid X_{i1}, b_i \sim N(\beta_0 + \beta_1 X_{i1} + b_i, \sigma_\epsilon^2),
$$

$$
X_{i2} = Y_{i2},
$$

$$
Y_{i3} \mid X_{i1}, Y_{i2}, X_{i2}, b_i \sim N(\beta_0 + \beta_1 X_{i2} + b_i, \sigma_\epsilon^2).
$$

### Translating the notation

The notation here is different from the notation used in the psychological sciences. In the psychological sciences, we would typically denote the random intercept as $u_{0i}$ instead of $b_i$. We would also denote the fixed intercept as $\gamma_{00}$ instead of $\beta_0$ and $\sigma_\epsilon^2$ as $\sigma_e^2$, but keep the fixed slope as $\beta_1$.

Let's now rewrite the model in this notation

$$
u_{0i} \sim N(0, \sigma_u^2),
$$

$$
X_{i1} \sim N(0, \sigma_{X_1}^2) \text{ independently of } u_{0i},
$$

$$
Y_{i2} \mid X_{i1}, u_{0i} \sim N(\gamma_{00} + \beta_1 X_{i1} + u_{0i}, \sigma_e^2),
$$

$$
X_{i2} = Y_{i2},
$$

$$
Y_{i3} \mid X_{i1}, Y_{i2}, X_{i2}, u_{0i} \sim N(\gamma_{00} + \beta_1 X_{i2} + u_{0i}, \sigma_e^2).
$$

### Visualizing the Model

We may now draw the DAG for this model

```{r}
#| label: DAG
#| cache: true
#| fig-cap: DAG for the data generating model in section 2.2 of Qian et al. (2020)
#| echo: false

section2.2_DAG <- dagitty(
  'dag {
bb="0,0,1,1"
X_1 [exposure,pos="0.182,0.160"]
X_2 [exposure,pos="0.454,0.160"]
Y_2 [outcome,pos="0.458,0.282"]
Y_3 [outcome,pos="0.732,0.283"]
X_1 -> Y_2
X_2 -> Y_3
Y_2 -> X_2
}')

ggdag::ggdag_status(section2.2_DAG) + ggdag::theme_dag()

# alternatively, we can include the random intercept
section2.2_DAG_randomeff <- dagitty(
  'dag {
bb="0,0,1,1"
X_1 [exposure,pos="0.182,0.160"]
X_2 [exposure,pos="0.454,0.160"]
Y_2 [outcome,pos="0.458,0.282"]
Y_3 [outcome,pos="0.732,0.283"]
u_0 [latent,pos="0.621,0.399"]
X_1 -> Y_2
X_2 -> Y_3
Y_2 -> X_2
u_0 -> Y_2
u_0 -> Y_3
}
')

ggdag::ggdag_status(section2.2_DAG_randomeff) + ggdag::theme_dag()
```

Note that there is an open biasing path from $Y_2$ to $X_2$ in the DAG: the exposure/predictor $X_2$ is *caused by* (in this case equivalent to) the previous outcome $Y_2$---and thus this time-varying covariate is *endogenous*.

### Generating the data

Let's now generate the data according to this model.

```{r}
#| label: Data Generation
#| cache: true

set.seed(123)

n_i <- 5000 # number of individuals

sigma_u <- 1 # variance of random intercept
sigma_X1 <- 1 # variance of X1
sigma_e <- 0.1 # residual variance
beta_1 <- 0.8 # overall slope
gamma_00 <- 2 # overall intercept

# simulate data
u_0i <- rnorm(n_i, 0, sigma_u)
X_i1 <- rnorm(n_i, 0, sigma_X1)
Y_i2 <- rnorm(n_i, gamma_00 + beta_1 * X_i1 + u_0i, sigma_e)
X_i2 <- Y_i2
Y_i3 <- rnorm(n_i, gamma_00 + beta_1 * X_i2 + u_0i, sigma_e)

# # create data frame in wide format
# section2.2.data_wide <- data.frame(id = 1:n_i,
#                                    X_i1 = X_i1,
#                                    Y_i2 = Y_i2,
#                                    X_i2 = X_i2,
#                                    Y_i3 = Y_i3)
# 
# head(section2.2.data_wide)
# 
# # create data frame in long format
# section2.2data_long <- data.frame(id = rep(1:n_i, each = 3),
#                              time = rep(1:3, n_i),
#                              X = rep(NA, 3 * n_i),
#                              Y = rep(NA, 3 * n_i))
# 
# for (i in 1:n_i){
#   section2.2data_long$X[section2.2data_long$id == i & section2.2data_long$time == 1] <- X_i1[i]
#   section2.2data_long$Y[section2.2data_long$id == i & section2.2data_long$time == 2] <- Y_i2[i]
#   section2.2data_long$X[section2.2data_long$id == i & section2.2data_long$time == 2] <- X_i2[i]
#   section2.2data_long$Y[section2.2data_long$id == i & section2.2data_long$time == 3] <- Y_i3[i]
# }
# 
# 
# head(section2.2data_long)


# create data frame in long format with lagged predictor
section2.2data_long_lagged <- data.frame(id = rep(1:n_i, each = 2),
                             "time" = rep(1:2, n_i),
                             "X_lag1" = rep(NA, 2 * n_i),
                             "Y" = rep(NA, 2 * n_i))

for (i in 1:n_i){
  section2.2data_long_lagged$X_lag1[section2.2data_long_lagged$id == i & section2.2data_long_lagged$time == 1] <- X_i1[i]
  section2.2data_long_lagged$X_lag1[section2.2data_long_lagged$id == i & section2.2data_long_lagged$time == 2] <- X_i2[i]
  section2.2data_long_lagged$Y[section2.2data_long_lagged$id == i & section2.2data_long_lagged$time == 1] <- Y_i2[i]
  section2.2data_long_lagged$Y[section2.2data_long_lagged$id == i & section2.2data_long_lagged$time == 2] <- Y_i3[i]
}

head(section2.2data_long_lagged)
```

### Estimating the model

Let's now estimate the model using a multilevel linear model (MLM) and a generalized estimating equation (GEE) model.

```{r}
#| label: Model Estimation
#| cache: true

# Multilevel Linear Model
section2.2_mlm_reml <- lmer(Y ~ 1 + X_lag1 + (1 | id), data = section2.2data_long_lagged)
section2.2_mlm_mle <- lmer(Y ~ 1 + X_lag1 + (1 | id), data = section2.2data_long_lagged, REML = FALSE)
# Generalized Estimating Equations
section2.2_gee_ind <- gee(Y ~ 1 + X_lag1, id = id, data = section2.2data_long_lagged, family = gaussian, corstr = "independence")
section2.2_gee_exch <- gee(Y ~ 1 + X_lag1, id = id, data = section2.2data_long_lagged, family = gaussian, corstr = "exchangeable")
section2.2_gee_ar1 <- gee(Y ~ 1 + X_lag1, id = id, data = section2.2data_long_lagged, family = gaussian, corstr = "AR-M", Mv = 1)
section2.2_gee_unstr <- gee(Y ~ 1 + X_lag1, id = id, data = section2.2data_long_lagged, family = gaussian, corstr = "unstructured")
# Generalized linear model (OLS): assumes independence of observations
section2.2_glm <- glm(Y ~ 1 + X_lag1, data = section2.2data_long_lagged, family = gaussian) 
# Marginal linear model (GLS)
section2.2_gls_symm_mle <- nlme::gls(Y ~ 1 + X_lag1, data = section2.2data_long_lagged, correlation = corSymm(form = ~ 1 | id), method = "ML") 
section2.2_gls_compsymm_mle <- nlme::gls(Y ~ 1 + X_lag1, data = section2.2data_long_lagged, correlation = corCompSymm(form = ~ 1 | id), method = "ML")

summary(section2.2_gls_symm_mle)

section2.2_coefs <- data.frame(
  row.names = c("Intercept", "X_lag1"),
  MLM_reml = fixef(section2.2_mlm_reml),
  MLM_mle = fixef(section2.2_mlm_mle),
  GEE_ind = coef(section2.2_gee_ind),
  GEE_exch = coef(section2.2_gee_exch),
  GEE_ar1 = coef(section2.2_gee_ar1),
  GEE_unstr = coef(section2.2_gee_unstr),
  GLM = coef(section2.2_glm),
  GLS_Symm = coef(section2.2_gls_symm_mle),
  GLS_CompSymm = coef(section2.2_gls_compsymm_mle)
)

knitr::kable(caption = "Section 2.2: Estimated coefficients from the MLM and GEE models", section2.2_coefs, digits = 3)
```

We can clearly see that the MLM and GEE models provide exactly the same estimates for the fixed intercept and fixed regression coefficient, with the exception of the GEE with independence working correlation structure.

> According to Pepe and Anderson (1994), this is the only structure that can avoid bias in the estimation of the fixed effects (i.e., that has a valid marginal interpretation).

As a reminder, the fixed effects were specified as $\gamma_{00} = 2$ and $\beta_1 = 0.8$. Thus, we can see that all models except the GEE with independence working correlation structure returns estimates that are very close to the true values---which represented the conditional mean of $Y$ given $X$ and $u_{0i}$ rather than the marginal mean of $Y$ given $X$.

To see why this makes sense, it is important to realize that the parameter estimates represent the parsimonious conditional relationship

$$
E[Y_{it+1} \mid X_{it}, u_{0i}] = \gamma_{00} + \beta_1 X_{it} + u_{0i}
$$ And not the marginal relationship, which is given by:

$$
E[Y_{i2} \mid X_{i1}] = \gamma_{00} + \beta_1 X_{i1}
$$

$$
E[Y_{i3} \mid X_{i2}] = (1- \rho \zeta - \rho ) \gamma_{00} + [(1-\rho \zeta)  \beta_1 + \rho] X_{i2}
$$

Let's confirm this by calculating the true marginal effect

```{r}
#| label: Marginal Effects

sigma2_u0 = sigma_u^2
sigma2_e = sigma_e^2

rho = sigma2_u0 / (sigma2_u0 + sigma2_e)

sigma2_X1 = sigma_X1^2
zeta = beta_1 * sigma2_X1 / (beta_1 * sigma2_X1 + sigma2_u0 + sigma2_e)

# Now let's compute the marginal effect of X1 on Y2
marginal_effect_X1_Y2 <- gamma_00 + beta_1
marginaleffect_X2_Y3 <- (1 - rho * zeta - rho) * gamma_00 + ((1 - rho * zeta) * beta_1 + rho)
```

> This is not correct, but how do we calculate the true marginal effects for intercept and slope?

### Intermezzo: What are marginal effects/models?

Marginal models are a class of models that are used to estimate the population average effect of a covariate on an outcome. This may be useful, for instance, when prediction or indeed complete modelling of the data are not the main goal of an analysis (Pepe and Anderson, 1994).

> "Consider, for example, the future practice of screening for risk of respiratory disease, where one might simply ascertain Vitamin A deficiency, weight, height and other covariates at a single time point and make a determination of the child's risk based on these measurements." (Pepe and Anderson, 1994)

Here, the cross-sectional model is of primary interest for use in future screening practices and an in-depth model of longitudinal data is of secondary interest (Pepe and Anderson, 1994).

This contrasts with psychological research, where the cross-sectional model is often deemed problematic in the context of longitudinal data analysis, because it conflates (rather than separates) within-subject and between-subject effects. Instead, we tend to be much more interested in (1) the model that best describes the data (i.e., has the best model fit) and (2) the "why" question: complete model of the effects (including within- and between-person effects). What differs here is the aim of the study.

In what situations may psychological researchers be primarily interested in marginal effects over finding the best description of the data?

1.  A *clinical psychologist* may be interested in screening for depression among a large sample of patients in a primary care setting.

2.  A *school psychologist* may focus on understanding how classroom behaviors (e.g., attention problems, peer conflicts) relate to academic achievement across a large student population.

3.  A *social psychologist* may study the impact of discrimination on mental health outcomes across different demographic groups.

4.  A *developmental psychologist* may study how early childhood factors (e.g., parental education, socioeconomic status, early trauma) influence cognitive development in children.

Whether marginal or conditional models are preferred depends simply upon the research question and aim of the study:

> "We do not suggest that marginal models are preferable in general to conditional models. In Section 1 we provided one example where the marginal model is, in fact, preferable but in many cases it will not be. Indeed, which model should be used depends entirely on the questions to be addressed with the data. If a good description of the process generating the data is required then fully conditional or random effects models might be pursued." (Pepe and Anderson, 1994)

### Intermezzo: What is the difference between REML and MLE?

When fitting a multilevel linear model (MLM) we can choose between restricted maximum likelihood (REML) and maximum likelihood estimation (MLE). In REML, $\sigma^2$ and $\rho$ (intra class correlation) are essentially considered nuisance parameters, which makes sure that small sample bias is reduced. However, since we do not obtain the complete log likelihood, we cannot compare models using the likelihood ratio test. When sample sizes are sufficiently large, the two methods are asymptotically equivalent.

the variance components $\sigma^2_u$ are estimated by maximizing the likelihood of the residuals, conditional on the fixed effects. In MLE, the residual variance $\sigma^2$ and the variance components $\sigma^2_u$ are estimated by maximizing the likelihood of the residuals, conditional on the fixed effects and the random effects.

The difference between the two methods lies in the way they estimate the variance components of the model.

source: [STAT 437: 007. Linear Marginal Models: Likelihood, Inference, and Asymptotics (Theory)](https://www.youtube.com/watch?v=ZjQo0H_FFwI&list=PL0Y1Z_F8RtX7jIbexsd3lizDXMET_WGX0&index=7)

## Main Simulation of Qian et al. (2020): With Treatment

### Original Section: "4. Simulation"

In the simulation, we considered three generative models (GMs), all of which have an endogenous covariate. In the first two GMs, the endogenous covariate $X_{it}$ equals the previous outcome $Y_{it}$ plus some random noise, so the conditional independence assumption (10) is valid. In GM 3, the endogenous covariate depends directly on $b_i$, violating assumption (10). The details of the generative models are described below.

In GM1, we considered a simple case with only a random intercept and a random slope for $A_{it}$, so that $Z_{i(t_0)} = Z_{i(t_2)} = 1$ in model (7). The outcome is generated as:

$$
Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + b_{i0} + A_{it} (\beta_0 + \beta_1 X_{it} + b_{i2}) + \epsilon_{it+1}.
$$

The random effects $b_{i0} \sim N(0, \sigma_{b0}^2)$ and $b_{i2} \sim N(0, \sigma_{b2}^2)$ are independent of each other. The covariate is generated as $X_{i1} \sim N(0, 1)$, and for $t \geq 2$,

$$
X_{it} = Y_{it} + N(0, 1).
$$

The randomization probability $p_t$ is constant at $1/2$. The exogenous noise is $\epsilon_{it+1} \sim N(0, \sigma_\epsilon^2)$.

In GM2, we considered the case where $Z_{i(t_0)} = Z_{i(t_2)} = 1$, with time-varying randomization probability. The outcome is generated as:

$$
Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + b_{i0} + b_{i1} X_{it} + A_{it} (\beta_0 + \beta_1 X_{it} + b_{i2} + b_{i3} X_{it}) + \epsilon_{it+1}.
$$

The random effects $b_{ij} \sim N(0, \sigma_{b_j}^2)$, for $0 \leq j \leq 3$, are independent of each other. The covariate is generated as $X_{i1} \sim N(0, 1)$, and for $t \geq 2$,

$$
X_{it} = Y_{it} + N(0, 1).
$$

The randomization probability depends on $X_{it}$:

$$
p_t = 0.7 \cdot 1(X_{it} > -1.27) + 0.3 \cdot 1(X_{it} \leq -1.27),
$$

where $1(\cdot)$ represents the indicator function, and the cutoff $-1.27$ was chosen so that $p_t$ equals 0.7 or 0.3 for about half of the time. The exogenous noise is $\epsilon_{it+1} \sim N(0, \sigma_\epsilon^2)$.

GM3 is the same as GM 1, except that the covariate $X_{it}$ depends directly on $b_i$:

$$
X_{i1} \sim N(b_{i0}, 1), \quad X_{it} = Y_{it} + N(b_{i0}, 1) \text{ for } t \geq 2.
$$ We chose the following parameter values:

$$
\alpha_0 = -2, \quad \alpha_1 = -0.3, \quad \beta_0 = 1, \quad \beta_1 = 0.3,
$$

$$
\sigma_{b0}^2 = 4, \quad \sigma_{b1}^2 = \frac{1}{4}, \quad \sigma_{b2}^2 = 1, \quad \sigma_{b3}^2 = \frac{1}{4}, \quad \sigma_\epsilon^2 = 1.
$$

## Generative Model 1

### Translation of Notation

In the table below, we will provide the translation of original notation in Qian et al. (2020) to notation more common in psychological research

| Parameter               | Original           | New                |
|-------------------------|--------------------|--------------------|
| Fixed intercept         | $\alpha_0$         | $\gamma_{00}$       |
| Fixed slope for $X_{it}$| $\alpha_1$         | $\gamma_{01}$       |
| Random intercept        | $b_{i0}$           | $u_{0i}$            |
| Random slope for $A_{it}$| $b_{i2}$           | $u_{1i}$            |
| Error term              | $\epsilon_{it+1}$  | $e_{it+1}$          |
| Fixed effect of $A_{it}$| $\beta_0$          | $\gamma_{10}$       |
| Interaction effect of $A_{it}$ and $X_{it}$ | $\beta_1$ | $\gamma_{11}$ |
| Covariate               | $X_{it}$           | $Z_{it}$           |
| Treatment               | $A_{it}$           | $X_{it}$           |

Let's first state the original model:

$$
Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + b_{i0} + A_{it} (\beta_0 + \beta_1 X_{it} + b_{i2}) + \epsilon_{it+1}.
$$

Using this new notation, we may thus rewrite GM1 as a within model:

$$
Y_{it+1} = \beta_{0i} + \beta_{1i} X_{it} + e_{it+1},
$$

where:

$$ 
\beta_{0i} = \gamma_{00} + \gamma_{01} Z_{it} + u_{0i} \quad \text{with} \quad u_{0i} \sim \mathcal{N}(0, \sigma^2_u),
$$

$$ 
\beta_{1i} = \gamma_{10} + \gamma_{11} Z_{it} + u_{1i} \quad \text{with} \quad u_{1i} \sim \mathcal{N}(0, \sigma^2_u).
$$

Combining these two equations, the model can be expressed as:

$$
Y_{it+1} = \gamma_{00} + \gamma_{01} Z_{it} + u_{0i} + X_{it} (\gamma_{10} + \gamma_{11} Z_{it} + u_{1i}) + e_{it+1}.
$$

### Visualzing the Model

As mentioned by Ellen in the last meeting (17-10):

> Conventional DAGs do not only represent main effects but rather the combination of main effects and interactions. Once you have drawn your DAG, you already assume that any variables pointing to the same outcome can modify the effect of the others pointing to the same outcome. [stackexchange](https://stats.stackexchange.com/questions/157775/representing-interaction-effects-in-directed-acyclic-graphs)

For $t = 1$, the DAG and path diagram are as follows:

```{r}
#| label: GM1_visual
#| cache: true
#| fig-subcap:
#|  - "DAG"
#|  - "Path Diagam"
#| layout-nrow: 2
#| echo: false

GM1_DAG <- dagitty('dag {
bb="0,0,1,1"
"Y_t+1" [outcome,pos="0.723,0.432"]
X_t [exposure,pos="0.421,0.431"]
Z_t [pos="0.424,0.602"]
X_t -> "Y_t+1"
Z_t -> "Y_t+1"
}')

ggdag::ggdag_status(GM1_DAG) + ggdag::theme_dag()

GM1_Path <- dagitty('dag {
bb="0,0,1,1"
"Y_t+1" [outcome,pos="0.723,0.432"]
. [pos="0.576,0.434"]
X_t [exposure,pos="0.421,0.431"]
Z_t [pos="0.576,0.556"]
. -> "Y_t+1"
X_t -> .
Z_t -> .
}')

ggdag::ggdag_status(GM1_Path) + ggdag::theme_dag()
```

Note that the interaction between $X_{it}$ and $Z_{it}$ is not explicitly shown in the DAG, but is explicit in the path diagram. This is because the interaction is a model assumption, which is not explicitly represented in the non-parametric DAG.

For $t \geq 2$, the DAG and path diagram are as follows:

```{r}
#| label: GM1_visual2
#| echo: false
#| fig-subcap:
#|  - "DAG"
#|  - "Path Diagam"
#| layout-nrow: 2
#| cache: true

GM1_DAG2 <- dagitty('dag {
bb="0,0,1,1"
"Y_t+1" [outcome,pos="0.780,0.566"]
"Z_t+1" [pos="0.780,0.741"]
X_t [exposure,pos="0.602,0.406"]
Y_t [outcome,pos="0.605,0.554"]
Z_t [pos="0.607,0.742"]
"Y_t+1" -> "Z_t+1"
X_t -> "Y_t+1"
Y_t -> Z_t
Z_t -> "Y_t+1"
}
')

ggdag::ggdag_status(GM1_DAG2) + ggdag::theme_dag()

GM1_Path2 <- dagitty('dag {
bb="0,0,1,1"
"Y_t+1" [outcome,pos="0.780,0.566"]
"Z_t+1" [pos="0.780,0.741"]
. [pos="0.693,0.565"]
X_t [exposure,pos="0.606,0.402"]
Y_t [outcome,pos="0.605,0.554"]
Z_t [pos="0.607,0.742"]
"Y_t+1" -> "Z_t+1"
. -> "Y_t+1"
X_t -> .
Y_t -> Z_t
Z_t -> .
}')

ggdag::ggdag_status(GM1_Path2) + ggdag::theme_dag()
```

So the DAG for the first two observations looks like

```{r}
#| label: GM1_visual3
#| echo: false
#| cache: true

GM1_DAG3 <- dagitty('dag {
bb="0,0,1,1"
X_1 [exposure,pos="0.439,0.432"]
X_2 [exposure,pos="0.605,0.428"]
Y_2 [outcome,pos="0.608,0.565"]
Y_3 [outcome,pos="0.742,0.571"]
Z_1 [pos="0.441,0.688"]
Z_2 [pos="0.610,0.694"]
X_1 -> Y_2
X_2 -> Y_3
Y_2 -> Z_2
Z_1 -> Y_2
Z_2 -> Y_3
}')

ggdag::ggdag_status(GM1_DAG3) + ggdag::theme_dag()
```

## Generative Model 2

### Translation of Notation

Now we need to translate more parameters:

| Parameter                                | Original           | New                |
|------------------------------------------|--------------------|--------------------|
| Fixed intercept                          | $\alpha_0$         | $\gamma_{00}$       |
| Fixed slope for $X_{it}$                 | $\alpha_1$         | $\gamma_{10}$       |
| Random intercept                         | $b_{i0}$           | $u_{0i}$            |
| Random slope for $X_{it}$                | $b_{i1}$           | $u_{1i}$            |
| Fixed effect of $A_{it}$                 | $\beta_0$          | $\gamma_{20}$       |
| Interaction effect of $A_{it}$ and $X_{it}$ | $\beta_1$          | $\gamma_{30}$       |
| Random slope for $A_{it}$                | $b_{i2}$           | $u_{2i}$            |
| Random interaction effect for $A_{it} \times X_{it}$ | $b_{i3}$          | $u_{3i}$            |
| Error term                               | $\epsilon_{it+1}$  | $e_{it+1}$          |
| Covariate                                | $X_{it}$           | $Z_{it}$           |
| Treatment                                | $A_{it}$           | $X_{it}$           |

Let's first restate the original model:

$$
Y_{it+1} = \alpha_0 + \alpha_1 X_{it} + b_{i0} + b_{i1} X_{it} + A_{it} (\beta_0 + \beta_1 X_{it} + b_{i2} + b_{i3} X_{it}) + \epsilon_{it+1}.
$$

Using the psychological notation, we rewrite GM2 as a within-person model:

$$
Y_{it+1} = \beta_{0i} + \beta_{1i} Z_{it} + \beta_{2i} X_{it} + \beta_{3i} X_{it} Z_{it} + e_{it+1},
$$

with:

$$
\beta_{0i} = \gamma_{00} + u_{0i} \quad \text{where} \quad u_{0i} \sim \mathcal{N}(0, \sigma^2_u),
$$

$$
\beta_{1i} = \gamma_{10} + u_{1i} \quad \text{where} \quad u_{1i} \sim \mathcal{N}(0, \sigma^2_u),
$$

$$
\beta_{2i} = \gamma_{20} + u_{2i} \quad \text{where} \quad u_{2i} \sim \mathcal{N}(0, \sigma^2_u),
$$

$$
\beta_{3i} = \gamma_{30} + u_{3i} \quad \text{where} \quad u_{3i} \sim \mathcal{N}(0, \sigma^2_u).
$$

Combining these, the full model becomes:

$$
Y_{it+1} = (\gamma_{00} + u_{0i}) + (\gamma_{10} + u_{1i}) Z_{it} + (\gamma_{20} + u_{2i}) X_{it} + (\gamma_{30} + u_{3i}) X_{it} Z_{it} + e_{it+1}.
$$

### Visualizing the Model


```{r}
#| label: GM2_visual
#| cache: true
#| fig-subcap:
#| - "DAG"
#| - "Path Diagram"
#| layout-nrow: 2
#| echo: false

# GM2_DAG <- dagitty('dag{

```

## Generative Model 3

### Translation of Notation

...

### Visualizing the Model

...


